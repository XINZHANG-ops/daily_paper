
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-03-20 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
        }
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .paper-card:hover {
            transform: translateY(-5px); /* Lift effect on hover */
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */
        }
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .paper-card p {
            margin: 5px 0;
        }
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        .paper-card a:hover {
            text-decoration: underline;
        }
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .category-chunk:hover {
            transform: translateY(-3px); /* Slightly smaller lift for categories */
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */
        }
        .category-chunk:nth-child(1) { /* 1. Topic and Domain */
            background-color: #d3e3fd; /* Blue */
        }
        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */
            background-color: #e6d6fa; /* Purple */
        }
        .category-chunk:nth-child(3) { /* 3. Problem */
            background-color: #d4f8d9; /* Green */
        }
        .category-chunk:nth-child(4) { /* 4. Methods */
            background-color: #ffd7d5; /* Pink */
        }
        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */
            background-color: #d3e3fd; /* Reuse Blue */
        }
    </style>
</head>
<body>
    <h1>2025-03-20 Papers</h1>
    
        <div class="paper-card">
            <h2>Paper: 1</h2>
            <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>
            <p><strong>Published: </strong>2025-03-19</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15417" target="_blank">http://arxiv.org/pdf/2503.15417</a></p>
            <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy involving frame-level and block-level perturbations.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used FLUX FLOW, which applies controlled temporal perturbations (frame shuffling and block reordering) to video data during training.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Results on UCF-101 and VBench benchmarks showed that FLUX FLOW improved temporal coherence and diversity across various video generation models, while maintaining spatial fidelity, and was evaluated using metrics like FVD, IS, and VBench's temporal/frame-wise quality scores, along with a user study.</div></div>
        </div>
        
        <div class="paper-card">
            <h2>Paper: 2</h2>
            <p><strong>Optimizing Decomposition for Optimal Claim Verification</strong></p>
            <p><strong>Published: </strong>2025-03-19</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15354" target="_blank">http://arxiv.org/pdf/2503.15354</a></p>
            <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on fact-checking of long-form text using a Decompose-Then-Verify paradigm, specifically within the domain of natural language processing and computational linguistics.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing Decompose-Then-Verify methods for fact-checking, but proposes a new reinforcement learning framework (dynamic decomposition) to optimize the decomposition process based on verifier feedback and a novel "atomicity" metric.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the misalignment between decomposition policies and verification performance in fact-checking systems, where existing methods don't generate subclaims with optimal information density for the verifier.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors use a reinforcement learning (RL) approach, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a decomposition policy that dynamically adjusts subclaim atomicity.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> The proposed dynamic decomposition method outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average, evaluated across various verifiers, datasets, and input claim atomicities.</div></div>
        </div>
        
        <div class="paper-card">
            <h2>Paper: 3</h2>
            <p><strong>œÜ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time
  Exploration and Exploitation</strong></p>
            <p><strong>Published: </strong>2025-03-17</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.13288" target="_blank">http://arxiv.org/pdf/2503.13288</a></p>
            <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on inference-time optimization for large language models (LLMs) in the domain of natural language processing and reasoning tasks.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on search-based inference-time optimization methods like Tree-of-Thoughts (ToT) and Monte Carlo Tree Search (MCTS), and proposes "œï-Decoding," a new adaptive foresight sampling strategy.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of balancing exploration and exploitation during inference-time optimization of LLMs, avoiding excessive exploration and achieving globally optimal step estimation.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors use a novel decoding strategy named œï-Decoding, which leverages foresight paths and clustering to estimate step value, along with in-width and in-depth pruning strategies for adaptive computation allocation.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> The results, evaluated across seven reasoning benchmarks, show that œï-Decoding outperforms strong baselines in both performance and efficiency, and demonstrates generalization across various LLMs and scalability.</div></div>
        </div>
        
</body>
</html>
