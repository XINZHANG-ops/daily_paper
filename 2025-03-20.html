
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-03-20 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
        }
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .paper-card:hover {
            transform: translateY(-5px); /* Lift effect on hover */
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */
        }
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .paper-card p {
            margin: 5px 0;
        }
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        .paper-card a:hover {
            text-decoration: underline;
        }
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .category-chunk:hover {
            transform: translateY(-3px); /* Slightly smaller lift for categories */
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */
        }
        .category-chunk:nth-child(1) { /* 1. Topic and Domain */
            background-color: #d3e3fd; /* Blue */
        }
        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */
            background-color: #e6d6fa; /* Purple */
        }
        .category-chunk:nth-child(3) { /* 3. Problem */
            background-color: #d4f8d9; /* Green */
        }
        .category-chunk:nth-child(4) { /* 4. Methods */
            background-color: #ffd7d5; /* Pink */
        }
        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */
            background-color: #d3e3fd; /* Reuse Blue */
        }
    </style>
</head>
<body>
    <h1>2025-03-20 Papers</h1>
    
    <div class="paper-card">
        <h2>Paper: 1</h2>
        <p><strong>DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement
Learning</strong></p>
        <p><strong>Published: </strong>2025-03-19</p>
        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15265" target="_blank">http://arxiv.org/pdf/2503.15265</a></p>
        <div>
            <div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong>The paper focuses on 3D mesh generation, specifically creating artist-like triangle meshes within the domain of computer graphics and computer vision.</div>
            <div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong>The paper builds upon auto-regressive mesh generation methods like MeshGPT and BPT, proposing a new tokenization algorithm, data curation strategies, and the novel application of Direct Preference Optimization (DPO) for aligning mesh generation with human preferences.</div>
            <div class="category-chunk">3.  <strong>‚ùì Problem:</strong>The paper aims to solve the limitations of existing auto-regressive mesh generation methods, such as limited face counts, mesh incompleteness, high computational costs, and the lack of alignment with human aesthetic preferences.</div>
            <div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong>The authors use an improved mesh tokenization algorithm, data curation and packaging strategies, a decoder-only transformer architecture with cross-attention, and Direct Preference Optimization (DPO) with a novel scoring standard combining 3D metrics and human evaluation.</div>
            <div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong>The results demonstrate that DeepMesh generates higher-quality, more detailed, and aesthetically pleasing meshes compared to state-of-the-art methods, evaluated through quantitative metrics (Chamfer Distance, Hausdorff Distance), a user study, and comparisons of tokenization efficiency.</div>
        </div>
    </div>
    
    <div class="paper-card">
        <h2>Paper: 1</h2>
        <p><strong>TULIP: Towards Unified Language-Image Pretraining</strong></p>
        <p><strong>Published: </strong>2025-03-19</p>
        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15485" target="_blank">http://arxiv.org/pdf/2503.15485</a></p>
        <div>
            <div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong>The paper introduces TULIP, a unified language-image pretraining model designed to improve both high-level semantic understanding and fine-grained visual detail representation in image-text tasks.</div>
            <div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong>The paper builds on contrastive image-text models like CLIP and SigLIP, but proposes generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization.</div>
            <div class="category-chunk">3.  <strong>‚ùì Problem:</strong>Existing contrastive image-text models often struggle with vision-centric tasks requiring high-fidelity image understanding, such as spatial reasoning and fine-grained object recognition.</div>
            <div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong>The authors used generative data augmentation (GeCo), multi-view contrastive learning (image-text, image-image, text-text), and a reconstruction loss to train the model.</div>
            <div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong>TULIP outperforms state-of-the-art models on zero-shot classification, fine-grained recognition, object detection, and multi-modal reasoning tasks, demonstrating improved visual and language understanding.</div>
        </div>
    </div>
    
    <div class="paper-card">
        <h2>Paper: 1</h2>
        <p><strong>Cube: A Roblox View of 3D Intelligence</strong></p>
        <p><strong>Published: </strong>2025-03-19</p>
        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15475" target="_blank">http://arxiv.org/pdf/2503.15475</a></p>
        <div>
            <div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong>The paper focuses on 3D generative AI and its application within the Roblox platform, specifically addressing 3D shape tokenization.</div>
            <div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong>The paper builds on foundation models, vector quantization, and transformer architectures, proposing Phase-Modulated Positional Encoding, stochastic linear shortcut, and self-supervised loss for 3D shape tokenization.</div>
            <div class="category-chunk">3.  <strong>‚ùì Problem:</strong>The paper aims to solve the challenge of representing and generating 3D shapes in a way that is compatible with large language models and suitable for various generative tasks.</div>
            <div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong>The authors used an encoder-decoder architecture with a Perceiver-based transformer, vector quantization, Phase-Modulated Positional Encoding, stochastic gradient shortcut, and self-supervised loss.</div>
            <div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong>The proposed shape tokenizer outperformed existing methods in shape reconstruction quality (measured by S-IoU and V-IoU), and enabled applications like text-to-shape, shape-to-text, and text-to-scene generation.</div>
        </div>
    </div>
        
</body>
</html>
