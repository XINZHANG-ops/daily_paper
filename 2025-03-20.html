
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-03-20 ËÆ∫ÊñáÊé®ÈÄÅ</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
        }
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .paper-card:hover {
            transform: translateY(-5px); /* Lift effect on hover */
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */
        }
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .paper-card p {
            margin: 5px 0;
        }
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        .paper-card a:hover {
            text-decoration: underline;
        }
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .category-chunk:hover {
            transform: translateY(-3px); /* Slightly smaller lift for categories */
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */
        }
        .category-chunk:nth-child(1) { /* 1. Topic and Domain */
            background-color: #d3e3fd; /* Blue */
        }
        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */
            background-color: #e6d6fa; /* Purple */
        }
        .category-chunk:nth-child(3) { /* 3. Problem */
            background-color: #d4f8d9; /* Green */
        }
        .category-chunk:nth-child(4) { /* 4. Methods */
            background-color: #ffd7d5; /* Pink */
        }
        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */
            background-color: #d3e3fd; /* Reuse Blue */
        }
    </style>
</head>
<body>
    <h1>2025-03-20 ËÆ∫ÊñáÊé®ÈÄÅ</h1>
    
        <div class="paper-card">
            <h2>Paper: 1</h2>
            <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>
            <p><strong>Published: </strong>2025-03-19</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15417" target="_blank">http://arxiv.org/pdf/2503.15417</a></p>
            <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation.

2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing video generation methods (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric, physics-informed, training dynamics optimization), proposing a novel data-level temporal augmentation strategy called FLUX FLOW.

3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion) and limited temporal diversity in videos produced by existing video generation models.

4.  <strong>üõ†Ô∏è Methods:</strong> The authors used FLUX FLOW, a data augmentation strategy involving frame-level and block-level temporal perturbations (shuffling and reordering) applied during training of video generation models.

5.  <strong>üìä Results and Evaluation:</strong> Results, evaluated on UCF-101 and VBench benchmarks, show that FLUX FLOW significantly improves temporal coherence and diversity across various video generation models, while maintaining spatial fidelity, and user studies confirmed improvements in motion quality.</div></div>
        </div>
        
        <div class="paper-card">
            <h2>Paper: 2</h2>
            <p><strong>œÜ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time
  Exploration and Exploitation</strong></p>
            <p><strong>Published: </strong>2025-03-17</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.13288" target="_blank">http://arxiv.org/pdf/2503.13288</a></p>
            <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on inference-time optimization for large language models (LLMs) in the domain of natural language processing, specifically reasoning tasks.

2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on search-based inference-time optimization methods like Tree-of-Thoughts (ToT) and proposes a new decoding strategy called œï-Decoding, which uses foresight sampling and dynamic pruning.

3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of balancing exploration and exploitation during inference-time optimization of LLMs, reducing the computational cost while maintaining or improving reasoning performance.

4.  <strong>üõ†Ô∏è Methods:</strong> The authors used foresight sampling to estimate step values, clustering of foresight paths for alignment assessment, and dynamic in-width and in-depth pruning strategies for adaptive computation allocation.

5.  <strong>üìä Results and Evaluation:</strong> œï-Decoding outperformed strong baselines in both performance and efficiency across seven reasoning benchmarks and demonstrated generalization across various LLMs and scalability across different computing budgets.</div></div>
        </div>
        
</body>
</html>
