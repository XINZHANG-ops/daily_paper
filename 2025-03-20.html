
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-03-20 Paper</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
        }
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
        }
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        .paper-card p {
            margin: 5px 0;
        }
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        .paper-card a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>2025-03-20 Paper</h1>
    
        <div class="paper-card">
            <h2>Paper: 1</h2>
            <p><strong>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</strong></p>
            <p><strong>Published: </strong>2025-03-18</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.14476" target="_blank">http://arxiv.org/pdf/2503.14476</a></p>
            <div>Here's a concise analysis of the paper, following your specified format:<br><br>1.  <strong>üìò Topic and Domain:</strong> The paper focuses on reinforcement learning (RL) for large language models (LLMs), specifically within the domain of mathematical reasoning.<br><br>2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds upon prior work in RL for LLMs like Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), and proposes a new algorithm called Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) with four key techniques to improve RL training.<br><br>3.  <strong>‚ùì Problem:</strong> The paper aims to solve the challenge of reproducing and improving upon state-of-the-art RL training results for LLMs in complex reasoning tasks, addressing issues like entropy collapse, reward noise, and training instability.<br><br>4.  <strong>üõ†Ô∏è Methods:</strong> The authors used the DAPO algorithm, incorporating techniques like Clip-Higher, Dynamic Sampling, Token-Level Policy Gradient Loss, and Overlong Reward Shaping, implemented on the `verl` framework.<br><br>5.  <strong>üìä Results and Evaluation:</strong> The DAPO algorithm, trained on the Qwen2.5-32B model, achieved 50 points on the AIME 2024 benchmark, outperforming previous state-of-the-art results with fewer training steps, and the results were evaluated using metrics like accuracy, entropy, and response length.<br></div>
        </div>
        
        <div class="paper-card">
            <h2>Paper: 2</h2>
            <p><strong>DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement
  Learning</strong></p>
            <p><strong>Published: </strong>2025-03-19</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15265" target="_blank">http://arxiv.org/pdf/2503.15265</a></p>
            <div>1.  <strong>üìò Topic and Domain:</strong> The paper focuses on 3D mesh generation, specifically creating artist-like, high-quality triangle meshes within the domain of computer graphics and computer vision.<br><br>2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on auto-regressive methods for mesh generation (like MeshGPT) and proposes a new tokenization algorithm and the use of Reinforcement Learning (specifically Direct Preference Optimization - DPO) for mesh generation.<br><br>3.  <strong>‚ùì Problem:</strong> The paper aims to solve the limitations of existing auto-regressive mesh generation methods, such as limited face counts, mesh incompleteness, high computational costs, and lack of alignment with human preferences.<br><br>4.  <strong>üõ†Ô∏è Methods:</strong> The authors used an improved mesh tokenization algorithm, data curation and packaging strategies, a transformer-based auto-regressive model, and Direct Preference Optimization (DPO) with a novel scoring standard combining 3D metrics and human evaluation.<br><br>5.  <strong>üìä Results and Evaluation:</strong> The results show that DeepMesh outperforms state-of-the-art methods in generating high-quality, detailed meshes with precise topology, evaluated through quantitative metrics (Chamfer Distance, Hausdorff Distance) and a user study.<br></div>
        </div>
        
        <div class="paper-card">
            <h2>Paper: 3</h2>
            <p><strong>TULIP: Towards Unified Language-Image Pretraining</strong></p>
            <p><strong>Published: </strong>2025-03-19</p>
            <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15485" target="_blank">http://arxiv.org/pdf/2503.15485</a></p>
            <div>1.  <strong>üìò Topic and Domain:</strong> The paper introduces TULIP, a unified language-image pretraining model in the domain of multimodal machine learning, specifically focusing on contrastive image-text learning.<br><br>2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing contrastive image-text (CIT) models like CLIP and SigLIP, and vision-centric self-supervised learning methods, proposing generative data augmentation, enhanced contrastive learning (image-image, text-text), and image/text reconstruction regularization.<br><br>3.  <strong>‚ùì Problem:</strong> The paper aims to solve the limitations of existing CIT models in understanding fine-grained visual details and spatial reasoning, while preserving their strengths in global semantic alignment.<br><br>4.  <strong>üõ†Ô∏è Methods:</strong> The authors used generative data augmentation (GeCo), multi-view contrastive learning (image-text, image-image, text-text), and a reconstruction objective (using MAE for images and a causal decoder for text).<br><br>5.  <strong>üìä Results and Evaluation:</strong> TULIP outperformed state-of-the-art models on zero-shot classification, fine-grained recognition, text-image retrieval, and multimodal reasoning tasks, demonstrated through evaluations on various benchmarks like ImageNet, COCO, Flickr, MMVP, and Winoground.<br></div>
        </div>
        
</body>
</html>
