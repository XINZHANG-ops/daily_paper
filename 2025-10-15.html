
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-10-15 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* 卡片容器样式 - 新增 */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            height: 600px; /* 固定高度 */
            cursor: pointer; /* 增加指针样式提示可点击 */
        }
        
        /* 卡片通用样式 */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
        }
        
        /* 轮播卡片样式 - 新增 */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%;
            transition: transform 0.5s ease, opacity 0.5s ease;
        }
        
        /* 非激活卡片的样式 - 新增 */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* 激活卡片的样式 - 新增 */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* 第一张卡片（文本内容）不需要滚动 */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* 第二张卡片（流程图）支持滚动 */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow: auto !important;
            padding-bottom: 50px; /* 添加底部填充 */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-height: none; /* 移除任何高度限制 */
        }
        
        /* 传统卡片样式 */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        
        .paper-card p {
            margin: 5px 0;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* 卡片计数器 - 新增 */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* 改为固定定位，不随滚动而移动 */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* 居中显示 */
            width: 90%;
            max-width: 500px; /* 增加最大宽度，适应长内容 */
            max-height: 80vh; /* 限制最大高度 */
            overflow-y: auto; /* 内容过多时可滚动 */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* 确保显示在最上层 */
        }
        
        /* 添加遮罩层，防止问题卡被其他内容遮挡 */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* 使用JavaScript控制问题卡的显示和隐藏，不再使用hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* 确保长单词自动换行 */
            overflow-wrap: break-word;
            hyphens: auto; /* 在必要时使用连字符 */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* 确保长单词自动换行 */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* 长文本左对齐 */
            display: block; /* 确保是块级元素 */
            white-space: normal; /* 允许自动换行 */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* 长文本选项的特殊样式 */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* 确保弹窗中的按钮文本不会溢出 */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* 适应超长选项文本 */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        @media (max-width: 768px) {
            .paper-container {
                flex-direction: column;
            }
            
            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                height: 650px; /* 移动设备上高度调整 */
            }
            
            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }
            
            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }
            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
            }
        }
    </style>
</head>
<body>
    <h1>2025-10-15 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/woven.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>Spatial Forcing: Implicit Spatial Representation Alignment for
  Vision-language-action Model</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-10-14</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2510.12276" target="_blank">http://arxiv.org/pdf/2510.12276</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper focuses on improving spatial awareness in Vision-Language-Action (VLA) models for robotic manipulation through implicit spatial representation alignment.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> Based on previous VLA models that rely on explicit 3D sensor inputs or depth estimators, this paper proposes a novel approach of implicitly developing spatial comprehension without relying on explicit 3D data.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper aims to solve the challenge of enabling VLA models to develop accurate spatial awareness without depending on explicit 3D sensor information or depth estimators, which are often unreliable or unavailable.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> The authors introduce Spatial Forcing (SF), which aligns intermediate visual embeddings of VLAs with geometric representations from pretrained 3D foundation models through cosine similarity scoring and representation alignment.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> SF achieved state-of-the-art results on LIBERO and RoboTwin benchmarks, accelerated training by up to 3.8x, improved data efficiency, and demonstrated superior performance in both simulated and real-world robotic tasks.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Spatial Forcing: Implicit Spatial Representation Alignment for
  Vision-language-action Model</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background gradient -->
  <defs>
    <linearGradient id="bgGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0f8ff;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#e6f3ff;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="blueGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#4a90e2;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#357abd;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="greenGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#5cb85c;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#449d44;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="orangeGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0ad4e;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#ec971f;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="purpleGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#9b59b6;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#8e44ad;stop-opacity:1" />
    </linearGradient>
  </defs>
  
  <!-- Background -->
  <rect width="1000" height="800" fill="url(#bgGrad)"/>
  
  <!-- Title -->
  <text x="500" y="40" font-family="Arial, sans-serif" font-size="24" font-weight="bold" text-anchor="middle" fill="#2c3e50">Spatial Forcing (SF) Methodology Flow</text>
  
  <!-- Phase 1: Problem Analysis -->
  <rect x="50" y="80" width="180" height="120" rx="10" fill="url(#blueGrad)" stroke="#2980b9" stroke-width="2"/>
  <text x="140" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Problem Analysis</text>
  <text x="140" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Depth Probing</text>
  <text x="140" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Experiment</text>
  <text x="140" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">VLA lacks spatial</text>
  <text x="140" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">understanding</text>
  <text x="140" y="185" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">in embeddings</text>
  
  <!-- Phase 2: 3D Foundation Model -->
  <rect x="280" y="80" width="180" height="120" rx="10" fill="url(#greenGrad)" stroke="#27ae60" stroke-width="2"/>
  <text x="370" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">3D Foundation Model</text>
  <text x="370" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">VGGT Processing</text>
  <text x="370" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Multi-view Images</text>
  <text x="370" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">→ Spatial</text>
  <text x="370" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Representations</text>
  <text x="370" y="185" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">f3D(I)</text>
  
  <!-- Phase 3: VLA Processing -->
  <rect x="510" y="80" width="180" height="120" rx="10" fill="url(#orangeGrad)" stroke="#e67e22" stroke-width="2"/>
  <text x="600" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">VLA Processing</text>
  <text x="600" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Vision Tokens</text>
  <text x="600" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Language Tokens</text>
  <text x="600" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Intermediate</text>
  <text x="600" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Visual Embeddings</text>
  <text x="600" y="185" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">xVi</text>
  
  <!-- Phase 4: Alignment Process -->
  <rect x="740" y="80" width="180" height="120" rx="10" fill="url(#purpleGrad)" stroke="#8e44ad" stroke-width="2"/>
  <text x="830" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Alignment Process</text>
  <text x="830" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Batch Normalization</text>
  <text x="830" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">+ MLP</text>
  <text x="830" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Cosine Similarity</text>
  <text x="830" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Alignment</text>
  
  <!-- Central Alignment Formula -->
  <rect x="250" y="250" width="500" height="80" rx="15" fill="#ecf0f1" stroke="#34495e" stroke-width="3"/>
  <text x="500" y="275" font-family="Arial, sans-serif" font-size="16" font-weight="bold" text-anchor="middle" fill="#2c3e50">Spatial Forcing Alignment Loss</text>
  <text x="500" y="300" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#34495e">Lalign = -1/N Σ S[MLP·Γ(xVi), f3Di(I) + E]</text>
  <text x="500" y="320" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#7f8c8d">where S[·,·] is cosine similarity</text>
  
  <!-- Training Objective -->
  <rect x="350" y="370" width="300" height="60" rx="10" fill="#e74c3c" stroke="#c0392b" stroke-width="2"/>
  <text x="500" y="395" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Combined Training Loss</text>
  <text x="500" y="415" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="white">LSF = Laction + α·Lalign</text>
  
  <!-- Results Section -->
  <g transform="translate(0, 480)">
    <!-- Performance Results -->
    <rect x="50" y="0" width="200" height="100" rx="10" fill="#16a085" stroke="#138d75" stroke-width="2"/>
    <text x="150" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Performance Gains</text>
    <text x="150" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">LIBERO: 98.5% SR</text>
    <text x="150" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">RoboTwin: SOTA</text>
    <text x="150" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Real-world: +47.5%</text>
    
    <!-- Training Efficiency -->
    <rect x="280" y="0" width="200" height="100" rx="10" fill="#d35400" stroke="#ba4a00" stroke-width="2"/>
    <text x="380" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Training Efficiency</text>
    <text x="380" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">3.8× faster</text>
    <text x="380" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">convergence</text>
    <text x="380" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Same success rates</text>
    
    <!-- Data Efficiency -->
    <rect x="510" y="0" width="200" height="100" rx="10" fill="#8e44ad" stroke="#7d3c98" stroke-width="2"/>
    <text x="610" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Data Efficiency</text>
    <text x="610" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">5.9× more efficient</text>
    <text x="610" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">75.8% SR with</text>
    <text x="610" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">only 5% data</text>
    
    <!-- Inference -->
    <rect x="740" y="0" width="200" height="100" rx="10" fill="#2980b9" stroke="#21618c" stroke-width="2"/>
    <text x="840" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Inference</text>
    <text x="840" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">No additional</text>
    <text x="840" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">computational</text>
    <text x="840" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">overhead</text>
  </g>
  
  <!-- Key Innovation Box -->
  <rect x="50" y="620" width="900" height="80" rx="15" fill="#f8f9fa" stroke="#495057" stroke-width="2" stroke-dasharray="5,5"/>
  <text x="500" y="645" font-family="Arial, sans-serif" font-size="16" font-weight="bold" text-anchor="middle" fill="#495057">Key Innovation</text>
  <text x="500" y="665" font-family="Arial, sans-serif" font-size="13" text-anchor="middle" fill="#6c757d">Implicit spatial comprehension without explicit 3D inputs or depth estimators</text>
  <text x="500" y="685" font-family="Arial, sans-serif" font-size="13" text-anchor="middle" fill="#6c757d">Aligns intermediate VLA embeddings with pretrained 3D foundation model representations</text>
  
  <!-- Connection lines -->
  <line x1="230" y1="140" x2="280" y2="140" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="460" y1="140" x2="510" y2="140" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="690" y1="140" x2="740" y2="140" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Vertical connections -->
  <line x1="370" y1="200" x2="370" y2="250" stroke="#27ae60" stroke-width="3"/>
  <line x1="600" y1="200" x2="600" y2="250" stroke="#e67e22" stroke-width="3"/>
  <line x1="500" y1="330" x2="500" y2="370" stroke="#34495e" stroke-width="3"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#34495e"/>
    </marker>
  </defs>
  
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Dependence on unreliable depth sensors and incomplete datasets">
                        <div class="quiz-question">1. What is the main limitation of existing 3D VLA approaches that Spatial Forcing aims to overcome?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="High computational cost of processing 3D data">High computational cost of processing 3D data</div><div class="quiz-choice" data-value="Dependence on unreliable depth sensors and incomplete datasets">Dependence on unreliable depth sensors and incomplete datasets</div><div class="quiz-choice" data-value="Inability to handle multiple camera views">Inability to handle multiple camera views</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="By aligning visual embeddings with pretrained 3D foundation model representations">
                        <div class="quiz-question">2. How does Spatial Forcing achieve spatial awareness in VLA models?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By adding extra 3D sensors to the robot">By adding extra 3D sensors to the robot</div><div class="quiz-choice" data-value="By training a separate depth estimation network">By training a separate depth estimation network</div><div class="quiz-choice" data-value="By aligning visual embeddings with pretrained 3D foundation model representations">By aligning visual embeddings with pretrained 3D foundation model representations</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Reduced training time by 3.8x">
                        <div class="quiz-question">3. What significant performance improvement did Spatial Forcing demonstrate in training efficiency?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Reduced training time by 3.8x">Reduced training time by 3.8x</div><div class="quiz-choice" data-value="Improved accuracy by 3.8%">Improved accuracy by 3.8%</div><div class="quiz-choice" data-value="Reduced memory usage by 3.8x">Reduced memory usage by 3.8x</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/black-linen-2.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>Advancing End-to-End Pixel Space Generative Modeling via Self-supervised
  Pre-training</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-10-14</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2510.12586" target="_blank">http://arxiv.org/pdf/2510.12586</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> Pixel-space generative modeling for image synthesis using diffusion and consistency models.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> Based on self-supervised learning approaches and prior diffusion models, proposes a novel two-stage training framework with self-supervised pre-training instead of relying on VAEs.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> Addressing the persistent performance and efficiency gap between pixel-space generative models and their latent-space counterparts.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> Uses a two-stage approach: pre-training encoders to capture semantics from clean images while aligning them along deterministic sampling trajectories, then fine-tuning with a randomly initialized decoder end-to-end.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> Achieved FID scores of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 NFEs for diffusion models, and 8.82 FID for one-step consistency model generation on ImageNet-256, surpassing previous pixel-space methods.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Advancing End-to-End Pixel Space Generative Modeling via Self-supervised
  Pre-training</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="18" font-weight="bold" fill="#2c3e50">
    EPG: End-to-End Pixel Space Generative Modeling Framework
  </text>
  
  <!-- Stage 1: Pre-training -->
  <rect x="50" y="80" width="400" height="320" fill="#e8f4fd" stroke="#3498db" stroke-width="2" rx="10"/>
  <text x="250" y="105" text-anchor="middle" font-size="16" font-weight="bold" fill="#2980b9">
    Stage 1: Self-Supervised Pre-training
  </text>
  
  <!-- Input Data -->
  <rect x="70" y="130" width="120" height="50" fill="#fff3cd" stroke="#f39c12" stroke-width="2" rx="5"/>
  <text x="130" y="160" text-anchor="middle" font-size="12" fill="#e67e22">Clean Images</text>
  
  <!-- Data Augmentation -->
  <rect x="210" y="130" width="120" height="50" fill="#d1ecf1" stroke="#17a2b8" stroke-width="2" rx="5"/>
  <text x="270" y="155" text-anchor="middle" font-size="11" fill="#138496">Data Augmentation</text>
  <text x="270" y="170" text-anchor="middle" font-size="11" fill="#138496">(y₁, y₂)</text>
  
  <!-- Noise Addition -->
  <rect x="350" y="130" width="80" height="50" fill="#f8d7da" stroke="#dc3545" stroke-width="2" rx="5"/>
  <text x="390" y="155" text-anchor="middle" font-size="11" fill="#721c24">Noise</text>
  <text x="390" y="170" text-anchor="middle" font-size="11" fill="#721c24">(xₜₙ, xₜₙ₋₁)</text>
  
  <!-- Three Encoders -->
  <rect x="70" y="200" width="100" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="120" y="225" text-anchor="middle" font-size="11" fill="#155724">Encoder Eθ</text>
  <text x="120" y="240" text-anchor="middle" font-size="10" fill="#155724">(Online)</text>
  
  <rect x="190" y="200" width="100" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="240" y="225" text-anchor="middle" font-size="11" fill="#155724">Encoder Eθ₋</text>
  <text x="240" y="240" text-anchor="middle" font-size="10" fill="#155724">(Momentum)</text>
  
  <rect x="310" y="200" width="100" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="360" y="225" text-anchor="middle" font-size="11" fill="#155724">Encoder Esg(θ)</text>
  <text x="360" y="240" text-anchor="middle" font-size="10" fill="#155724">(Stop Grad)</text>
  
  <!-- Projectors -->
  <rect x="70" y="280" width="80" height="40" fill="#e2e3e5" stroke="#6c757d" stroke-width="2" rx="5"/>
  <text x="110" y="305" text-anchor="middle" font-size="11" fill="#495057">Projector Lθ</text>
  
  <rect x="190" y="280" width="80" height="40" fill="#e2e3e5" stroke="#6c757d" stroke-width="2" rx="5"/>
  <text x="230" y="305" text-anchor="middle" font-size="11" fill="#495057">Projector Lθ₋</text>
  
  <!-- Loss Functions -->
  <rect x="70" y="340" width="140" height="40" fill="#fff3cd" stroke="#ffc107" stroke-width="2" rx="5"/>
  <text x="140" y="365" text-anchor="middle" font-size="11" fill="#856404">Contrastive Loss</text>
  
  <rect x="230" y="340" width="180" height="40" fill="#fff3cd" stroke="#ffc107" stroke-width="2" rx="5"/>
  <text x="320" y="365" text-anchor="middle" font-size="11" fill="#856404">Representation Consistency Loss</text>
  
  <!-- Stage 2: Fine-tuning -->
  <rect x="550" y="80" width="400" height="320" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2" rx="10"/>
  <text x="750" y="105" text-anchor="middle" font-size="16" font-weight="bold" fill="#7b1fa2">
    Stage 2: End-to-End Fine-tuning
  </text>
  
  <!-- Pre-trained Encoder -->
  <rect x="570" y="130" width="120" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="630" y="155" text-anchor="middle" font-size="12" fill="#155724">Pre-trained</text>
  <text x="630" y="170" text-anchor="middle" font-size="12" fill="#155724">Encoder Eθ</text>
  
  <!-- Random Decoder -->
  <rect x="710" y="130" width="120" height="60" fill="#ffeaa7" stroke="#fdcb6e" stroke-width="2" rx="5"/>
  <text x="770" y="155" text-anchor="middle" font-size="12" fill="#e17055">Random Init</text>
  <text x="770" y="170" text-anchor="middle" font-size="12" fill="#e17055">Decoder Dθ</text>
  
  <!-- Complete Model -->
  <rect x="620" y="220" width="160" height="50" fill="#dda0dd" stroke="#9370db" stroke-width="2" rx="5"/>
  <text x="700" y="250" text-anchor="middle" font-size="12" fill="#4b0082">Complete Model fθ</text>
  
  <!-- Training Options -->
  <rect x="570" y="300" width="140" height="50" fill="#ffe4e1" stroke="#ff6b6b" stroke-width="2" rx="5"/>
  <text x="640" y="320" text-anchor="middle" font-size="11" fill="#c0392b">Diffusion Training</text>
  <text x="640" y="335" text-anchor="middle" font-size="10" fill="#c0392b">(Equation 1)</text>
  
  <rect x="730" y="300" width="140" height="50" fill="#e4f3ff" stroke="#74b9ff" stroke-width="2" rx="5"/>
  <text x="800" y="320" text-anchor="middle" font-size="11" fill="#0984e3">Consistency Training</text>
  <text x="800" y="335" text-anchor="middle" font-size="10" fill="#0984e3">(Equation 5 + 9)</text>
  
  <!-- Key Components -->
  <rect x="50" y="450" width="900" height="120" fill="#f1f2f6" stroke="#57606f" stroke-width="2" rx="10"/>
  <text x="500" y="475" text-anchor="middle" font-size="16" font-weight="bold" fill="#2f3542">
    Key Technical Components
  </text>
  
  <!-- Component 1 -->
  <rect x="70" y="490" width="180" height="60" fill="#e8f8f5" stroke="#1dd1a1" stroke-width="2" rx="5"/>
  <text x="160" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#00a085">Representation Learning</text>
  <text x="160" y="525" text-anchor="middle" font-size="10" fill="#00a085">Visual semantics from</text>
  <text x="160" y="540" text-anchor="middle" font-size="10" fill="#00a085">clean & noisy images</text>
  
  <!-- Component 2 -->
  <rect x="270" y="490" width="180" height="60" fill="#fff4e6" stroke="#ff9f43" stroke-width="2" rx="5"/>
  <text x="360" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#e55039">Temperature Scheduling</text>
  <text x="360" y="525" text-anchor="middle" font-size="10" fill="#e55039">τ(t) = τ₁*(1-t) + τ₂*t</text>
  <text x="360" y="540" text-anchor="middle" font-size="10" fill="#e55039">Stable training</text>
  
  <!-- Component 3 -->
  <rect x="470" y="490" width="180" height="60" fill="#e8f4f8" stroke="#3c6382" stroke-width="2" rx="5"/>
  <text x="560" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#40739e">ODE Trajectory Alignment</text>
  <text x="560" y="525" text-anchor="middle" font-size="10" fill="#40739e">Points on same trajectory</text>
  <text x="560" y="540" text-anchor="middle" font-size="10" fill="#40739e">maintain consistency</text>
  
  <!-- Component 4 -->
  <rect x="670" y="490" width="180" height="60" fill="#f8e8ff" stroke="#8c7ae6" stroke-width="2" rx="5"/>
  <text x="760" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#6c5ce7">Auxiliary Contrastive Loss</text>
  <text x="760" y="525" text-anchor="middle" font-size="10" fill="#6c5ce7">For consistency models</text>
  <text x="760" y="540" text-anchor="middle" font-size="10" fill="#6c5ce7">Equation 9</text>
  
  <!-- Results -->
  <rect x="50" y="600" width="900" height="120" fill="#f8f9fa" stroke="#495057" stroke-width="2" rx="10"/>
  <text x="500" y="625" text-anchor="middle" font-size="16" font-weight="bold" fill="#343a40">
    Achievements
  </text>
  
  <!-- Result 1 -->
  <rect x="70" y="645" width="200" height="60" fill="#d1f2eb" stroke="#27ae60" stroke-width="2" rx="5"/>
  <text x="170" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#27ae60">Diffusion Model</text>
  <text x="170" y="680" text-anchor="middle" font-size="10" fill="#27ae60">FID 2.04 (ImageNet-256)</text>
  <text x="170" y="695" text-anchor="middle" font-size="10" fill="#27ae60">75 NFE, SOTA pixel-space</text>
  
  <!-- Result 2 -->
  <rect x="290" y="645" width="200" height="60" fill="#ebf3fd" stroke="#3498db" stroke-width="2" rx="5"/>
  <text x="390" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#3498db">Consistency Model</text>
  <text x="390" y="680" text-anchor="middle" font-size="10" fill="#3498db">FID 8.82 (ImageNet-256)</text>
  <text x="390" y="695" text-anchor="middle" font-size="10" fill="#3498db">Single-step generation</text>
  
  <!-- Result 3 -->
  <rect x="510" y="645" width="200" height="60" fill="#fef9e7" stroke="#f39c12" stroke-width="2" rx="5"/>
  <text x="610" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#f39c12">High Resolution</text>
  <text x="610" y="680" text-anchor="middle" font-size="10" fill="#f39c12">FID 2.35 (ImageNet-512)</text>
  <text x="610" y="695" text-anchor="middle" font-size="10" fill="#f39c12">Efficient scaling</text>
  
  <!-- Result 4 -->
  <rect x="730" y="645" width="200" height="60" fill="#f4ecf7" stroke="#9b59b6" stroke-width="2" rx="5"/>
  <text x="830" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#9b59b6">Training Efficiency</text>
  <text x="830" y="680" text-anchor="middle" font-size="10" fill="#9b59b6">Competitive with VAE-based</text>
  <text x="830" y="695" text-anchor="middle" font-size="10" fill="#9b59b6">No external models</text>
  
  <!-- Flow indicators -->
  <polygon points="450,240 470,250 450,260" fill="#34495e"/>
  <text x="480" y="255" font-size="12" fill="#34495e">Transfer</text>
  
  <!-- Vertical flow in pre-training -->
  <polygon points="245,190 250,210 255,190" fill="#3498db"/>
  <polygon points="245,270 250,290 255,270" fill="#3498db"/>
  <polygon points="245,330 250,350 255,330" fill="#3498db"/>
  
  <!-- Vertical flow in fine-tuning -->
  <polygon points="745,200 750,220 755,200" fill="#9c27b0"/>
  <polygon points="745,280 750,300 755,280" fill="#9c27b0"/>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Two-stage training with self-supervised pre-training of encoders">
                        <div class="quiz-question">1. What is the main innovation in the paper's training framework compared to traditional approaches?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Using VAEs for latent space compression">Using VAEs for latent space compression</div><div class="quiz-choice" data-value="Two-stage training with self-supervised pre-training of encoders">Two-stage training with self-supervised pre-training of encoders</div><div class="quiz-choice" data-value="Single-stage end-to-end training with larger models">Single-stage end-to-end training with larger models</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Successfully training without VAEs or pre-trained diffusion models">
                        <div class="quiz-question">2. What is the most impressive achievement of the paper's consistency model variant?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Achieving 2.04 FID score on ImageNet-256">Achieving 2.04 FID score on ImageNet-256</div><div class="quiz-choice" data-value="Successfully training without VAEs or pre-trained diffusion models">Successfully training without VAEs or pre-trained diffusion models</div><div class="quiz-choice" data-value="Using only 32 sampling steps for generation">Using only 32 sampling steps for generation</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="By decomposing training into semantic learning and pixel generation stages">
                        <div class="quiz-question">3. How does the paper's approach improve the training efficiency?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By using more powerful GPUs and larger batch sizes">By using more powerful GPUs and larger batch sizes</div><div class="quiz-choice" data-value="By compressing images into latent space representations">By compressing images into latent space representations</div><div class="quiz-choice" data-value="By decomposing training into semantic learning and pixel generation stages">By decomposing training into semantic learning and pixel generation stages</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/argyle.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>Scaling Language-Centric Omnimodal Representation Learning</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-10-13</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2510.11693" target="_blank">http://arxiv.org/pdf/2510.11693</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> Language-centric omnimodal representation learning in multimodal large language models (MLLMs), focusing on cross-modal alignment and embedding capabilities.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> Based on previous CLIP-style and MLLM-based embedding approaches, proposing that MLLMs achieve implicit cross-modal alignment during generative pretraining, allowing for lightweight contrastive learning refinement.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> Understanding why MLLM-based embedding approaches outperform traditional CLIP-based models and developing more efficient methods for cross-modal representation learning.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> Developed LCO-EMB framework using language-centric paired data for contrastive learning refinement, analyzed through anisotropy and kernel similarity studies, and validated on various benchmarks.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> Achieved state-of-the-art performance across diverse modalities and benchmarks, discovered a Generation-Representation Scaling Law showing representation capabilities scale with generative abilities, and validated findings on a challenging visual-document retrieval task.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Scaling Language-Centric Omnimodal Representation Learning</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="18" font-weight="bold" fill="#2c3e50">
    LCO-EMB: Language-Centric Omnimodal Representation Learning
  </text>
  
  <!-- Stage 1: Analysis -->
  <rect x="50" y="60" width="200" height="100" rx="10" fill="#e8f4f8" stroke="#3498db" stroke-width="2"/>
  <text x="150" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Analysis Phase</text>
  <text x="150" y="105" text-anchor="middle" font-size="10" fill="#34495e">Anisotropy Analysis</text>
  <text x="150" y="120" text-anchor="middle" font-size="10" fill="#34495e">Kernel Similarity</text>
  <text x="150" y="135" text-anchor="middle" font-size="10" fill="#34495e">Cross-modal Alignment</text>
  
  <!-- Stage 2: MLLM Backbone -->
  <rect x="300" y="60" width="180" height="100" rx="10" fill="#fff2e6" stroke="#e67e22" stroke-width="2"/>
  <text x="390" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">MLLM Backbone</text>
  <text x="390" y="105" text-anchor="middle" font-size="10" fill="#34495e">LLaVA-Next / Qwen2.5-VL</text>
  <text x="390" y="120" text-anchor="middle" font-size="10" fill="#34495e">Qwen2.5-Omni</text>
  <text x="390" y="135" text-anchor="middle" font-size="10" fill="#34495e">Pretrained Alignment</text>
  
  <!-- Stage 3: Text-only CL -->
  <rect x="530" y="60" width="180" height="100" rx="10" fill="#e8f5e8" stroke="#27ae60" stroke-width="2"/>
  <text x="620" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Text-only CL</text>
  <text x="620" y="105" text-anchor="middle" font-size="10" fill="#34495e">LoRA Fine-tuning</text>
  <text x="620" y="120" text-anchor="middle" font-size="10" fill="#34495e">NLI / Scale-1M</text>
  <text x="620" y="135" text-anchor="middle" font-size="10" fill="#34495e">Minimal Perturbation</text>
  
  <!-- Stage 4: Multimodal Refinement -->
  <rect x="750" y="60" width="180" height="100" rx="10" fill="#f4e8f8" stroke="#9b59b6" stroke-width="2"/>
  <text x="840" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Multimodal Refinement</text>
  <text x="840" y="105" text-anchor="middle" font-size="10" fill="#34495e">94k Synthetic Pairs</text>
  <text x="840" y="120" text-anchor="middle" font-size="10" fill="#34495e">Task Space Calibration</text>
  <text x="840" y="135" text-anchor="middle" font-size="10" fill="#34495e">Optional Enhancement</text>
  
  <!-- Data Sources -->
  <rect x="50" y="200" width="280" height="80" rx="8" fill="#fdf2e9" stroke="#d68910" stroke-width="1"/>
  <text x="190" y="220" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Training Data Sources</text>
  <text x="190" y="240" text-anchor="middle" font-size="9" fill="#34495e">• MNLI + SNLI (276k) • Scale-1M (1M pairs)</text>
  <text x="190" y="255" text-anchor="middle" font-size="9" fill="#34495e">• Visual Documents • Retrieval & Compositionality</text>
  <text x="190" y="270" text-anchor="middle" font-size="9" fill="#34495e">• Multilingual Data • Synthetic Samples</text>
  
  <!-- Generation-Representation Scaling Law -->
  <rect x="370" y="200" width="260" height="80" rx="8" fill="#eaf2f8" stroke="#2980b9" stroke-width="1"/>
  <text x="500" y="220" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Generation-Representation Scaling Law</text>
  <text x="500" y="240" text-anchor="middle" font-size="9" fill="#34495e">Generative Quality ∝ Representation Performance</text>
  <text x="500" y="255" text-anchor="middle" font-size="9" fill="#34495e">PAC-Bayesian Bound: L_pop ≤ log(N) - I_P(X;Y) + ε_P</text>
  <text x="500" y="270" text-anchor="middle" font-size="9" fill="#34495e">Theoretical Justification</text>
  
  <!-- Evaluation Benchmarks -->
  <rect x="670" y="200" width="280" height="80" rx="8" fill="#e8f8f5" stroke="#16a085" stroke-width="1"/>
  <text x="810" y="220" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Evaluation Benchmarks</text>
  <text x="810" y="240" text-anchor="middle" font-size="9" fill="#34495e">• MIEB-Lite (51 tasks) • Audio-Text: AudioCaps, Clotho</text>
  <text x="810" y="255" text-anchor="middle" font-size="9" fill="#34495e">• Video-Text: MSR-VTT, ActivityNet</text>
  <text x="810" y="270" text-anchor="middle" font-size="9" fill="#34495e">• SeaDoc (Cross-lingual Document Retrieval)</text>
  
  <!-- Key Components -->
  <rect x="100" y="320" width="160" height="60" rx="8" fill="#fff5f5" stroke="#e74c3c" stroke-width="1"/>
  <text x="180" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Modality Encoders</text>
  <text x="180" y="355" text-anchor="middle" font-size="9" fill="#34495e">Vision / Audio</text>
  <text x="180" y="370" text-anchor="middle" font-size="9" fill="#34495e">(Frozen)</text>
  
  <rect x="300" y="320" width="160" height="60" rx="8" fill="#f0f8ff" stroke="#3498db" stroke-width="1"/>
  <text x="380" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Projector</text>
  <text x="380" y="355" text-anchor="middle" font-size="9" fill="#34495e">Alignment Layer</text>
  <text x="380" y="370" text-anchor="middle" font-size="9" fill="#34495e">(Frozen)</text>
  
  <rect x="500" y="320" width="160" height="60" rx="8" fill="#f0fff0" stroke="#27ae60" stroke-width="1"/>
  <text x="580" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Language Decoder</text>
  <text x="580" y="355" text-anchor="middle" font-size="9" fill="#34495e">LLM Backbone</text>
  <text x="580" y="370" text-anchor="middle" font-size="9" fill="#34495e">(LoRA Tuned)</text>
  
  <rect x="700" y="320" width="160" height="60" rx="8" fill="#faf0e6" stroke="#d68910" stroke-width="1"/>
  <text x="780" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Embeddings</text>
  <text x="780" y="355" text-anchor="middle" font-size="9" fill="#34495e">Unified Space</text>
  <text x="780" y="370" text-anchor="middle" font-size="9" fill="#34495e">Similarity Matching</text>
  
  <!-- Results Section -->
  <rect x="50" y="420" width="900" height="100" rx="10" fill="#f8f9fa" stroke="#6c757d" stroke-width="2"/>
  <text x="500" y="445" text-anchor="middle" font-size="14" font-weight="bold" fill="#2c3e50">Key Results & Achievements</text>
  
  <text x="100" y="470" font-size="10" fill="#34495e">• State-of-the-art on MIEB-Lite: 68.8% (LCO-EMB-Omni 7B)</text>
  <text x="100" y="485" font-size="10" fill="#34495e">• 21× less training data than competing methods</text>
  <text x="100" y="500" font-size="10" fill="#34495e">• Text-only variants outperform advanced baselines</text>
  
  <text x="500" y="470" font-size="10" fill="#34495e">• Discovers latent cross-modal alignment in MLLMs</text>
  <text x="500" y="485" font-size="10" fill="#34495e">• Establishes Generation-Representation Scaling Law</text>
  <text x="500" y="500" font-size="10" fill="#34495e">• Generalizes across vision, audio, and video modalities</text>
  
  <!-- Theoretical Foundation -->
  <rect x="50" y="550" width="900" height="80" rx="10" fill="#f4f1fb" stroke="#8e44ad" stroke-width="2"/>
  <text x="500" y="575" text-anchor="middle" font-size="14" font-weight="bold" fill="#2c3e50">Theoretical Foundation</text>
  
  <text x="150" y="600" font-size="10" fill="#34495e">Generative Bottleneck:</text>
  <text x="150" y="615" font-size="9" fill="#34495e">log(N) - I_P(X;Y)</text>
  
  <text x="350" y="600" font-size="10" fill="#34495e">Optimization Inefficiency:</text>
  <text x="350" y="615" font-size="9" fill="#34495e">ε_P (minimized by strong prior)</text>
  
  <text x="550" y="600" font-size="10" fill="#34495e">Fine-tuning Cost:</text>
  <text x="550" y="615" font-size="9" fill="#34495e">√(KL(Q||P) + log(1/δ))/2n</text>
  
  <text x="750" y="600" font-size="10" fill="#34495e">LoRA Justification:</text>
  <text x="750" y="615" font-size="9" fill="#34495e">Keeps KL(Q||P) small</text>
  
  <!-- Innovation Highlights -->
  <rect x="50" y="660" width="280" height="100" rx="8" fill="#e8f5e8" stroke="#27ae60" stroke-width="1"/>
  <text x="190" y="680" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Key Innovations</text>
  <text x="190" y="700" text-anchor="middle" font-size="9" fill="#34495e">• Language-centric training paradigm</text>
  <text x="190" y="715" text-anchor="middle" font-size="9" fill="#34495e">• Minimal multimodal data requirement</text>
  <text x="190" y="730" text-anchor="middle" font-size="9" fill="#34495e">• Preservation of generative capabilities</text>
  <text x="190" y="745" text-anchor="middle" font-size="9" fill="#34495e">• Cross-modal generalization</text>
  
  <!-- Validation -->
  <rect x="360" y="660" width="280" height="100" rx="8" fill="#fdf2e9" stroke="#d68910" stroke-width="1"/>
  <text x="500" y="680" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Empirical Validation</text>
  <text x="500" y="700" text-anchor="middle" font-size="9" fill="#34495e">• Multiple MLLM backbones tested</text>
  <text x="500" y="715" text-anchor="middle" font-size="9" fill="#34495e">• Comprehensive benchmark evaluation</text>
  <text x="500" y="730" text-anchor="middle" font-size="9" fill="#34495e">• SeaDoc challenge task validation</text>
  <text x="500" y="745" text-anchor="middle" font-size="9" fill="#34495e">• Ablation studies on LoRA settings</text>
  
  <!-- Future Directions -->
  <rect x="670" y="660" width="280" height="100" rx="8" fill="#eaf2f8" stroke="#2980b9" stroke-width="1"/>
  <text x="810" y="680" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Impact & Applications</text>
  <text x="810" y="700" text-anchor="middle" font-size="9" fill="#34495e">• Efficient multimodal representation</text>
  <text x="810" y="715" text-anchor="middle" font-size="9" fill="#34495e">• Low-resource language support</text>
  <text x="810" y="730" text-anchor="middle" font-size="9" fill="#34495e">• Document understanding advances</text>
  <text x="810" y="745" text-anchor="middle" font-size="9" fill="#34495e">• Scalable training paradigm</text>
  
  <!-- Flow connections (simplified lines) -->
  <line x1="250" y1="110" x2="300" y2="110" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="110" x2="530" y2="110" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="710" y1="110" x2="750" y2="110" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="260" y1="320" x2="300" y2="320" stroke="#7f8c8d" stroke-width="1"/>
  <line x1="460" y1="320" x2="500" y2="320" stroke="#7f8c8d" stroke-width="1"/>
  <line x1="660" y1="320" x2="700" y2="320" stroke="#7f8c8d" stroke-width="1"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#34495e"/>
    </marker>
  </defs>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="MLLMs achieve implicit cross-modal alignment during generative pretraining">
                        <div class="quiz-question">1. What is the key insight about MLLMs that the paper discovers?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="MLLMs require extensive contrastive learning to achieve cross-modal alignment">MLLMs require extensive contrastive learning to achieve cross-modal alignment</div><div class="quiz-choice" data-value="MLLMs achieve implicit cross-modal alignment during generative pretraining">MLLMs achieve implicit cross-modal alignment during generative pretraining</div><div class="quiz-choice" data-value="MLLMs cannot perform as well as CLIP-based models in embedding tasks">MLLMs cannot perform as well as CLIP-based models in embedding tasks</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Representational capabilities scale positively with generative abilities">
                        <div class="quiz-question">2. What novel scaling law does the paper identify?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Model size directly correlates with embedding quality">Model size directly correlates with embedding quality</div><div class="quiz-choice" data-value="Training data size determines representation capabilities">Training data size determines representation capabilities</div><div class="quiz-choice" data-value="Representational capabilities scale positively with generative abilities">Representational capabilities scale positively with generative abilities</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="To preserve the latent cross-modal alignment while enhancing representation capability">
                        <div class="quiz-question">3. Why does the paper's LCO-EMB framework use LoRA for fine-tuning?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="To reduce computational costs during training">To reduce computational costs during training</div><div class="quiz-choice" data-value="To preserve the latent cross-modal alignment while enhancing representation capability">To preserve the latent cross-modal alignment while enhancing representation capability</div><div class="quiz-choice" data-value="To enable training on larger batch sizes">To enable training on larger batch sizes</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 创建遮罩层
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // 获取所有问题标签
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // 设置点击事件处理
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // 点击标签切换问题卡的显示状态
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // 阻止事件冒泡
                    
                    // 如果当前问题卡已经显示，则隐藏它
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // 先隐藏所有其他问题卡
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // 将弹窗内容复制到页面最外层的弹窗中
                        document.body.appendChild(popup);
                        
                        // 显示当前问题卡和背景遮罩
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // 确保点击问题卡内部时不会关闭问题卡
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // 点击遮罩层或页面任何其他位置时隐藏所有问题卡
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // 为每个选项添加点击事件
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // 重置所有选项
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // 标记当前选项为已选
                    this.classList.add('selected');
                    
                    // 检查是否正确
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '✔️ Correct！';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '❌ Wrong！';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // 卡片轮播功能 - 新增
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // 更新计数器显示
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // 显示指定索引的卡片
                function showCard(index) {
                    // 处理循环
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // 更新当前索引
                    currentIndex = index;
                    
                    // 更新卡片显示
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // 更新计数器
                    updateCounter();
                }
                
                // 下一张卡片
                function nextCard(e) {
                    e.stopPropagation(); // 防止事件冒泡导致问题卡关闭
                    showCard(currentIndex + 1);
                }
                
                // 为卡片容器添加点击事件
                cardDeck.addEventListener('click', function(e) {
                    // 检查点击是否发生在流程图卡片内部的滚动区域
                    // 如果是在滚动条上点击，不切换卡片
                    const targetCard = e.target.closest('.paper-card');
                    if (targetCard && targetCard.classList.contains('flowchart-card')) {
                        // 计算点击位置是否在滚动条区域
                        const rect = targetCard.getBoundingClientRect();
                        const isScrollbarClick = 
                            (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                            (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);
                        
                        if (!isScrollbarClick) {
                            nextCard(e);
                        }
                    } else {
                        nextCard(e);
                    }
                });
                
                // 键盘导航
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
</body>
</html>
