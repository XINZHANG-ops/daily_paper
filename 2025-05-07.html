
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-05-07 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* 卡片容器样式 - 新增 */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            height: 600px; /* 固定高度 */
            cursor: pointer; /* 增加指针样式提示可点击 */
        }
        
        /* 卡片通用样式 */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
        }
        
        /* 轮播卡片样式 - 新增 */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%;
            transition: transform 0.5s ease, opacity 0.5s ease;
        }
        
        /* 非激活卡片的样式 - 新增 */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* 激活卡片的样式 - 新增 */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* 第一张卡片（文本内容）不需要滚动 */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* 第二张卡片（流程图）支持滚动 */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow: auto !important;
            padding-bottom: 50px; /* 添加底部填充 */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-height: none; /* 移除任何高度限制 */
        }
        
        /* 传统卡片样式 */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        
        .paper-card p {
            margin: 5px 0;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* 卡片计数器 - 新增 */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* 改为固定定位，不随滚动而移动 */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* 居中显示 */
            width: 90%;
            max-width: 500px; /* 增加最大宽度，适应长内容 */
            max-height: 80vh; /* 限制最大高度 */
            overflow-y: auto; /* 内容过多时可滚动 */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* 确保显示在最上层 */
        }
        
        /* 添加遮罩层，防止问题卡被其他内容遮挡 */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* 使用JavaScript控制问题卡的显示和隐藏，不再使用hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* 确保长单词自动换行 */
            overflow-wrap: break-word;
            hyphens: auto; /* 在必要时使用连字符 */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* 确保长单词自动换行 */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* 长文本左对齐 */
            display: block; /* 确保是块级元素 */
            white-space: normal; /* 允许自动换行 */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* 长文本选项的特殊样式 */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* 确保弹窗中的按钮文本不会溢出 */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* 适应超长选项文本 */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        @media (max-width: 768px) {
            .paper-container {
                flex-direction: column;
            }
            
            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                height: 650px; /* 移动设备上高度调整 */
            }
            
            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }
            
            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }
            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
            }
        }
    </style>
</head>
<body>
    <h1>2025-05-07 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/woven.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-05-06</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2505.03318" target="_blank">http://arxiv.org/pdf/2505.03318</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper introduces a unified multimodal Chain-of-Thought (CoT) reward model for evaluating both visual understanding and generation tasks in AI.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> Based on previous multimodal reward models that provided direct or shallow reasoning responses, this paper proposes incorporating explicit long chain-of-thought reasoning to enhance reliability and robustness.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper addresses the limitation of current reward models that lack rigorous logical structure and deep analysis capabilities, often leading to inaccurate reward signals in complex scenarios.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> The authors use a three-stage approach: cold start with GPT-4o distillation for initial CoT format learning, rejection sampling for generalization, and Group Relative Policy Optimization (GRPO) for reinforcement fine-tuning.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> The model demonstrated superior performance across various vision tasks, showing that incorporating long CoT reasoning significantly improved reward signal accuracy and enabled better implicit reasoning capabilities even without explicit reasoning traces.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Unified Multimodal Chain-of-Thought Reward Model through Reinforcement
  Fine-Tuning</h2>
                        <svg width="100%" viewBox="0 0 1000 800" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .title-text { font-family: '
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Their lack of rigorous logical structure and capacity for multi-dimensional, deep reasoning.">
                        <div class="quiz-question">1. What is the primary limitation of existing multimodal reward models that UNIFIED REWARD-THINK addresses?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Their inability to handle video generation tasks.">Their inability to handle video generation tasks.</div><div class="quiz-choice" data-value="Their lack of rigorous logical structure and capacity for multi-dimensional, deep reasoning.">Their lack of rigorous logical structure and capacity for multi-dimensional, deep reasoning.</div><div class="quiz-choice" data-value="Their reliance on outdated visual recognition techniques.">Their reliance on outdated visual recognition techniques.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Group Relative Policy Optimization (GRPO)">
                        <div class="quiz-question">2. Which reinforcement learning technique is used in the final stage of the UNIFIED REWARD-THINK training pipeline to enhance reasoning capabilities?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Proximal Policy Optimization (PPO)">Proximal Policy Optimization (PPO)</div><div class="quiz-choice" data-value="Deep Q-Networks (DQN)">Deep Q-Networks (DQN)</div><div class="quiz-choice" data-value="Group Relative Policy Optimization (GRPO)">Group Relative Policy Optimization (GRPO)</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="They are strengthened, leading to better performance even without explicit CoT traces.">
                        <div class="quiz-question">3. According to the paper, what happens to the model's implicit reasoning capabilities after it has mastered explicit Chain-of-Thought reasoning?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="They remain unchanged, only explicit reasoning improves.">They remain unchanged, only explicit reasoning improves.</div><div class="quiz-choice" data-value="They weaken, making the model rely solely on explicit CoT.">They weaken, making the model rely solely on explicit CoT.</div><div class="quiz-choice" data-value="They are strengthened, leading to better performance even without explicit CoT traces.">They are strengthened, leading to better performance even without explicit CoT traces.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/my-little-plaid-dark.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-05-05</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2505.02835" target="_blank">http://arxiv.org/pdf/2505.02835</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper focuses on developing a multimodal reward model (R1-Reward) through reinforcement learning, operating in the domain of multimodal large language models and reward modeling.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> Previous research focused on improving reward models through data and structural aspects, while this paper introduces a novel approach of using reinforcement learning to enhance reward modeling performance and long-term reasoning capabilities.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper addresses the challenge of training stable and effective multimodal reward models, particularly focusing on issues with training instability, advantage normalization limitations, and inconsistencies between reasoning and results in existing approaches.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> The authors developed StableReinforce algorithm with pre-clipping, advantage filtering, and consistency rewards, combined with a progressive difficulty training strategy using 200K preference data samples collected from diverse datasets.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> R1-Reward achieved significant improvements over previous state-of-the-art models: 8.4% improvement on VL Reward-Bench, 14.3% improvement on Multimodal Reward Bench, and superior performance on MM-RLHF Reward Bench, with further enhancements through inference compute scaling.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement
  Learning</h2>
                        <svg width="100%" viewBox="0 0 1000 800" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#555" />
    </marker>
    <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
      <feGaussianBlur in="SourceAlpha" stdDeviation="3"/>
      <feOffset dx="2" dy="2" result="offsetblur"/>
      <feMerge>
        <feMergeNode/>
        <feMergeNode in="SourceGraphic"/>
      </feMerge>
    </filter>
  </defs>

  <style>
    .title-text { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-weight: bold; fill: #2c3e50; text-anchor: middle; }
    .section-title { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-weight: bold; fill: #ffffff; text-anchor: middle; }
    .section-title-dark { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-weight: bold; fill: #333333; text-anchor: middle; }
    .content-text { font-family: 'Segoe UI', Arial, sans-serif; fill: #ffffff; }
    .content-text-dark { font-family: 'Segoe UI', Arial, sans-serif; fill: #333333; }
    .content-text-small { font-size: 11px; }
    .content-text-medium { font-size: 12px; }
    .content-text-bold { font-weight: bold; }
    .box { stroke: #333; stroke-width: 1px; filter: url(#shadow); }
  </style>

  <rect width="100%" height="100%" fill="#f0f4f8"/>

  <!-- Title -->
  <text x="500" y="35" class="title-text" font-size="24px">R1-Reward: Method Flowchart</text>

  <!-- Problem Block -->
  <g>
    <rect x="150" y="60" width="700" height="80" rx="10" ry="10" fill="#e74c3c" class="box"/>
    <text x="500" y="80" class="section-title" font-size="16px">Problem: Limitations in MRM & RL Training</text>
    <text x="170" y="100" class="content-text content-text-medium">
      <tspan x="170" dy="0em">- Existing RL (PPO, Reinforce++) instability for reward modeling.</tspan>
      <tspan x="170" dy="1.2em">- Advantage Normalization issues with low-variance rewards.</tspan>
      <tspan x="170" dy="1.2em">- Inconsistency between model's reasoning and final judgment.</tspan>
    </text>
  </g>

  <!-- Goal Block -->
  <g>
    <rect x="250" y="155" width="500" height="45" rx="10" ry="10" fill="#f1c40f" class="box"/>
    <text x="500" y="182" class="section-title-dark" font-size="15px">Goal: Enhance MRM Reasoning via Stable Reinforcement Learning</text>
  </g>

  <!-- R1-Reward Training Pipeline Block -->
  <g>
    <rect x="40" y="215" width="920" height="430" rx="15" ry="15" fill="#d6eaf8" class="box"/>
    <text x="500" y="240" class="section-title-dark" font-size="18px" style="fill:#2980b9;">R1-Reward Training Pipeline</text>

    <!-- Step 1: Data Prep & SFT -->
    <g>
      <rect x="60" y="260" width="880" height="95" rx="8" ry="8" fill="#3498db" class="box"/>
      <text x="500" y="280" class="section-title" font-size="14px">Step 1: Data Preparation & SFT (Cold Start)</text>
      <text x="75" y="300" class="content-text content-text-medium">
        <tspan x="75" dy="0em">- Collect 200K preference pairs (R1-Reward-200K dataset).</tspan>
        <tspan x="75" dy="1.2em">- GPT-4o generates "thinking processes" (Long-CoT) & records sample difficulty.</tspan>
        <tspan x="75" dy="1.2em">- Supervised Fine-Tuning (SFT) of base MLLM (QwenVL-2.5-7B-Instruct) for task familiarization.</tspan>
      </text>
    </g>

    <!-- Step 2: RL Training Data Selection -->
    <g>
      <rect x="60" y="365" width="880" height="55" rx="8" ry="8" fill="#1abc9c" class="box"/>
      <text x="500" y="383" class="section-title" font-size="14px">Step 2: RL Training Data Selection</text>
      <text x="75" y="403" class="content-text content-text-medium">
        <tspan x="75" dy="0em">- Select difficult samples (e.g., GPT-4o required ≥2 attempts or failed).</tspan>
      </text>
    </g>

    <!-- Step 3: RL Training with StableReinforce -->
    <g>
      <rect x="60" y="430" width="880" height="185" rx="8" ry="8" fill="#2
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these in MRMs.">
                        <div class="quiz-question">1. What is the primary limitation of existing Multimodal Reward Model (MRM) research that the R1-Reward paper aims to address using Reinforcement Learning (RL)?</div>
                        <div class="quiz-choices"><div class="quiz-choice long-text" data-value="Limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these in MRMs.">Limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these in MRMs.</div><div class="quiz-choice" data-value="The lack of diverse and large-scale multimodal preference datasets for training MRMs.">The lack of diverse and large-scale multimodal preference datasets for training MRMs.</div><div class="quiz-choice" data-value="Existing MRMs are computationally too expensive for practical use.">Existing MRMs are computationally too expensive for practical use.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Refinements to clipping operations and advantage normalization through Pre-CLIP and Advantage Filter.">
                        <div class="quiz-question">2. The StableReinforce algorithm, proposed in the paper to address training instability, includes which of the following key algorithmic modifications?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="A completely new neural network architecture for the reward head.">A completely new neural network architecture for the reward head.</div><div class="quiz-choice" data-value="A progressive difficulty training strategy based on data samples' difficulty.">A progressive difficulty training strategy based on data samples' difficulty.</div><div class="quiz-choice long-text" data-value="Refinements to clipping operations and advantage normalization through Pre-CLIP and Advantage Filter.">Refinements to clipping operations and advantage normalization through Pre-CLIP and Advantage Filter.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="By using a majority voting strategy over multiple inference samples.">
                        <div class="quiz-question">3. How does the paper demonstrate that R1-Reward can achieve further performance improvements with more inference compute?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By fine-tuning the model on additional data during the inference phase.">By fine-tuning the model on additional data during the inference phase.</div><div class="quiz-choice" data-value="By significantly reducing the model's parameter count for faster inference.">By significantly reducing the model's parameter count for faster inference.</div><div class="quiz-choice" data-value="By using a majority voting strategy over multiple inference samples.">By using a majority voting strategy over multiple inference samples.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/tileable-wood.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at
  Scale</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-05-05</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2505.03005" target="_blank">http://arxiv.org/pdf/2505.03005</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper presents RADLADS, a method for converting large language models from traditional transformer architectures to linear attention models in natural language processing.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> Based on previous work in model distillation and linear attention, it introduces new RWKV-variant architectures (RADFinch and RADGoose) and a more efficient conversion process requiring far fewer training tokens than previous methods.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper addresses the challenge of converting expensive transformer models to more efficient linear attention models while maintaining performance, as traditional training methods require prohibitive computational resources.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> Uses a 3-step process: attention weights transfer, attention hidden state alignment, and knowledge distillation, followed by fine-tuning, requiring only 350-700M tokens of training data.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> Achieved state-of-the-art performance for linear attention models across standard benchmarks, with converted models maintaining close to original transformer performance while requiring less than $2,000 USD in training costs for even the largest (72B parameter) model.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at
  Scale</h2>
                        <svg width="100%" viewBox="0 0 1000 1650" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style type="text/css">
      @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap');
      .title-text { font-family: 'Roboto', sans-serif; font-size: 32px; font-weight: 700; fill: #2C3E50; text-anchor: middle; }
      .box-title { font-family: 'Roboto', sans-serif; font-size: 20px; font-weight: 700; fill: #1A237E; text-anchor: middle; }
      .box-subtitle { font-family: 'Roboto', sans-serif; font-size: 16px; font-weight: 500; fill: #3F51B5; }
      .box-text { font-family: 'Roboto', sans-serif; font-size: 14px; fill: #37474F; }
      .box-text-small { font-family: 'Roboto', sans-serif; font-size: 12px; fill: #455A64; }
      .connector-line { stroke: #78909C; stroke-width: 2.5px; fill: none; }
      .connector-dot { fill: #78909C; }
    </style>
    <linearGradient id="gradInput" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#E1F5FE;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#B3E5FC;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="gradSetup" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#E8F5E9;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#C8E6C9;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="gradStep1" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#FFFDE7;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#FFF9C4;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="gradStep2" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#EDE7F6;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#D1C4E9;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="gradStep3" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#E0F7FA;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#B2EBF2;stop-opacity:1" />
    </linearGradient>
     <linearGradient id="gradAltStep3" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#FFF3E0;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#FFE0B2;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="gradOutput" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#F1F8E9;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#DCEDC8;stop-opacity:1" />
    </linearGradient>
  </defs>

  <rect width="100%" height="100%" fill="#F4F6F8"/>

  <text x="500" y="50" class="title-text">RADLADS Conversion Protocol</text>

  <!-- Variables for layout -->
  <script>
    let y_cursor = 80;
    const box_width = 550;
    const box_x = (1000 - box_width) / 2; // 225
    const line_x = 500;
    const space_between_boxes = 70;
    const text_margin_x = 20;
    const text_start_y_offset = 35;
    const line_height = 18;
    const dot_radius = 5;

    function drawConnector(y1, y2) {
      const g = document.createElementNS("http://www.w3.org/2000/svg", "g");
      const line = document.createElementNS("http://www.w3.org/2000/svg", "line");
      line.setAttribute("x1", line_x);
      line.setAttribute("y1", y1);
      line.setAttribute("x2", line_x);
      line.setAttribute("y2", y2);
      line.setAttribute("class", "connector-line");
      g.appendChild(line);
      
      const dot1 = document.createElementNS("http://www.w3.org/2000/svg", "circle");
      dot1.setAttribute("cx", line_x);
      dot1.setAttribute("cy", y1);
      dot1.setAttribute("r", dot_radius);
      dot1.setAttribute("class", "connector-dot");
      g.appendChild(dot1);
      
      const dot2 = document.createElementNS("http://www.w3.org/2000/svg", "circle");
      dot2.setAttribute("cx", line_x);
      dot2.setAttribute("cy", y2);
      dot2.setAttribute("r", dot_radius);
      dot2.setAttribute("class", "connector-dot");
      g.appendChild(dot2);
      return g;
    }
  </script>

  <!-- Input Model -->
  <g id="input_model">
    <rect x="${box_x}" y="${y_cursor}" width="${box_width}" height="90" rx="15" ry="15" fill="url(#gradInput)" stroke="#90CAF9" stroke-width="1.5"/>
    <text x="500" y="${y_cursor + text_start_y_offset}" class="box-title">Input: Pre-trained Teacher Model</text>
    <text x="${box_x + text_margin_x}" y="${y_cursor + text_start_y_offset + line_height * 1.5}" class="box-text">
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Type: Softmax Attention Transformer (e.g., Qwen2.5)</tspan>
    </text>
    <script>y_cursor += 90;</script>
  </g>
  
  <g transform="translate(0, ${y_cursor})">
    <path d="M ${line_x} 0 V ${space_between_boxes/2}" class="connector-line"/>
    <circle cx="${line_x}" cy="0" r="${dot_radius}" class="connector-dot"/>
    <circle cx="${line_x}" cy="${space_between_boxes/2}" r="${dot_radius}" class="connector-dot"/>
    <script>y_cursor += space_between_boxes/2;</script>
  </g>

  <!-- Setup Phase -->
  <g id="setup_phase" transform="translate(0, ${y_cursor})">
    <rect x="${box_x}" y="0" width="${box_width}" height="190" rx="15" ry="15" fill="url(#gradSetup)" stroke="#A5D6A7" stroke-width="1.5"/>
    <text x="500" y="${text_start_y_offset}" class="box-title">Setup: Attention Weights Transfer &amp; Student Init</text>
    <text class="box-text">
      <tspan x="${box_x + text_margin_x}" dy="${text_start_y_offset + line_height * 1.5}">Student Model Architecture:</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- MLPs &amp; Embeddings: Copied from Teacher.</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Attention Blocks: Replaced with recurrent mixers (e.g., RAD-RWKV6/7).</tspan>
      <tspan x="${box_x + text_margin_x}" dy="${line_height*1.5}">Weight Initialization:</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Attention (Wq, Wk, Wv, Wo): Transferred from Teacher to equivalent params.</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Other recurrent-specific weights: Standard pretraining init (e.g., 'w' in RWKV).</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Special weights (e.g., tokenshift): Init to mimic teacher, learnable.</tspan>
    </text>
    <script>y_cursor += 190;</script>
  </g>

  <g transform="translate(0, ${y_cursor})">
    <path d="M ${line_x} 0 V ${space_between_boxes}" class="connector-line"/>
    <circle cx="${line_x}" cy="0" r="${dot_radius}" class="connector-dot"/>
    <circle cx="${line_x}" cy="${space_between_boxes}" r="${dot_radius}" class="connector-dot"/>
    <script>y_cursor += space_between_boxes;</script>
  </g>
  
  <!-- Step 1 -->
  <g id="step_1" transform="translate(0, ${y_cursor})">
    <rect x="${box_x}" y="0" width="${box_width}" height="260" rx="15" ry="15" fill="url(#gradStep1)" stroke="#FFECB3" stroke-width="1.5"/>
    <text x="500" y="${text_start_y_offset}" class="box-title">Step 1: Attention Hidden State Alignment</text>
    <text class="box-text">
      <tspan x="${box_x + text_margin_x}" dy="${text_start_y_offset + line_height * 1.5}">Goal: Student recurrent attention layer outputs ≈ Teacher attention layer outputs.</tspan>
      <tspan x="${box_x + text_margin_x}" dy="${line_height*1.5}">Process:</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Frozen Teacher Model (for hidden states reference).</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Trainable Student recurrent attention layers (all layers at once).</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Loss: L2 Distance (or MSE) between student &amp; teacher hidden states.</tspan>
      <tspan x="${box_x + text_margin_x}" dy="${line_height*1.5}">Hyperparameters:</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Dataset: DCLM</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Tokens: 100M</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Sequence Length: 512</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Learning Rate: 1e-3 to 1e-5 (cosine anneal)</tspan>
      <tspan x="${box_x + text_margin_x}" dy="${line_height*1.5}">Output: Student model with aligned recurrent attention (teacher attention layers removed).</tspan>
    </text>
    <script>y_cursor += 260;</script>
  </g>

  <g transform="translate(0, ${y_cursor})">
    <path d="M ${line_x} 0 V ${space_between_boxes}" class="connector-line"/>
    <circle cx="${line_x}" cy="0" r="${dot_radius}" class="connector-dot"/>
    <circle cx="${line_x}" cy="${space_between_boxes}" r="${dot_radius}" class="connector-dot"/>
    <script>y_cursor += space_between_boxes;</script>
  </g>

  <!-- Step 2 -->
  <g id="step_2" transform="translate(0, ${y_cursor})">
    <rect x="${box_x}" y="0" width="${box_width}" height="230" rx="15" ry="15" fill="url(#gradStep2)" stroke="#B39DDB" stroke-width="1.5"/>
    <text x="500" y="${text_start_y_offset}" class="box-title">Step 2: Knowledge Distillation</text>
    <text class="box-text">
      <tspan x="${box_x + text_margin_x}" dy="${text_start_y_offset + line_height * 1.5}">Goal: Student model output logits ≈ Teacher model output logits.</tspan>
      <tspan x="${box_x + text_margin_x}" dy="${line_height*1.5}">Process:</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Frozen Teacher Model (for logits reference).</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Train all layers of the Student Model.</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">- Loss: Kullback-Leibler (KL) Divergence.</tspan>
      <tspan x="${box_x + text_margin_x}" dy="${line_height*1.5}">Hyperparameters:</tspan>
      <tspan x="${box_x + text_margin_x + 10}" dy="${line_height}">
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Hundreds of millions of tokens, less than 0.005% of the teacher's pre-training data.">
                        <div class="quiz-question">1. A key achievement of the RADLADS method highlighted in the paper is its efficiency in converting large transformer models. How many tokens are typically required for the conversion process?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Tens of trillions of tokens, similar to the original teacher model training.">Tens of trillions of tokens, similar to the original teacher model training.</div><div class="quiz-choice" data-value="Hundreds of billions of tokens, significantly less than pre-training but still substantial.">Hundreds of billions of tokens, significantly less than pre-training but still substantial.</div><div class="quiz-choice" data-value="Hundreds of millions of tokens, less than 0.005% of the teacher's pre-training data.">Hundreds of millions of tokens, less than 0.005% of the teacher's pre-training data.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Skipping Step 1 (Attention Hidden State Alignment) and starting directly with Step 2.">
                        <div class="quiz-question">2. The RADLADS protocol involves several steps. Which of the following approaches was explicitly found to *not* work well or resulted in significantly lower performance according to the paper's "What Did Not Work" section?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Using a cosine annealed learning rate during Step 1.">Using a cosine annealed learning rate during Step 1.</div><div class="quiz-choice" data-value="Skipping Step 1 (Attention Hidden State Alignment) and starting directly with Step 2.">Skipping Step 1 (Attention Hidden State Alignment) and starting directly with Step 2.</div><div class="quiz-choice" data-value="Using a flat learning rate during Step 2.">Using a flat learning rate during Step 2.</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="RAD-RWKV6 (RADFinch) and RAD-RWKV7 (RADGoose)">
                        <div class="quiz-question">3. The paper introduces two new RWKV-variant architectures used in the conversion process. What are they named?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="RAD-RWKV5 and RAD-RWKV6">RAD-RWKV5 and RAD-RWKV6</div><div class="quiz-choice" data-value="RAD-RWKV6 (RADFinch) and RAD-RWKV7 (RADGoose)">RAD-RWKV6 (RADFinch) and RAD-RWKV7 (RADGoose)</div><div class="quiz-choice" data-value="RWKV-A and RWKV-B">RWKV-A and RWKV-B</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 创建遮罩层
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // 获取所有问题标签
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // 设置点击事件处理
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // 点击标签切换问题卡的显示状态
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // 阻止事件冒泡
                    
                    // 如果当前问题卡已经显示，则隐藏它
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // 先隐藏所有其他问题卡
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // 将弹窗内容复制到页面最外层的弹窗中
                        document.body.appendChild(popup);
                        
                        // 显示当前问题卡和背景遮罩
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // 确保点击问题卡内部时不会关闭问题卡
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // 点击遮罩层或页面任何其他位置时隐藏所有问题卡
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // 为每个选项添加点击事件
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // 重置所有选项
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // 标记当前选项为已选
                    this.classList.add('selected');
                    
                    // 检查是否正确
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '✔️ Correct！';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '❌ Wrong！';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // 卡片轮播功能 - 新增
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // 更新计数器显示
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // 显示指定索引的卡片
                function showCard(index) {
                    // 处理循环
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // 更新当前索引
                    currentIndex = index;
                    
                    // 更新卡片显示
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // 更新计数器
                    updateCounter();
                }
                
                // 下一张卡片
                function nextCard(e) {
                    e.stopPropagation(); // 防止事件冒泡导致问题卡关闭
                    showCard(currentIndex + 1);
                }
                
                // 为卡片容器添加点击事件
                cardDeck.addEventListener('click', function(e) {
                    // 检查点击是否发生在流程图卡片内部的滚动区域
                    // 如果是在滚动条上点击，不切换卡片
                    const targetCard = e.target.closest('.paper-card');
                    if (targetCard && targetCard.classList.contains('flowchart-card')) {
                        // 计算点击位置是否在滚动条区域
                        const rect = targetCard.getBoundingClientRect();
                        const isScrollbarClick = 
                            (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                            (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);
                        
                        if (!isScrollbarClick) {
                            nextCard(e);
                        }
                    } else {
                        nextCard(e);
                    }
                });
                
                // 键盘导航
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
</body>
</html>
