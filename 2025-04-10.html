
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-04-10 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* 卡片容器样式 - 新增 */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            height: 600px; /* 固定高度 */
            cursor: pointer; /* 增加指针样式提示可点击 */
        }
        
        /* 卡片通用样式 */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
        }
        
        /* 轮播卡片样式 - 新增 */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%;
            transition: transform 0.5s ease, opacity 0.5s ease;
        }
        
        /* 非激活卡片的样式 - 新增 */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* 激活卡片的样式 - 新增 */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* 第一张卡片（文本内容）不需要滚动 */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* 第二张卡片（流程图）支持滚动 */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow: auto !important;
            padding-bottom: 50px; /* 添加底部填充 */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-height: none; /* 移除任何高度限制 */
        }
        
        /* 传统卡片样式 */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        
        .paper-card p {
            margin: 5px 0;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* 卡片计数器 - 新增 */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* 改为固定定位，不随滚动而移动 */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* 居中显示 */
            width: 90%;
            max-width: 500px; /* 增加最大宽度，适应长内容 */
            max-height: 80vh; /* 限制最大高度 */
            overflow-y: auto; /* 内容过多时可滚动 */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* 确保显示在最上层 */
        }
        
        /* 添加遮罩层，防止问题卡被其他内容遮挡 */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* 使用JavaScript控制问题卡的显示和隐藏，不再使用hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* 确保长单词自动换行 */
            overflow-wrap: break-word;
            hyphens: auto; /* 在必要时使用连字符 */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* 确保长单词自动换行 */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* 长文本左对齐 */
            display: block; /* 确保是块级元素 */
            white-space: normal; /* 允许自动换行 */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* 长文本选项的特殊样式 */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* 确保弹窗中的按钮文本不会溢出 */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* 适应超长选项文本 */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        @media (max-width: 768px) {
            .paper-container {
                flex-direction: column;
            }
            
            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                height: 650px; /* 移动设备上高度调整 */
            }
            
            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }
            
            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }
            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
            }
        }
    </style>
</head>
<body>
    <h1>2025-04-10 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/black-orchid.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of
  Photography</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-04-09</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2504.07083" target="_blank">http://arxiv.org/pdf/2504.07083</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper focuses on auto-regressive camera trajectory generation for cinematography, operating in the domain of computer vision and video production.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> The paper builds on previous trajectory generation methods that used geometric optimization, procedural systems, or diffusion models, but proposes a novel auto-regressive approach to generate more artistic and expressive camera movements.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper addresses the limitation of existing camera trajectory generation methods that lack artistic expression, directorial intent, and fine-grained textual alignment for creative video production.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> The authors introduce GenDoP, an auto-regressive model that treats camera parameters as discrete tokens and leverages a decoder-only Transformer architecture, conditioned on text descriptions and optional RGBD information.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> GenDoP outperforms state-of-the-art methods across fine-grained textual controllability, motion stability, and complexity metrics, with extensive human validation confirming its superior performance in generating artistic, expressive camera trajectories.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of
  Photography</h2>
                        <svg width="100%" viewBox="0 0 1000 800" xmlns="http://www.w3.org/2000/svg" font-family="Arial, sans-serif">

  <!-- Definitions for markers and gradients -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#555" />
    </marker>
    <linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(150,200,255);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(100,150,220);stop-opacity:1" />
    </linearGradient>
    <linearGradient id="grad2" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(150,255,150);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(100,220,100);stop-opacity:1" />
    </linearGradient>
     <linearGradient id="grad3" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(255,220,150);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(255,180,100);stop-opacity:1" />
    </linearGradient>
    <filter id="dropshadow" height="130%">
      <feGaussianBlur in="SourceAlpha" stdDeviation="3"/>
      <feOffset dx="2" dy="2" result="offsetblur"/>
      <feComponentTransfer>
        <feFuncA type="linear" slope="0.5"/>
      </feComponentTransfer>
      <feMerge>
        <feMergeNode/>
        <feMergeNode in="SourceGraphic"/>
      </feMerge>
    </filter>
  </defs>

  <!-- Title -->
  <text x="500" y="40" font-size="28" font-weight="bold" text-anchor="middle" fill="#333">GenDoP Methodology Flowchart</text>

  <!-- Section 1: DataDoP Dataset Construction -->
  <rect x="50" y="80" width="400" height="650" rx="15" ry="15" fill="url(#grad1)" stroke="#888" stroke-width="1" filter="url(#dropshadow)"/>
  <text x="250" y="110" font-size="20" font-weight="bold" text-anchor="middle" fill="#114">1. DataDoP Dataset Construction</text>

  <!-- DataDoP Steps -->
  <g transform="translate(70, 140)">
    <!-- Input -->
    <rect x="0" y="0" width="360" height="40" rx="5" ry="5" fill="#e0f0ff" stroke="#a0c0e0"/>
    <text x="180" y="25" text-anchor="middle" font-size="14" fill="#114">Input: Raw Videos (Movies, Documentaries)</text>

    <!-- Pre-processing -->
    <rect x="0" y="60" width="360" height="80" rx="5" ry="5" fill="#d0e8ff" stroke="#90b8d8"/>
    <text x="10" y="80" font-size="14" fill="#114" font-weight="bold">Pre-processing:</text>
    <text x="20" y="100" font-size="13" fill="#114">- Shot Segmentation (PySceneDetect)</text>
    <text x="20" y="115" font-size="13" fill="#114">- Quality/Semantic Filtering (Length, Light, GPT-4o Motion Type)</text>

    <!-- Trajectory Extraction -->
    <rect x="0" y="160" width="360" height="80" rx="5" ry="5" fill="#c0e0ff" stroke="#80b0d0"/>
    <text x="10" y="180" font-size="14" fill="#114" font-weight="bold">Trajectory Extraction & Refinement:</text>
    <text x="20" y="200" font-size="13" fill="#114">- Extract Pose & Depth (MonST3R)</text>
    <text x="20" y="215" font-size="13" fill="#114">- Clean, Smooth (Kalman), Interpolate Trajectories</text>

    <!-- Motion Tagging -->
    <rect x="0" y="260" width="360" height="100" rx="5" ry="5" fill="#b0d8ff" stroke="#70a8c8"/>
    <text x="10" y="280" font-size="14" fill="#114" font-weight="bold">Motion Tagging:</text>
    <text x="20" y="300" font-size="13" fill="#114">- Segment Trajectories</text>
    <text x="20" y="315" font-size="13" fill="#114">- Assign Tags: Translation (27 types) + Rotation (7 types)</text>
    <text x="20" y="330" font-size="13" fill="#114">- Combine & Smooth Tags</text>

    <!-- Caption Generation -->
    <rect x="0" y="380" width="360" height="100" rx="5" ry="5" fill="#a0d0ff" stroke="#60a0c0"/>
    <text x="10" y="400" font-size="14" fill="#114" font-weight="bold">Caption Generation (GPT-4o):</text>
    <text x="20" y="420" font-size="13" fill="#114">- Motion Captions (from Motion Tags)</text>
    <text x="20" y="435" font-size="13" fill="#114">- Directorial Captions (Tags + Scene Grid + Intent)</text>

    <!-- Output -->
    <rect x="0" y="500" width="360" height="60" rx="5" ry="5" fill="#90c8ff" stroke="#5098b8"/>
    <text x="180" y="525" text-anchor="middle" font-size="14" fill="#114" font-weight="bold">Output: DataDoP Dataset</text>
    <text x="180" y="545" text-anchor="middle" font-size="13" fill="#114">(Trajectories, RGBD Frames, Captions)</text>

    <!-- Arrows -->
    <line x1="180" y1="40" x2="180" y2="60" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/>
    <line x1="180" y1="140" x2="180" y2="160" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/>
    <line x1="180" y1="240" x2="180" y2="260" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/>
    <line x1="180" y1="360" x2="180" y2="380" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/>
    <line x1="180" y1="480" x2="180" y2="500" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/>
  </g>

  <!-- Section 2: GenDoP Trajectory Generation -->
  <rect x="500" y="80" width="450" height="650" rx="15" ry="15" fill="url(#grad2)" stroke="#888" stroke-width="1" filter="url(#dropshadow)"/>
  <text x="725" y="110" font-size="20" font-weight="bold" text-anchor="middle" fill="#141">2. GenDoP Trajectory Generation</text>

  <!-- GenDoP Steps -->
  <g transform="translate(520, 140)">
    <!-- Input -->
    <rect x="0" y="0" width="410" height="60" rx="5" ry="5" fill="#e0ffe0" stroke="#a0e0a0"/>
    <text x="205" y="25" text-anchor="middle" font-size="14" fill="#141">Input: Text Caption (Motion/Directorial)</text>
    <text x="205" y="45" text-anchor="middle" font-size="14" fill="#141">[Optional: Initial Frame RGBD]</text>

    <!-- Encoding -->
    <rect x="0" y="80" width="410" height="80" rx="5" ry="5" fill="#d0ffd0" stroke="#90d090"/>
    <text x="10" y="100" font-size="14" fill="#141" font-weight="bold">Multi-modal Encoding:</text>
    <text x="20" y="120" font-size="13" fill="#141">- Text Encoder (SD2.1 based)</text>
    <text x="20" y="135" font-size="13" fill="#141">- RGBD Encoders (CLIP Vision based)</text>
    <text x="205" y="155" text-anchor="middle" font-size="13" fill="#141">-> Concatenated Latent Code Z</text>

    <!-- Tokenization (Side block) -->
    <rect x="250" y="180" width="160" height="130" rx="5" ry="5" fill="#c0ffc0" stroke="#80c080"/>
    <text x="330" y="200" text-anchor="middle" font-size="14" fill="#141" font-weight="bold">Trajectory Tokenization</text>
    <text x="260" y="225" font-size="12" fill="#141">- Canonical Norm.</text>
    <text x="260" y="240" font-size="12" fill="#141">- Param Conversion</text>
    <text x="260" y="255" font-size="12" fill="#141">  (Quat, Trans, Intr, Scale)</text>
    <text x="260" y="270" font-size="12" fill="#141">- Discretization (Bins)</text>
    <text x="260" y="285" font-size="12" fill="#141">- Codebook Lookup</text>
    <text x="330" y="305" text-anchor="middle" font-size="12" fill="#141">-> Pose Tokens</text>

    <!-- Auto-regressive Decoder -->
    <rect x="0" y="180" width="230" height="130" rx="5" ry="5" fill="#b0ffb0" stroke="#70b070"/>
    <text x="115" y="200" text-anchor="middle" font-size="14" fill="#141" font-weight="bold">Auto-regressive Decoding</text>
    <text x="115" y="220" text-anchor="middle" font-size="13" fill="#141">(OPT Transformer)</text>
    <text x="10" y="245" font-size="12" fill="#141">- Input: Latent Code Z +</text>
    <text x="30" y="260" font-size="12" fill="#141">Previous Pose Tokens</text>
    <text x="10" y="280" font-size="12" fill="#141">- Predicts Next Pose Token</text>
    <text x="10" y="295" font-size="12" fill="#141">  Sequentially</text>

    <!-- Output -->
    <rect x="0" y="330" width="410" height="60" rx="5" ry="5" fill="#a0ffa0" stroke="#60a060"/>
    <text x="205" y="355" text-anchor="middle" font-size="14" fill="#141" font-weight="bold">Output: Generated Pose Token Sequence</text>
    <text x="205" y="375" text-anchor="middle" font-size="13" fill="#141">-> De-tokenize -> Generated Camera Trajectory</text>

    <!-- Evaluation & Application -->
    <rect x="0" y="410" width="410" height="150" rx="15" ry="15" fill="url(#grad3)" stroke="#d8b080" filter="url(#dropshadow)"/>
    <text x="205" y="435" text-anchor="middle" font-size="16" fill="#531" font-weight="bold">Evaluation & Application</text>

    <rect x="20" y="455" width="180" height="80" rx="5" ry="5" fill="#fff0d0" stroke="#e8c090"/>
    <text x="110" y="475" text-anchor="middle" font-size="14" fill="#531" font-weight="bold">Evaluation</text>
    <text x="30" y="495" font-size="13" fill="#531">- Metrics (CLaTr, F1)</text>
    <text x="30" y="510" font-size="13" fill="#531">- User Study (AUR)</text>
    <text x="30" y="525" font-size="13" fill="#531">- Ablation Studies</text>

    <rect x="210" y="455" width="180" height="80" rx="5" ry="5" fill="#fff0d0" stroke="#e8c090"/>
    <text x="300" y="475" text-anchor="middle" font-size="14" fill="#531" font-weight="bold">Application</text>
    <text x="220" y="495" font-size="13" fill="#531">- Camera Control for</text>
    <text x="230" y="510" font-size="13" fill="#531">Text/Image-to-Video</text>
    <text x="230" y="525" font-size="13" fill="#531">Generation</text>


    <!-- Arrows -->
    <line x1="205" y1="60" x2="205" y2="80" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/>
    <line x1="205" y1="160" x2="115" y2="180" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/> <!-- Encoding to Decoder -->
    <line x1="115" y1="310" x2="205" y2="330" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/> <!-- Decoder to Output -->
    <line x1="205" y1="390" x2="205" y2="410" stroke="#555" stroke-width="1.5" marker-end="url(#arrowhead)"/> <!-- Output to Eval/App -->

    <!-- Connection: Decoder <> Tokenization -->
    <path d="M 230 245 Q 240 245 250 245" stroke="#555" stroke-width="1.5" fill="none" marker-end="url(#arrowhead)"/>
    <path d="M 250 265 Q 240 265 230 265" stroke="#555" stroke-width="1.5" fill="none" marker-end="url(#arrowhead)"/>
    <text x="240" y="260" font-size="10" fill="#555" text-anchor="middle">Uses/Produces</text>

  </g>

  <!-- Connecting Arrow between sections -->
   <path d="M 450 405 Q 475 405 500 405" stroke="#555" stroke-width="2" stroke-dasharray="5,5" fill="none" marker-end="url(#arrowhead)"/>
   <text x="475" y="395" font-size="12" fill="#555" text-anchor="middle">Provides Training Data</text>

</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="It employs an auto-regressive model treating camera parameters as discrete tokens">
                        <div class="quiz-question">1. What is the primary innovation of GenDoP compared to previous camera trajectory generation methods?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It uses a reinforcement learning approach to optimize camera movements">It uses a reinforcement learning approach to optimize camera movements</div><div class="quiz-choice" data-value="It employs an auto-regressive model treating camera parameters as discrete tokens">It employs an auto-regressive model treating camera parameters as discrete tokens</div><div class="quiz-choice" data-value="It introduces a diffusion-based framework with human-centric tracking">It introduces a diffusion-based framework with human-centric tracking</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Free-moving trajectories that enable unrestricted 3D camera motion">
                        <div class="quiz-question">2. What type of camera trajectories does the DataDoP dataset focus on?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Object/Scene-centric trajectories that focus on specific objects">Object/Scene-centric trajectories that focus on specific objects</div><div class="quiz-choice" data-value="Tracking trajectories that follow moving subjects">Tracking trajectories that follow moving subjects</div><div class="quiz-choice" data-value="Free-moving trajectories that enable unrestricted 3D camera motion">Free-moving trajectories that enable unrestricted 3D camera motion</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Two types: Motion captions and Directorial captions">
                        <div class="quiz-question">3. How many types of captions are generated for each trajectory in the DataDoP dataset?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="One type: Technical captions describing camera parameters">One type: Technical captions describing camera parameters</div><div class="quiz-choice" data-value="Two types: Motion captions and Directorial captions">Two types: Motion captions and Directorial captions</div><div class="quiz-choice" data-value="Three types: Translation, Rotation, and Intent captions">Three types: Translation, Rotation, and Intent captions</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/tileable-wood.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
  Tokens</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-04-09</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2504.07096" target="_blank">http://arxiv.org/pdf/2504.07096</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper introduces OLMoTrace, a system for tracing language model outputs back to their training data in real-time.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> The paper builds on infini-gram (a text search engine) and extends it with a novel parallel algorithm to efficiently trace language model outputs to their training data, which was previously computationally intractable at trillion-token scale.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper addresses the challenge of understanding why language models generate certain responses by tracing their outputs back to training data, which was previously impossible at scale due to computational constraints.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> The authors use a five-step inference pipeline that finds maximal matching spans in LM outputs, filters for long and unique spans, retrieves enclosing documents, merges spans and documents, and ranks documents by relevance using BM25 scoring.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> The system achieves an average inference latency of 4.46 seconds per query on responses averaging 458 tokens, with document relevance evaluations showing the top documents displayed having an average relevance score of 1.82 (on a 0-3 scale) according to LLM-as-a-Judge evaluation.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training
  Tokens</h2>
                        <svg width="100%" viewBox="0 0 1000 800" xmlns="http://www.w3.org/2000/svg">

  <!-- Background -->
  <defs>
    <linearGradient id="bgGradient" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0f4f8;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#d9e2ec;stop-opacity:1" />
    </linearGradient>
  </defs>
  <rect width="100%" height="100%" fill="url(#bgGradient)" />

  <!-- Title -->
  <text x="500" y="50" font-family="Arial, sans-serif" font-size="32" font-weight="bold" text-anchor="middle" fill="#1a237e">
    OLMoTrace Inference Pipeline
  </text>
  <text x="500" y="80" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#546e7a">
    Tracing LM Outputs to Training Data (Focus on Method)
  </text>

  <!-- Input -->
  <rect x="350" y="120" width="300" height="50" rx="10" ry="10" fill="#ffecb3" stroke="#ffa000" stroke-width="1.5"/>
  <text x="500" y="150" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#4e342e">
    Input: LM Response & User Prompt
  </text>

  <!-- Arrow -->
  <line x1="500" y1="170" x2="500" y2="190" stroke="#546e7a" stroke-width="2"/>
  <polygon points="495,190 505,190 500,195" fill="#546e7a"/>

  <!-- Step 1: Find Maximal Matching Spans -->
  <rect x="150" y="200" width="700" height="120" rx="15" ry="15" fill="#c5cae9" stroke="#3f51b5" stroke-width="2"/>
  <text x="500" y="225" font-family="Arial, sans-serif" font-size="18" font-weight="bold" text-anchor="middle" fill="#1a237e">
    Step 1: Find Maximal Matching Spans
  </text>
  <text x="170" y="255" font-family="Arial, sans-serif" font-size="14" fill="#303f9f">
    - Tokenize LM output (Llama-2).
  </text>
  <text x="170" y="275" font-family="Arial, sans-serif" font-size="14" fill="#303f9f">
    - Identify verbatim spans in training data meeting:
  </text>
  <text x="190" y="295" font-family="Arial, sans-serif" font-size="13" fill="#303f9f" font-style="italic">
      Existence, Self-contained, Maximality.
  </text>
  <text x="500" y="255" font-family="Arial, sans-serif" font-size="14" fill="#303f9f">
    - Key Tech: Parallel algorithm using <tspan font-weight="bold">infini-gram</tspan>
  </text>
  <text x="500" y="275" font-family="Arial, sans-serif" font-size="14" fill="#303f9f">
      (Suffix Array index on Trillion+ tokens).
  </text>
   <text x="500" y="295" font-family="Arial, sans-serif" font-size="14" fill="#303f9f">
     - Fast lookup: O(1) FIND query per suffix (parallelized).
  </text>

  <!-- Arrow -->
  <line x1="500" y1="320" x2="500" y2="340" stroke="#546e7a" stroke-width="2"/>
  <polygon points="495,340 505,340 500,345" fill="#546e7a"/>

  <!-- Step 2: Filter Spans -->
  <rect x="250" y="350" width="500" height="60" rx="10" ry="10" fill="#b2dfdb" stroke="#00796b" stroke-width="1.5"/>
  <text x="500" y="375" font-family="Arial, sans-serif" font-size="18" font-weight="bold" text-anchor="middle" fill="#004d40">
    Step 2: Filter for Long & Unique Spans
  </text>
  <text x="500" y="400" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#00695c">
    Keep top K spans with lowest <tspan font-style="italic">span unigram probability</tspan>.
  </text>

  <!-- Arrow -->
  <line x1="500" y1="410" x2="500" y2="430" stroke="#546e7a" stroke-width="2"/>
  <polygon points="495,430 505,430 500,435" fill="#546e7a"/>

  <!-- Step 3: Retrieve Documents -->
  <rect x="250" y="440" width="500" height="60" rx="10" ry="10" fill="#b2dfdb" stroke="#00796b" stroke-width="1.5"/>
  <text x="500" y="465" font-family="Arial, sans-serif" font-size="18" font-weight="bold" text-anchor="middle" fill="#004d40">
    Step 3: Retrieve Enclosing Documents
  </text>
  <text x="500" y="490" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#00695c">
    Retrieve up to 10 document snippets per kept span (sample if >10).
  </text>

  <!-- Arrow -->
  <line x1="500" y1="500" x2="500" y2="520" stroke="#546e7a" stroke-width="2"/>
  <polygon points="495,520 505,520 500,525" fill="#546e7a"/>

  <!-- Step 4: Merge -->
  <rect x="250" y="530" width="500" height="60" rx="10" ry="10" fill="#b2dfdb" stroke="#00796b" stroke-width="1.5"/>
  <text x="500" y="555" font-family="Arial, sans-serif" font-size="18" font-weight="bold" text-anchor="middle" fill="#004d40">
    Step 4: Merge Spans & Documents
  </text>
  <text x="500" y="580" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#00695c">
    Merge overlapping spans for UI; merge snippets from same source doc.
  </text>

  <!-- Arrow -->
  <line x1="500" y1="590" x2="500" y2="610" stroke="#546e7a" stroke-width="2"/>
  <polygon points="495,610 505,610 500,615" fill="#546e7a"/>

  <!-- Step 5: Rerank & Color -->
  <rect x="200" y="620" width="600" height="80" rx="10" ry="10" fill="#e1bee7" stroke="#8e24aa" stroke-width="1.5"/>
  <text x="500" y="645" font-family="Arial, sans-serif" font-size="18" font-weight="bold" text-anchor="middle" fill="#4a148c">
    Step 5: Rerank & Color by Relevance
  </text>
  <text x="500" y="670" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#6a1b9a">
    - Rerank documents using <tspan font-weight="bold">BM25</tspan> (Query: Prompt+Response, Corpus: Retrieved Docs).
  </text>
  <text x="500" y="690" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#6a1b9a">
    - Color document sidebars & span highlights based on relevance score (High/Med/Low).
  </text>

  <!-- Arrow -->
  <line x1="500" y1="700" x2="500" y2="720" stroke="#546e7a" stroke-width="2"/>
  <polygon points="495,720 505,720 500,725" fill="#546e7a"/>

  <!-- Output -->
  <rect x="300" y="730" width="400" height="50" rx="10" ry="10" fill="#ffccbc" stroke="#d84315" stroke-width="1.5"/>
  <text x="500" y="760" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#bf360c">
    Output: Highlighted Spans & Ranked Source Docs
  </text>

</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="A parallel algorithm built on infini-gram that processes suffixes simultaneously">
                        <div class="quiz-question">1. What is the primary innovation that allows OLMoTrace to efficiently trace language model outputs back to training data?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="A novel tokenization algorithm that reduces the size of training data">A novel tokenization algorithm that reduces the size of training data</div><div class="quiz-choice" data-value="A parallel algorithm built on infini-gram that processes suffixes simultaneously">A parallel algorithm built on infini-gram that processes suffixes simultaneously</div><div class="quiz-choice" data-value="A reinforcement learning approach that predicts likely training sources">A reinforcement learning approach that predicts likely training sources</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Using color saturation levels to indicate the relevance of source documents">
                        <div class="quiz-question">2. How does OLMoTrace highlight spans in language model responses?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Using a single color for all matching spans regardless of document relevance">Using a single color for all matching spans regardless of document relevance</div><div class="quiz-choice" data-value="Using different colors based on the length of the matching span">Using different colors based on the length of the matching span</div><div class="quiz-choice" data-value="Using color saturation levels to indicate the relevance of source documents">Using color saturation levels to indicate the relevance of source documents</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Approximately 4.6 trillion tokens">
                        <div class="quiz-question">3. What is the total size of the training data that OLMoTrace indexes and searches for OLMo-2-32B-Instruct?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Approximately 460 billion tokens">Approximately 460 billion tokens</div><div class="quiz-choice" data-value="Approximately 4.6 trillion tokens">Approximately 4.6 trillion tokens</div><div class="quiz-choice" data-value="Approximately 46 trillion tokens">Approximately 46 trillion tokens</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- 卡片计数器 -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- 第一张卡片：论文概述 -->
                    <div class="paper-card active" style="background-image: url('bg/tasky.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>A Unified Agentic Framework for Evaluating Conditional Image Generation</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-04-09</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2504.07046" target="_blank">http://arxiv.org/pdf/2504.07046</a></p>
                        <div><div class="category-chunk">1.  <strong>📘 Topic and Domain:</strong> The paper introduces CIGEVAL, a unified agentic framework for evaluating conditional image generation across various tasks such as text-guided image generation, subject-driven image editing, and control-guided image generation.</div><div class="category-chunk">2.  <strong>💡 Previous Research and New Ideas:</strong> The paper builds upon previous image evaluation metrics like CLIP-Score, LPIPS, and VIESCORE, but proposes a novel approach that integrates large multimodal models (LMMs) with specialized tools to overcome limitations in task specificity, explainability, and human alignment.</div><div class="category-chunk">3.  <strong>❓ Problem:</strong> The paper addresses the challenge of developing task-agnostic, reliable, and explainable evaluation metrics for conditional image generation that can align with human judgment across diverse generation tasks.</div><div class="category-chunk">4.  <strong>🛠️ Methods:</strong> The authors implement an agentic framework that combines LMMs (like GPT-4o or open-source models) with a multi-functional toolbox (including Grounding, Highlight, Difference, and Scene Graph tools) and fine-grained evaluation through task decomposition, tool selection, and analysis.</div><div class="category-chunk">5.  <strong>📊 Results and Evaluation:</strong> CIGEVAL with GPT-4o achieves a Spearman correlation of 0.4625 with human assessments across seven tasks, closely matching the human-to-human correlation of 0.47, and when implemented with fine-tuned 7B open-source LMMs using only 2.3K training trajectories, it surpasses previous GPT-4o-based state-of-the-art methods.</div></div>
                    </div>
                    
                    <!-- 第二张卡片：流程图 -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>A Unified Agentic Framework for Evaluating Conditional Image Generation</h2>
                        <svg width="100%" viewBox="0 0 1000 1000" xmlns="http://www.w3.org/2000/svg">

  <!-- Define styles -->
  <defs>
    <linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(150,200,255);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(200,230,255);stop-opacity:1" />
    </linearGradient>
     <linearGradient id="grad2" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(255,220,150);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(255,240,200);stop-opacity:1" />
    </linearGradient>
     <linearGradient id="grad3" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(160,255,160);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(210,255,210);stop-opacity:1" />
    </linearGradient>
     <linearGradient id="grad4" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(255,180,180);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(255,220,220);stop-opacity:1" />
    </linearGradient>
    <linearGradient id="grad5" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(200, 180, 255);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(230, 220, 255);stop-opacity:1" />
    </linearGradient>
     <linearGradient id="grad6" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:rgb(180, 180, 180);stop-opacity:1" />
      <stop offset="100%" style="stop-color:rgb(220, 220, 220);stop-opacity:1" />
    </linearGradient>
    <style>
      .box { stroke: #333; stroke-width: 1.5; rx: 10; ry: 10; filter: drop-shadow( 3px 3px 2px rgba(0,0,0,0.2) ); }
      .title { font-family: 'Arial', sans-serif; font-size: 24px; font-weight: bold; fill: #222; }
      .subtitle { font-family: 'Arial', sans-serif; font-size: 16px; font-weight: bold; fill: #444; }
      .text { font-family: 'Arial', sans-serif; font-size: 13px; fill: #333; }
      .arrow { stroke: #555; stroke-width: 2; marker-end: url(#arrowhead); }
      .dashed-arrow { stroke: #777; stroke-width: 1.5; stroke-dasharray: 5, 5; marker-end: url(#arrowhead-dashed); }
      .tool-box { fill: #f0f0f0; stroke: #aaa; stroke-width: 1; rx: 5; ry: 5; }
      .tool-text { font-family: 'Arial', sans-serif; font-size: 12px; fill: #555; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#555" />
    </marker>
     <marker id="arrowhead-dashed" markerWidth="10" markerHeight="7" refX="0" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#777" />
    </marker>
  </defs>

  <!-- Title -->
  <text x="500" y="40" text-anchor="middle" class="title">CIGEVAL: Methodology Flowchart</text>

  <!-- Input -->
  <rect x="350" y="70" width="300" height="70" class="box" fill="url(#grad1)"/>
  <text x="500" y="95" text-anchor="middle" class="subtitle">Input: Conditional Image Generation Task</text>
  <text x="500" y="120" text-anchor="middle" class="text">Generated Image (O), Conditions (C*), Instruction (I)</text>

  <!-- Agent Core -->
  <rect x="375" y="170" width="250" height="60" class="box" fill="url(#grad2)"/>
  <text x="500" y="195" text-anchor="middle" class="subtitle">CIGEVAL Agent Core</text>
  <text x="500" y="215" text-anchor="middle" class="text">Large Multimodal Model (LMM)</text>
  <line x1="500" y1="140" x2="500" y2="170" class="arrow" />

  <!-- Toolbox -->
  <rect x="700" y="170" width="200" height="170" class="box" fill="#f9f9f9" stroke="#ccc"/>
  <text x="800" y="195" text-anchor="middle" class="subtitle">Multi-functional Toolbox</text>
  <rect x="720" y="215" width="160" height="25" class="tool-box"/>
  <text x="800" y="230" text-anchor="middle" class="tool-text">Grounding</text>
  <rect x="720" y="245" width="160" height="25" class="tool-box"/>
  <text x="800" y="260" text-anchor="middle" class="tool-text">Highlight</text>
  <rect x="720" y="275" width="160" height="25" class="tool-box"/>
  <text x="800" y="290" text-anchor="middle" class="tool-text">Difference</text>
  <rect x="720" y="305" width="160" height="25" class="tool-box"/>
  <text x="800" y="320" text-anchor="middle" class="tool-text">Scene Graph</text>
  <line x1="625" y1="200" x2="700" y2="255" class="dashed-arrow" />
  <text x="665" y="220" text-anchor="middle" class="tool-text" transform="rotate(-20 665,220)">Uses</text>

  <!-- Evaluation Framework -->
  <rect x="300" y="260" width="400" height="300" class="box" fill="url(#grad3)"/>
  <text x="500" y="285" text-anchor="middle" class="subtitle">Fine-grained Evaluation Framework</text>
  <line x1="500" y1="230" x2="500" y2="260" class="arrow" />

  <!-- Steps within Framework -->
  <rect x="320" y="300" width="360" height="50" class="box" fill="#ffffff" stroke="#aaddaa"/>
  <text x="500" y="325" text-anchor="middle" class="text">(1) Task Decomposition (based on C* & I)</text>
  <line x1="500" y1="350" x2="500" y2="370" class="arrow" />

  <rect x="320" y="370" width="360" height="50" class="box" fill="#ffffff" stroke="#aaddaa"/>
  <text x="500" y="395" text-anchor="middle" class="text">(2) Tool Selection (Agent decides, uses Toolbox if needed)</text>
  <line x1="500" y1="420" x2="500" y2="440" class="arrow" />

  <rect x="320" y="440" width="360" height="50" class="box" fill="#ffffff" stroke="#aaddaa"/>
  <text x="500" y="458" text-anchor="middle" class="text">(3) Analysis (ReAct Style: Observation, Thought, Action)</text>
   <text x="500" y="475" text-anchor="middle" class="text">(Analyzes inputs & tool outputs)</text>
  <line x1="500" y1="490" x2="500" y2="510" class="arrow" />

  <rect x="320" y="510" width="170" height="40" class="box" fill="#ffffff" stroke="#aaddaa"/>
  <text x="405" y="530" text-anchor="middle" class="text">(4) Fine-grained Scoring</text>
  <line x1="490" y1="530" x2="510" y2="530" class="arrow" />

  <rect x="510" y="510" width="170" height="40" class="box" fill="#ffffff" stroke="#aaddaa"/>
  <text x="595" y="530" text-anchor="middle" class="text">(5) Score Aggregation (min)</text>

  <!-- Output -->
  <rect x="350" y="590" width="300" height="60" class="box" fill="url(#grad4)"/>
  <text x="500" y="615" text-anchor="middle" class="subtitle">Output</text>
  <text x="500" y="635" text-anchor="middle" class="text">Rationale & Final Score (0.0 - 1.0)</text>
  <line x1="500" y1="560" x2="500" y2="590" class="arrow" />

  <!-- Agent Tuning Flow -->
  <rect x="50" y="700" width="900" height="250" class="box" fill="url(#grad5)" stroke="#aaaacc"/>
  <text x="500" y="725" text-anchor="middle" class="subtitle">Agent Tuning (for Open-Source LMMs)</text>

  <rect x="100" y="750" width="200" height="80" class="box" fill="#ffffff" stroke="#aaaacc"/>
  <text x="200" y="780" text-anchor="middle" class="text">Generate Evaluation</text>
  <text x="200" y="795" text-anchor="middle" class="text">Trajectories using</text>
  <text x="200" y="810" text-anchor="middle" class="text">GPT-4o Agent</text>

  <line x1="300" y1="790" x2="350" y2="790" class="arrow" />

  <rect x="350" y="750" width="200" height="80" class="box" fill="#ffffff" stroke="#aaaacc"/>
  <text x="450" y="780" text-anchor="middle" class="text">Filter Trajectories</text>
  <text x="450" y="795" text-anchor="middle" class="text">(Keep if agent score ≈</text>
  <text x="450" y="810" text-anchor="middle" class="text">human score)</text>

  <line x1="550" y1="790" x2="600" y2="790" class="arrow" />

  <rect x="600" y="750" width="200" height="80" class="box" fill="#ffffff" stroke="#aaaacc"/>
  <text x="700" y="780" text-anchor="middle" class="text">Supervised Fine-Tuning</text>
  <text x="700" y="795" text-anchor="middle" class="text">(SFT) on Filtered Data</text>
  <text x="700" y="810" text-anchor="middle" class="text">(Loss on Thought & Action)</text>

  <line x1="800" y1="790" x2="850" y2="790" class="arrow" />

  <rect x="850" y="765" width="80" height="50" class="box" fill="url(#grad2)"/>
  <text x="890" y="785" text-anchor="middle" class="text">Tuned</text>
  <text x="890" y="800" text-anchor="middle" class="text">OS-LMM</text>

  <!-- Link Tuning back to Agent Core -->
   <path d="M 890 765 Q 890 700 500 700 Q 110 700 110 790" fill="none" stroke="#aaaacc" stroke-width="1.5" stroke-dasharray="5,5"/>
   <path d="M 500 230 C 500 680, 870 680, 870 765" stroke="#aaaacc" stroke-width="1.5" stroke-dasharray="5, 5" fill="none" marker-end="url(#arrowhead-dashed)"/>
   <text x="690" y="700" text-anchor="middle" class="tool-text">Resulting Tuned Agent</text>

  <!-- Evaluation (Mentioned, not detailed flow) -->
   <rect x="50" y="860" width="900" height="80" class="box" fill="url(#grad6)" stroke="#888888"/>
   <text x="500" y="885" text-anchor="middle" class="subtitle">Framework Evaluation</text>
   <text x="500" y="905" text-anchor="middle" class="text">Benchmarked on ImagenHub against baselines & human correlation.</text>
   <text x="500" y="920" text-anchor="middle" class="text">Ablation studies performed to validate tool contributions.</text>
   <line x1="500" y1="650" x2="500" y2="860" class="dashed-arrow" />

</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="It integrates LMMs with specialized tools in an agentic framework">
                        <div class="quiz-question">1. What is the main innovation of CIGEVAL compared to previous image evaluation metrics?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It uses only GPT-4o as the evaluation model">It uses only GPT-4o as the evaluation model</div><div class="quiz-choice" data-value="It integrates LMMs with specialized tools in an agentic framework">It integrates LMMs with specialized tools in an agentic framework</div><div class="quiz-choice" data-value="It focuses exclusively on text-guided image generation">It focuses exclusively on text-guided image generation</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="2,300 trajectories">
                        <div class="quiz-question">2. How many training trajectories were used to fine-tune the open-source 7B LMMs in CIGEVAL?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="47,000 trajectories">47,000 trajectories</div><div class="quiz-choice" data-value="23,000 trajectories">23,000 trajectories</div><div class="quiz-choice" data-value="2,300 trajectories">2,300 trajectories</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Difference">
                        <div class="quiz-question">3. What tool in CIGEVAL's toolbox is used to detect subtle differences between two similar images?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Scene Graph">Scene Graph</div><div class="quiz-choice" data-value="Grounding">Grounding</div><div class="quiz-choice" data-value="Difference">Difference</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 创建遮罩层
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // 获取所有问题标签
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // 设置点击事件处理
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // 点击标签切换问题卡的显示状态
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // 阻止事件冒泡
                    
                    // 如果当前问题卡已经显示，则隐藏它
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // 先隐藏所有其他问题卡
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // 将弹窗内容复制到页面最外层的弹窗中
                        document.body.appendChild(popup);
                        
                        // 显示当前问题卡和背景遮罩
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // 确保点击问题卡内部时不会关闭问题卡
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // 点击遮罩层或页面任何其他位置时隐藏所有问题卡
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // 为每个选项添加点击事件
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // 重置所有选项
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // 标记当前选项为已选
                    this.classList.add('selected');
                    
                    // 检查是否正确
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '✔️ Correct！';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '❌ Wrong！';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // 卡片轮播功能 - 新增
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // 更新计数器显示
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // 显示指定索引的卡片
                function showCard(index) {
                    // 处理循环
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // 更新当前索引
                    currentIndex = index;
                    
                    // 更新卡片显示
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // 更新计数器
                    updateCounter();
                }
                
                // 下一张卡片
                function nextCard(e) {
                    e.stopPropagation(); // 防止事件冒泡导致问题卡关闭
                    showCard(currentIndex + 1);
                }
                
                // 为卡片容器添加点击事件
                cardDeck.addEventListener('click', function(e) {
                    // 检查点击是否发生在流程图卡片内部的滚动区域
                    // 如果是在滚动条上点击，不切换卡片
                    const targetCard = e.target.closest('.paper-card');
                    if (targetCard && targetCard.classList.contains('flowchart-card')) {
                        // 计算点击位置是否在滚动条区域
                        const rect = targetCard.getBoundingClientRect();
                        const isScrollbarClick = 
                            (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                            (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);
                        
                        if (!isScrollbarClick) {
                            nextCard(e);
                        }
                    } else {
                        nextCard(e);
                    }
                });
                
                // 键盘导航
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
</body>
</html>
