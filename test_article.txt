<document index="0" media_type="application/pdf"><document_content page="1">DeepSeek LLM
Scaling Open-Source Language Models with Longtermism

Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,
Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao,
Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He,
Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang,
Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu,
Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu,
Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song,
Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang,
Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie,
Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu,
Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang,
Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao,
Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou *

*DeepSeek-AI

Abstract

The rapid development of open-source large language models (LLMs) has been truly remarkable.
However, the scaling laws described in previous literature presents varying conclusions, which
casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our
distinctive findings that facilitate the scaling of large scale models in two prevalent used open-
source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM,
a project dedicated to advancing open-source language models with a long-term perspective.
To support the pre-training phase, we have developed a dataset that currently consists of 2
trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT)
and direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM
67B surpasses LLaMA-2 70B across a range of benchmarks, especially in the domains of code,
mathematics, and reasoning. Furthermore, open-ended evaluations reveal that our DeepSeek
LLM 67B Chat exhibits superior performance compared to GPT-3.5.

arXiv:2401.02954v1  [cs.CL]  5 Jan 2024

*Authors are ordered alphabetically by the last name.</document_content><document_content page="2">Contents

1

Introduction

2 Pre-Training

2.1 Data .

.

.

.

.

.

2.2 Architecture .

.

.

.

.

.

.

2.3 Hyperparameters .

2.4

Infrastructures .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Scaling Laws

3.1

Scaling Laws for Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Estimating Optimal Model and Data Scaling . . . . . . . . . . . . . . . . . . . . .

3

4

4

5

5

6

7

8

9

3.3

Scaling Laws with Different Data . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

4 Alignment

5 Evaluation

5.1 Public Benchmark Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.1 Base Model

5.1.2 Chat Model

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 Open-Ended Evaluation .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2.1 Chinese Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . .

5.2.2 English Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . .

5.3 Held-Out Evaluation .

5.4

Safety Evaluation .

5.5 Discussion .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Conclusion, Limitation, and Future Work

A Appendix

A.1 Acknowledgments

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 Different Model Scale Representations . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 Benchmark Metrics Curves

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.4 Comparison with Code or Math Specific Models . . . . . . . . . . . . . . . . . . .

A.5 Benchmark Results w/ DPO Stage . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.6 Evaluation Formats .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12

13

13

14

14

17

17

18

18

19

20

23

30

30

30

31

32

32

32

2</document_content><document_content page="3">1. Introduction

Over the past few years, Large Language Models (LLMs) based on decoder-only Transformers
(Vaswani et al., 2017) have increasingly become the cornerstone and pathway to achieving Arti-
ficial General Intelligence (AGI). By predicting the next word in continuous text, LLMs undergo
self-supervised pre-training on massive datasets, enabling them to achieve various purposes and
possess many abilities, such as novel creation, text summarization, code completion, and more.
Subsequent developments like supervised fine-tuning and reward modeling have enabled Large
Language Models (LLMs) to better follow user intentions and instructions. This has endowed
them with more versatile conversational capabilities and rapidly expanded their influence.

This wave is sparked with closed products, such as ChatGPT (OpenAI, 2022), Claude (An-
thropic, 2023), and Bard (Google, 2023), which are developed with extensive computational
resources and substantial annotation costs. These products have significantly raised the commu-
nityâ€™s expectations for the capabilities of open-source LLMs, consequently inspiring a series of
work (Bai et al., 2023; Du et al., 2022; Jiang et al., 2023; Touvron et al., 2023a,b; Yang et al., 2023).
Among these, the LLaMA series models (Touvron et al., 2023a,b) stand out. It consolidates a
range of works to create an efficient and stable architecture, building well-performing models
ranging from 7B to 70B parameters. Consequently, the LLaMA series has become the de facto
benchmark for architecture and performance among open-source models.

Following LLaMA, the open-source community has primarily focused on training fixed-size
(7B, 13B, 34B, and 70B), high-quality models, often neglecting research exploration into LLM
scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020). Nonetheless, research on scaling laws is
of utmost importance, considering that the current open-source models are merely at the initial
stage of Artificial General Intelligence (AGI) development. In addition, early works (Hoffmann
et al., 2022; Kaplan et al., 2020) reached varying conclusions on the scaling of model and data
with increased compute budgets and inadequately addressed hyperparameter discussions. In
this paper, we extensively investigate the scaling behavior of language models and apply our
findings in two widely used large-scale model configurations, namely 7B and 67B. Our study
aims to lay the groundwork for future scaling of open-source LLMs, paving the way for further
advancements in this domain. Specifically, we first examined the scaling laws of batch size
and learning rate, and found their trends with model size. Building on this, we conducted a
comprehensive study of the scaling laws of the data and model scale, successfully revealing the
optimal model/data scaling-up allocation strategy and predicting the expected performance
of our large-scale models. Additionally, during development, we discovered that the scaling
laws derived from different datasets show significant differences. This suggests that choice
of dataset remarkably affects the scaling behavior, indicating that caution should be exercised
when generalizing scaling laws across datasets.

Under the guidance of our scaling laws, we build from scratch open-source large language
models, and release as much information as possible for community reference. We collect
2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we
generally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler
with a multi-step learning rate scheduler, maintaining performance while facilitating continual
training. We collected over 1 million instances for supervised fine-tuning (SFT) (Ouyang et al.,
2022) from diverse sources. This paper shares our experiences with different SFT strategies
and findings in data ablation techniques. Additionally, we have utilized direct preference
optimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the
model.

3</document_content></document>