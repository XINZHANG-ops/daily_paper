# For paper **Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting**

I think this paper find a very interesting problem. Why SFT training make the model forgets what it knows before, technical term is **catastrophic forgetting**.

The paper try to find why SFT has this problem but RL not introduce this issue. The authors found a really interesting reason, it is not because SFT algorithm itself, it is because of those low entropy, low probability tokens in the training data. Let me translate what that means, low entropy token means given the context, model think this token must be the next token (model is super confident), low probabilty is our ground truth give a very different token instead of the high confident one. 

And they in fact prove this, they mask those kind of tokens and do SFT, and forgoting is almost gone. Thus what they proposed is Entropy-Adaptive Fine-Tuning (EAFT), so the idea is simple, we still do the SFT training, but for those low entropy low prob tokens, we restrict their gradient update, for other tokens we do as usual. And their experiment shows significantly better on the forgetting issue.

I think this is very interesting, this makes me think about when we (human) really believe something, and someone else tell us we are totally wrong, as human to protect ourself, we tend to not believe them, even if later time we know that's a fact, we can still convince ourself to believe what we believe before. I think this kind of protection machnism is quite important for us, if we are like the LLMs, we might also easily get lost.

Github: https://github.com/PRIS-CV/EAFT

![My diagram notes](entropy-prob.png)