# Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers

This is the first paper I have seen talks about the sampling strategy of LLM logits, in this context, they call it decoding. 

For the parameters like top_p, top_k, tempreture, the adjustments of those is like an art, there is no good way of setting those, for some settings it works well, but no one knows why, and it feels like they are 3 independent parameters just some how work together.

This paper points out that, all these different parameters, foundamentally, they are just all solving one math problem, they write all those into one formula, and claim that they all just doing one optimization problem with different hyper-parameters.

They create a algorithm called **Mirror Ascent**, this is a general solver, iteratively solve for the probability of a word. So that they created the **Best-of-k BoK**, which is saying that we need certain distribution, and if I can draw K times, I want to optimize the probability of that distrubution.

At the end of the day, they claim the sampling is an optimization problem.

To be honest, I think this is a really insightful thought, but I do not fully understand why the samplings are end of the day a optimization problem, and the different strategies like topK, tempreture is just different regularizations, I may need to think more to understand it.


![My diagram notes](image_1.png)