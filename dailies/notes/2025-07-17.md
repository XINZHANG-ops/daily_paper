SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories? is a quite unique paper among all those papers who targeting to write better code with LLMs. This paper not focusing on code accuracy or bugs, but on the code efficiency, or performance. It pulls 140 real PRs from real git repositories, like sklearn, sympy etc, and it compares the code performance before and after the LLM modifications. They have 2 metrics, oracle and realistic, oracle is tell the model which function or method to optmize and provide the relevant context for the LLM to optimize, realistic is give the LLM entire repository ask the LLM optimize on repository level. And the experiment shows that doesn't matter GPT or Gemini or Claude, None of the model can compete with human expert optimization on both oracle or realistic, and the difference is about 10% performance. Which shows LLM still have a long way to go on tasks like this. This reminds me about my coding behavior, the first time I coding something I care less about efficiency and just try to make something work, and optimization is not the priority. And I believe a lot of the open repos are like this, people like to have code that just works not fully optimized which is time cosuming and with diminishing return. And LLMs are triained on those suboptimal codes thus the ability is not there yet.
Github: [link](https://swe-perf.github.io/)
![My diagram notes](unnamed.png)
