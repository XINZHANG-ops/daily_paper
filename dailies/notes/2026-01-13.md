# MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head

Paper from peking university, my home city, feel quite nice. So first of all we need to understand what is linear attention. We know that the vanilla attention is of formula: 

$$o_i = \sum_{j=1}^{N} \operatorname{softmax}\!\left(\frac{q_i^\top k_j}{\sqrt{d}}\right) v_j
$$

so each token will find a set of attention to other tokens in the sentence. And the softmax is appiled to this token's attention. And the complexity is of $O(N^2)$ since the number of attention is the square of number of tokens $N$. So linear attention try to reduce this complexity to $O(N)$, it uses a kernel trick to make all tokens share a same KV summary, this is like previously each token finds its own attention, but linear attention finds all together, and each token go get from it. This is efficient but lost the previous token-level attention, it is like all kinds of tokens are looking at the same **summary** to get its own attentions, which could be bad obviously. So the idea of this paper is like, can we write not just one summary, but multiple summaries, and each token can get their attentions from different summaries. So this keep a balance between the traditional attention and the linear attention.

Project Page: https://dagroup-pku.github.io/MHLA

GitHub Repo: https://github.com/DAGroup-PKU/MHLA

Huggingface Repo: https://huggingface.co/DAGroup-PKU/MHLA


![My diagram notes](image_1.png)
![My diagram notes](image_2.png)