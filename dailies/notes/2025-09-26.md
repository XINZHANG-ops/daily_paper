The paper Tree Search for LLM Agent Reinforcement Learning proposed a new way of sampling step-level chain-of-thought trajectory for GRPO, what they call it tree search GRPO. The traditional GRPO for CoT is a an complete CoT trajectory, and each one has one reward for advantages calculation. But the tree GRPO for each node of the tree it has splits, and naturally at the nodes we have preference, like a step-wise DPO, and indeed the paper proves that their method is equivalent to step-level DPO. Obviously this new way of sampling is more efficient, and it also have the signal at which step of the tree the difference happens. I wonder if this can also be applied into text generation.
Github: [link](https://github.com/AMAP-ML/Tree-GRPO)
![My diagram notes](unnamed.png)
