For paper Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR, to understand this paper we need some background understanding of the current RL methods for LLMs. PPO is a actor-critic method, which means it is a policy gradient method that based on a value function to calculate advantages for lower variance more stable learning. Whereas GRPO is not strict actor-critic, although it is still policy gradient, but it does not rely on the value function to find the advantages, instead it uses the comparison inside of the group to approximate the advantages. And this paper proposed a method, unlike PPO explicit use actor(policy) and critic(value function), it implicit combined the two, no explicit critic anymore. The way they do it is they create a supervise learning problem that build on a scoring function, this function will predict the reward probability(reward is 0 or 1), and it showed mathematically, the gradient of this scoring function is exactly the same as doing policy gradient and value function critic, but without really using the value function. 
Github: [link](https://github.com/ritzz-ai/PACS)
![My diagram notes](unnamed.png)
