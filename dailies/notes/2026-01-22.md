# Aligning Agentic World Models via Knowledgeable Experience Learning

This paper talks about a very common type of hallucination of the models, it knows the goal, but do not understand the physics world. Model knows the exsitence of physical laws and etc, but since it never feel such things, all knowledge from it is from **reading**, thus when it faces a more complex task, it may forgot the real world physical interactions, the paper call it **physical hallucination**, for example there is a wall between starting point and ending point, but LLM just go from point A to B directly, since he forgot or unrealize we cannot go thru walls. 

They introduce their solution called **WorldMind**, which reminds of FeiFei Li **WordLabs** again. What they do is they use a simulated world - which already exist from previous researches, and each time model has to generate 2 things, **Action**, and the outcome of the action, the **Predicted Next State**, and this action will be act in the simulator, and the result or the new **state** of the execution of the action will be told to the model, and the model will observe the outcome and summary a new relation and save to the worldmind repo. For example, model action **cut**, and he think the result is **vegatable is cut**, however he obverse the vegetable is **uncut**, then the model abstracts the failure and records a conditional constraint, e.g., that the action cut is invalid when the agent is not holding a knife.

IMO this method is not a very efficient way of learning, it still need to explicit write down the rules, even if the rules are not hand crafted, but still it is not scalable and the rules could be contradict to each other easily. But papers like this more like PoC, bring of new way of thinking, intereting still.

![My diagram notes](image_1.png)