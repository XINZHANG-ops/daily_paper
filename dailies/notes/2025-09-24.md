The paper Reinforcement Learning on Pre-Training Data proposed a quite interesting idea that it can make pretraining data as RL training data. Traditionally we know if we want to do RL training we need to have a reward model to give rewards, and we need some dedicated prepared data. But what if we do not want to create reward model and just want to RL training on pre-training data? The paper proposed a quite smart way, it ask the model to predict next whole sentence in the same autogressive way and ask the model to fill in the missing sentence or segments. Since these are from pre-training data, we know the original text, then we can use a semantic embedding to compare the semantic meaning to give rewards, thus only with an embedding model and whole bunch of data without labels we can do RL. It totally removes the human dependency on RL training and give model reasoning abilities.
![My diagram notes](unnamed.png)
