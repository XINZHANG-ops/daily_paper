Lifelong Safety Alignment for Language Models is the first paper I read about safety alignment of LLMs, we  all know the LLM prompts is a big potential risk for attacking or data leakage etc, and Gemini provide things like safety filters. And this paper proposed a evolving method, so it creates a so called meta-attacker keep attacking the model and finds new jailbreaking methods, collect them and then finetuning the model to increase the safety ability. What I most interested in is what new jailbreaking they found, so basically to jailbreak, you have to wrap your bad intention within some legal or good intention, for example. you want to which house has valable to steal, then you need to camouflage your goal as house investing for valuable houses etc, and indeed these are hard to detect since you would never know a person's intention by some normal questions.
Github: [link](https://github.com/sail-sg/LifelongSafetyAlignment)
![My diagram notes](unnamed.png)
