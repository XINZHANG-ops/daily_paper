# MAXS: Meta-Adaptive Exploration with LLM Agents

The moment I read about the problem the paper try to solve, there are two phrases come into my mind immediately: **Butterfly Effect** and **Chaotic System**. I think in principal, LLM itself is a strong **Chaotic System**, since every single next token will be depends on all previous tokens, even if there is a small disturbance, the butterfly effect will kick in, and the error will accumulate and self-acclerate. The desciption above might be abit too much, but I feel LLM has some of those nature in it, what I call it $$Autoregressive \space Amplifier$$. 
The paper try to solve the issue when agents do planning, it is very short sighted and also the trajactory is very unstable. The paper applies a couple steps: 

 1. Make a couple rollouts and check for the possibl outcomes, does any rollout intriduce highly unstable behavior, this is looking ahead.

2. For those unstable steps, worth mentioning the step here means the step in agent action not token genereations. If one step with high logprobs, then suddently low logprobs, means these steps are very sensitive and unstable, these steps will be in the punish term.

3. I call it **Early Stopping** in generation, if we obverve the variance of multiple generation is within certain range, we stop there.

Need to emphasis here the paper is not training the model, but proposed a agent planning scoring and choosing strategy that will make the agent look ahead, at the sometime, make more stable plannings.



![My diagram notes](image_1.png)
![My diagram notes](image_2.png)