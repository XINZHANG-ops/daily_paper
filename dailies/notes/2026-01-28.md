# Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models

World model is indeed one of the trend of research domain. I have heard criticize on the history taught us, man crafted features are not going to work for AI, AI needs to learn features itself, this has been proven in both CV and NLP area. But I feel this saying understand the world model, the idea of world model is also ask the model to learn the physics of the real world, instead of human input crafted physical laws. 

What this paper mentions is in physical related tasks, using image generation for reasoning, is much much more effient, cheap and fast than pure language reasoning, and its train of thoughts is more like human.

They think of world model has two types of abilities, **world reconstruction** and **world simulation**,  reconstruction means when you see a 3D stuff it can image how it looks from the side and back etc, simulation means it can simulation the physical interactions like a ball drop, object moving etc.

The way they do it is they still use text in the reasoning as state description, but also the model will generate image in the middle to help the reasoning, so called the **Interleaved CoT**.

And they observed great increase of the performance on those physical tasks. In short they tried combine text + image reasoning.

Link: https://thuml.github.io/Reasoning-Visual-World/



![My diagram notes](image_1.png)