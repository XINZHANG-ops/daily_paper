# For paper **Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers**

The paper discussed a quite deep problem I first time seen. It says the magtitutes of weight matrices inside the network is not decided by training data, but the weight decay and learning rate parameters. This not making sense at the first glance, since we all know the weights are decided by backpropagation with traing data loss.
$$
\|W\| \;\approx\; \sqrt{\frac{\eta}{\lambda}}
$$

where η​ us the learning rate, λ is the weight decay. 

And since normalization is applied everywhere inside the network, like RMSNorm, the scale is hard restricted, so the only place different scale can happen is the matrix themselves, but the scale is controlled not by data but hyper parameters. The paper did some experiments proved this argument.

And they proposed the **Multipliers** for matrix, we simply add this learnable **s** to previous layer's matrix multiplication, like **y=Wx** to **y=s⋅Wx**, and this s is not effected by weight decay. The goal of this s is to change the scale of the weights. They also mentioned more detailed multipliers for rows and columns seperately, but the main idea is the same.


HuggingFace: https://huggingface.co/tiiuae

![My diagram notes](performance-comparison.png)