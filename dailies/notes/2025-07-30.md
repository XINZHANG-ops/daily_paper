The other days ago we read Group Sequence Policy Optimization, which is sequence level improvement on GROP, and today the paper Geometric-Mean Policy Optimization is another improvement on GRPO. GROP sample n generations use old policy, for each token in each generation, we find the new policy corresponding probs, and new prob/ old prob we get the importance ratio for each token. And for each sequence generation, we find the advantage inside the group. Now the difference is, GROP takes the mean of sum all tokens importance ratio * advantage, whereas the GMPO first find the geometric mean of all tokens importance ratio than multiply by advantage. The reason why they do this geometric is more stable to outliers. Here is one example the mean of [1.0, 10.0, 0.1] is 3.7 whereas the geometric mean of [1.0, 10.0, 0.1]  is (1x10x0.1)**(1/3) = 1, which is much less sensitive to outliers like big value 10 and small value 0.1, makes the training a lot more stable. And we saw it is less sensitive to extrem values, it is less likely domninate by large values to become overfit.
reasearch link: [link](https://thegenerality.com/agi/)
![My diagram notes](unnamed.png)
