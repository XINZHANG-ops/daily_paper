or paper MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer. For a long time I have rare understanding of multi-modal models, I know it can input and output visual data. What this paper points out for me is for a lone time multi-modal models struggle between understanding and outputting visual information. The reason is, in order to better understanding visual, we need continuous embedding, but to get better generation, we need discrete code index, thus they are contradictory, and use both make the model hallucinate more. The paper proposed a unified Vision Tokenizer, it can generate continuous embedding for understanding, and discreate for generation, the discreate code index will be used by diffusion decoder. So overall it used a unified vision tokenizer + LLM autoregression + diffusion decoder.
![My diagram notes](unnamed.png)
