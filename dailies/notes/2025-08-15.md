Recently the papers are focusing a lot of 3D reconstruction and video generation. The paper xToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing is also relevant. It is doing a very specific domain, the cartoon generation. What it does is producers only need to supply a couple keyframes and coloring, the framework is able to generate a consistent cartoon based on those. More specifically, the method is built on top of a Diffusion Transformer (DiT) video foundation model. The key method is what they call post-keyframing: instead of separating inbetweening (drawing frames between keyframes) and colorization, they merge them into a single generative step. This avoids the common issue of error accumulation when one stage relies on the imperfect output of the other. The result is a pipeline where one colored frame + one or two sketches are enough to generate a full cartoon sequence. 
Gitlab: [link](https://lg-li.github.io/project/tooncomposer/)
![My diagram notes](unnamed.png)
