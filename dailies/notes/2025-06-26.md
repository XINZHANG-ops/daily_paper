The paper OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling answers a quite interesting question, given the same size LLM, training on exactly the same set up of RL for reasoning tasks, why one can outperform the other. Although there are tiny differences on the architecture, still cannot explain the huge discrepency of the results. The paper compared the two model for this question, QWEN3B and Llama3B. What they first found is for the same reasoning RL training, Llama3B get abnormal results, it outputs extremely long generations up to 4096 tokens, and it gets weird outputs like \boxed:{} and Solution Solution loops. The paper guess it origins from the two models huge differences on the pretraining data. Thus they experiment with before the RL training, give the Llama model a so called mid-training, which is a mid-stage whose computational and data (token) requirements are interme-diate between pre-training and post-training. And after this step, they redo the comparison of the RL training, then the Llama model has comparable performance with Qwen model. Thus they proves that pre-training data is a huge factor on model's reasoning capability, some low quality data like FineMath-4plus not only useless but also dragging the performance of the model.
Github: [link](https://github.com/GAIR-NLP/OctoThinker)
![My diagram notes](unnamed.png)
