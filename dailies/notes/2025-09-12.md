The paper VLA-Adapter proposed a new bridging paradigm called Bridge Attention for Vision-Language-Action models. A key concept here is ActionQuery — it’s a set of learnable query tokens that sit between the vision-language model (VLM) and the policy. Instead of just passing raw features from the VLM, these ActionQuery tokens actively pull multimodal information (image + text) into themselves, becoming a compact representation tailored for action generation.

In Bridge Attention, the policy doesn’t have to choose between raw features (which carry fine-grained details) and ActionQuery (which encodes higher-level aggregated knowledge). Instead, it uses two cross-attention steps: one attends to raw features, another attends to ActionQuery (plus proprioception), and a learnable ratio controls how much raw detail should be injected. This way, the action space gets both local detail and global context.

What I find exciting is that this isn’t just another scaling trick — it’s a real algorithmic structural change. With only a tiny 0.5B backbone, VLA-Adapter outperforms much larger models like OpenVLA-OFT, and can be trained in just 8 hours on a single GPU. It feels like a promising direction to make VLA more efficient and practical.
Github: [link](https://vla-adapter.github.io/)
![My diagram notes](unnamed.png)
