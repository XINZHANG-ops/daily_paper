For paper Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning, I don't really care the task it trying to deal with, but I'm more interested in the Pref-GRPO they proposed, since recently I dive into RL algorithms. The traditiona GRPO use group of generations to calculate rewards and advantages, normally the rewards of a group does not have big differences, however when we calculate the advantages, it divides the variances of the rewards, if the rewards are similar, which means advantages would have huge differences, the page call it illusory advantage, the model would tends to focus on the small differences(reward hacking). Thus the paper instead of using the absolute rewards, it uses the pairwise rewards, it will calculate the win rate of each candidate inside the group, and use that as the advantages to make a more robust learning.
![My diagram notes](unnamed.png)
