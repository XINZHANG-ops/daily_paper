To understand  paper EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning, we need to know in the traditional RL methods like PPO, except the clipped surrogate loss, there is also a entropy term, the goal was to increase diversity of generations, to avoid converge too fast. But since the traditional entropy is calculated when each action is done, it does not aware of the trajectory level, but normally in those training, in early stage is entropy is large and the model discover so random, and end stage the entropy is so low the model almost no explore. And the paper introduced regularization on the entropy term, it constraint the entropy dynmiacally through the training, limit early stage, and encourage late stage, and it average out the entropy on trajectory level to make the entire trajectory overall diverse. 
Github: [link](https://github.com/WujiangXu/EPO)
Trajectory entropy
![My diagram notes](unnamed.png)
