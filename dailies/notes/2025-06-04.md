Quite interesting paper GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents. Since I did some GUI agent for hechthon and general practice recently, this paper draws my attention. So traditionally when we want the agnet to click anywhere on the screem, people ask model to predict the coordinates of the location of the screen to click, however, that introduce a couple issues: 1. the coordinates does not encode any semantic meaning of the screen, 2. Since button's or box normally is an area on the screen, not a single coordinate, when training on such data, we are punishing many correct other coordinates, 3. ViT models are patching the images, but ask the model to predict one of the coordinates from the patches is hard.
Thus the paper proposed a GUI-Actor framework, basically now the model predict the correct bounding box of the operation area like object detection models, for patches are correct labeled as 1, other patches are negative. Then there is another verifier step at the end to verify the patch heatmap to increase the correctness. Unlike traditinal object detection, the frame input is text + image and output the attention map of the patches for the action area.
Gitlab: [link](https://microsoft.github.io/GUI-Actor/)
![My diagram notes](unnamed.png)
