# FASA: FREQUENCY-AWARE SPARSE ATTENTION

Have not been reading papers regarding new attention machnism for quite long. And this paper in my opinion is quite imformative and I feel it might be a new trend.

So the paper propose a new attention called **FASA**, it is based on the natraul of RoPE embedding. Since RoPE has difference frequency on the different dimensions, and the paper find that LLM use different frequence for different purposes, the low frequence high dim locations, LLM use them for long term meaning capturing, low dim high freq locationsm KKM use them for mainly postional information, and only some of the frequence are doing the dominate importance, in other words, if we can capture some key frequences, we do not need to look at the whole specturm.

So they designed a linear layer to select the important tokens from the sentence, and for only those **key** tokens we do full attention calculation. 

You can imagine how fast and efficient that would be, so the attention calculation is not going exponentially as context goes longer, and it can also keep paying attention to the most important part.

And they also have the **FASA-M** which optimize on memoery level save the not important cache in cpu rather than gpu, and **FASA-C** which optimize on calculation which is only focusing on the key feature to make things faster.


![My diagram notes](image_1.png)