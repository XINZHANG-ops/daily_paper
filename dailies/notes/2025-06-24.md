The paper RLPR: Extrapolating RLVR to General Domains without Verifiers created a new format of rewarding function in RL learning. What they do is they abandoned rule based or model based rewarder, instead, they take the prompt + generated reasoning part then calculate the probability of generating the correct response by token, then using that probability as the reward. At the first glance, it seems similar to the supervised finetuning, which is also trying to maximize the probability of seeing the correct answer. But what makes them different is SFT use that as the loss, and this use it as a reward, which means SFT force the answer to be the only correct answer, this method only make the model moving towards the correct direction. And most importantly, it makes the RL process much easier, no need to find proper reward rules manually.
![My diagram notes](unnamed.png)
