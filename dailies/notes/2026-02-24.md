# Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device

Yesterday I was talking about using LLM on personal devices, and today this paper draw my attention. It tries to run multi-modal on phone devices, in this case, the team achieved on iPhone 17 Pro, they can generate imagine in 3 seconds, with a model of size 1.5B.

It called **Mobile-O**, so there are two main things the paper does, firstly, they create a new architecture, the **Mobile Conditioning Projector - MCP**, they removed the query tokens that connect the two types of data, text and visual, instead, they directly pass the hidden states of VLM to difussion model state.

Secondly, they invented the **Unified Multimodal Post-Training**,  what it is is that they don't use traditional text-image pair, instead they use 

$$\mathcal{S} = \{p, x_{img}, q, a\}$$

 where p is prompt for image, x is the image, q is a quetsion of the image, a is the answer of the image, so they force the model learn text2image and image2text together, which makes it use much less data to do the allignment between text and image.

Looks like they have 0.5B and 1.5B version:

Codebase: https://github.com/Amshaker/Mobile-O

Project Page: https://amshaker.github.io/Mobile-O/

Models: https://huggingface.co/collections/Amshaker/mobile-o-models

Datasets: https://huggingface.co/collections/Amshaker/mobile-o-datasets



![My diagram notes](image_1.png)