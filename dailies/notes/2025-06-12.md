The papar's name PlayerOne: Egocentric World Simulator, player one makes me interested, it borrows the name from the movie player one. What the paper trying to do is  to create a realistic immersive environment simulation. It takes in one single image, with a  sequence of human motions, and it will be able to generate a video that map the human motions with all context consistently. So what you can imagine the possibility of this, if all user wear some devices that capture the body movements, and all player can live in a same virtual env with accurate and consistent body movements. So they are using video diffusion models to generate the video and cov-nets to extract the features of different body parts for better control of the motion. To deal with the data is limited issue, they do a 2 step training they call coarse-to-fine, basically train a poc model then improve on the details by fixing some weights. This brings me back the memory of the idea of metaverse a couple years ago, this kind of research defininely bring that ideas back, but imo the hard part now is the hardware.
Github: [link](https://playerone-hku.github.io/)
![My diagram notes](unnamed.png)
