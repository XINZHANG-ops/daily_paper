# Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs

This Paper in my opinion, addressing the issue that similar to **Reward Hacking** and the **Data imbanlance** in the LLM training domain. Basically when LLM find one good strategy to answer questions, it will apply that same strategy all the time instead of use diverse strategies even with multiple rollouts. The paper says for multiple rollouts, the different runs looks like differently only on the surface, which means LLM do the same thing but in different wording, but stategically, the method is fundemantally the same. 

To stop this kind of reward hacking behavior, the model mannaully amplifies the gradients of thos **rare** strategies, but those so common approaches, their weights in the gradient are surpressed, here the paper emphasises that *the object to regularize is different rollouts - different approaches, not different tokens*. As we can imagine, this way the model will reply the questions in a more diverse way, like the paper showes, the pass@k score increases, but I think in general this might reduce the stability of the top 1 solution, if we only cares about the accuracy of the first reponse instead of diversity.

 
![My diagram notes](image_1.png)