# Towards a Science of AI Agent Reliability

This would one of the paper express the exactly same feeling when I use AI agents, and very informative when we want to build evaluators for our own agentic systems.

The paper mentioned the huge issue of agents:
1. Inconsistency, same inputs, different runs may generate entirely different outcome.

2. Fragile, a very tiny change in the prompt or tool desription will lead to different results.

3. Unpredictable, agents do not know when it fails, and still 99% percent of the time the agents is confident about the answer but the answer could be totally wrong.

Thus the paper propsed very practical test framework instead of the traditinal accuracy benchmarks, since the paper realized that an agent can do something is very different from it can consistently do something in practice.

so it test exactly what we care about:

1. Consistency, run some task many times, calculate the variance of output, path, and token costs.

2. Robustness, change the env a bit, like prompt change, json formats, check for how big the metrics changes.

3. Test for its predictability, when the agent is 80% percent confident, if it can do 8 times correct out of 10.

4. Safety, when fails, how bad the outcomes are. like deleting data by mistake etc.

What the paper finds is, bigger model does not mean stable, since it got more approaches for same questions. Capability does not mean reliable, it turns out even tho the models are more and more capable, but they are still not reliable. Models are still not predictable, even reasoning model cannot evaluate its confident level correctly.

Overall I like the paper's idea so much, I would like to introduce those metrics into the agent i'm working on in my job.

dashboard: https://hal.cs.princeton.edu/reliability/


![My diagram notes](image_1.png)