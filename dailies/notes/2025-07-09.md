I do not expect there are more room to optimize on the LoRA method, the paper SingLoRA: Low Rank Adaptation Using a Single Matrix in fact further optimized the good old LoRA trick for model finetunings. For traditional LoRA, people use 2 small matrices A and B, due to the size design of them, after we multiply these 2 matrices, we are able to add the results to the original large weights matrix, thus we only train on these 2 small ones to achieve of the goal of finetuning. However, traditional LoRA normally start A with 0, and B with random normal, this sometimes cause gradient explosion or diminishing, so people have to pick learning rate carefully. What this paper does is they use a single matrix A, now A x A to get the same thing before as A x B. Now only a half of parameters will be learnt, the update for the 2 matrix is just the update on one matrix, so now there is no problem of A and B changes in not same rate, so reduce the explosion and diminishing(I'm not entire clear on this). And also the paper find out that using single matrix is not reducing the performace since the same A is applied to Q(query) and K(Key) in the transformer independently, which means they have different As, so the language ability is not dropping.
![My diagram notes](unnamed.png)
