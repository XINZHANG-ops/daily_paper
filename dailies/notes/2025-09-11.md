The paper Causal Attention with Lookahead Keys proposed a new attention calculation mechanism. Its been quite a long time not seeing fundamental algorithm level changes, thus this change is quite exciting. In traditional attention calculation, when we calculate the next token in the autoregressive way, say we have t tokens now, and now want to predict t + 1 token, and for any of the 1 <= s < t, the key of token s only encoded information from 1 to s. However we know now we have t tokens, the paper's method will update the key for all t tokens' keys to see all t tokens, which means for s < t, it sees the future, but technically we still generate token by token, just making the new generated tokens be aware of what we generated later. This method make sense to me, and it should be adopted to new LLMs imo.
![My diagram notes](unnamed.png)
