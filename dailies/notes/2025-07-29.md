This paper SmallThinker: A Family of Efficient Large Language Models Natively is honestly very impressive to me. Unlike most LLMs that are designed for big cloud GPUs, this one is made from scratch just to run locally, like your laptop, phone etc.  What’s brutal is that they didn’t just compress(Distillation of big models) an existing model like LLaMA or GPT. They rethought the whole thing. The first big trick they used is Mixture of Experts (MoE),  but not the regular chunky version. It’s a fine-grained version where only a few experts are turned on for each token, so it saves a lot of compute. And then inside those experts, they used something called ReGLU, which basically means even inside an active expert, most neurons don’t have to do anything. So it’s double-layered effienciency. The second cool part is their Pre-Attention Router. This one’s genius. Because local devices (like phones) have slow storage (like SSDs), loading model weights takes time. So what they do is before even running attention, the model already predicts which parts it’ll need, and starts loading them while attention runs. It hides the slowness of storage with smart prefetching. Also, their attention system is special, they combine NoPE (no positional embedding) and RoPE (rotary embeddings) in a 1:3 pattern, which lets them handle long context but still keep memory low. Basically, they reduced how much they need to store during conversation without hurting performance. And the best part is they tested two models:  a 4B one that runs with just 1GB of RAM, and a 21B one that runs with 8GB, no GPU needed, ust the CPU, and it can do 20+ tokens/sec.
Github: [link](https://github.com/SJTU-IPADS/PowerInfer/tree/main/smallthinker)
Model: [link](https://huggingface.co/Tiiny/SmallThinker-4BA0.6B-Instruct)