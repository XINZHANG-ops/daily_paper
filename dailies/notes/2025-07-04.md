The paper LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion created a new method that can recover 3D object from at little as 2 images only. I did a project last year on how to use gaussian-splatting to create 3D object from a sequence of images. The way I did it was in fact take a video of the object from all different angles, normally we need a dozens of images to create a decent object, I also made a short video on how to do this step by step. And this paper's method is a game changer for this task, 2 images means you can almost do it for all kinds of objects, since you may not have chance to video a tall building from all angles. And also their model can also do query of the generated scene, people can input natural language to ask where is what object in the scene etc, this means the model also naturally has the ability to segment the semantic meaning of the image and be able to understand languages. 
Github: [link](https://liuff19.github.io/LangScene-X/)
![My diagram notes](unnamed.png)
