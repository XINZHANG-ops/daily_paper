Papers are really crazy about reasoning right now. The paper Reinforcement Pre-Training even extend it into the pretraining step of LLMs. As we all know, traditional pretraining is simply feed all the raw text data into the transformer blocks and predict for next token with cross entropy loss. But this paper makes the model to think before making prediction for every single token. So basically the model during the pretraining also doing some thinking, and a predicted token, each trajectory will be rewarded based on the correctness of the token. This is an interesting idea for sure, but I can also imagine it will extremely slow to apply such model in practice, this model thinks before every single word it says, which makes me feel kind of funny.
Web demo: [link](https://thegenerality.com/agi/)
![My diagram notes](unnamed.png)
