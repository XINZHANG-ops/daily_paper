# Toward Efficient Agents: A Survey of Memory, Tool learning, and Planning

The paper mentioned the agent using is particular costly, and in fact growing exponentially and much faster than regular LLM calls, the reasons are three:

1. long context window
2. too many tool calls
3. inefficient route planning

Each will accumulate to the long context window in a exponential way, since we all know new context is not just itself, it will append to all previous messages and resent to LLM.

IMO this paper is not really bring any new ideas, but I still want to talk about this one to really see one of recent research trend, how to solve the long chat history issue. As LLMs are getting better, they are involved in longer and longer conversations, including regular usage and coding. This should soon be a bottleneck to many tasks if there is no industry level solution to this. Paper mentioned reducing LLM size is defeinitely useful, but not a scalable method in the long run. There has to be some methods that can help LLM remember the trajectory but also not overwhelmed by all the context somehow, so far this is also one of my personal study area - casual study not profession.


![My diagram notes](image_1.png)