Paper The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason draws my attention today as its quite unusual conclusion. For traditional RL training, the most crucial thing we need to have is a good reward function, otherwise the model will totally go off. But this paper concludes during their experiments, they introduce 40% of noise into the rewards of the RL process for reasoning training, but the LLM performance still increase dramatically in math problems. This makes me think what reasoning RL is different from traditional RL, the biggest difference is RL for reasoning is not really teaching the LLM new knowledge, but bait the LLM to use its own knowledge better, but traditional RL like playing video games the model has no idea what is the game at all at the beginning. And rewarding the good reasoning can also increase the performance, some noise add can make the final response reverted, but it can still rewarding the good reasoning, thus LLM RL training is more robust than traditional RL.
Gitlab: [link](https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason)
![My diagram notes](unnamed.png)
