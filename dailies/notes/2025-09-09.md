The paper Reverse-Engineered Reasoning for Open-Ended Generation is trying to solve a very practical issue in RLHF, for problems like math, games, we have really good rewarding function, thus we can just use GRPO or any RL methods to training the model. But for open ending generations, like produce an article, its super hard to define a good reward function thus those RL methods are not really practical. So they instead skip RL, they make a method called the Reverse-Engineered Reasoning for Open-Ended Generation (REER), so they have a good answer(maybe from human), and generate a reasoning trajectory that can produce the answer, i.e, they reverse generate the reasoning from the answer instead of the other way. They collected a dataset from this method, and they will SFT model to do reasoning and write good answer using this dataset instead of rewarding RL.
Github: [link](https://m-a-p.ai/REER_DeepWriter/)
![My diagram notes](unnamed.png)
