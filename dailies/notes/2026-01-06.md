# For paper **K-EXAONE Technical Report Journey to Frontier-Level Performance of Foundation Models**

Today I want to pay attention to this paper, it is not really about a new model break a new record of any benchmarks. It is a technical report of a new LLM developed by Korea. As we know, US and China are the 2 biggest player of LLM. One reason is to train a **Large** language model, energy and chips are the 2 biggest bottleneck, as the paper mentioned, Korea wants to get involved in the game, due to the limiation of data center and GPU chips, Korea was only be able to train reletive **Small** language models below 100B parameters. 

Now Korea government and LG work together, try to make Korea catch up the game, the government started programs to supply chips, and LG will be the company to do the research, and the **K-EXAONE** would be the next LLM from Korea. 

I believe some recent work from deepseek and MoE (Mixture of Expert) really open the gate for such goals. All these new techniques bring down the price and size of LLMs which enable other countries and companies participate in the game.

There is not too much to say about the LLM itself from the paper, it use all regular techniques we know, I think what we really see from this is in the future, US and CN still will be the 2 biggest player in this area, and many other counties like Korea, Japan etc will take advantages of new techniques and contribute to the development and compete on the table.

Github: https://github.com/LG-AI-EXAONE/K-EXAONE
HuggingFace: https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B

![My diagram notes](K-EXAONE.png)