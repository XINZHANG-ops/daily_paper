For paper LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model, what we want to pay attention to is the Critic here is not referring to the critic from "actor-critic". The critic from actor-critic is the advantage estimator, normally using the approximated value function. The critic here is referring the to the reward function in the RLHF, which is the direct rewarding function we use to score the output. The paper trained such a reward function, the goal was to give rewards for the generations from the policy model, and it turns out at the end of the day the reward function itself becomes a good policy, which means a good LLM for the task. Since they trained their reward model with reasoning and everything, and they are able to make the reward and policy the same model.
Github: [link](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main/llava-critic-r1)
![My diagram notes](unnamed.png)
