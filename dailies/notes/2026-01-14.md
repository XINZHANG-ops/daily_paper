# Ministral 3
For this paper is very simple, it is a very straightforward paper today, it is the techical report or I should say the detailed training and explaining of the **Ministral 3** series model. 

The reason why I want to look at this paper today is recently I use this model a lot for my personal page, I host the local Ministral 3-3b version on my 4080s pc with Ollama, the response is like instantaneous, I do not even notice any delay, which is expected due to its size. 

A key feature of this model series as the paper mentioned is the **Cascade Distillation training strategy, an iterative pruning and distillation method**, we all know the distill training method, is teach the smaller model with the knowledge of the larger model(the entire loss distribution), and they do this in a progressive way, which means the model size goes down step by step, not from large to small directly, but from **large** to **medium large** to **medium** to **medium small** to **small**, not exactly the size I mentioned but the same idea. So not from **24B to 3B**, but **24B → 14B → 8B → 3B**.

Also I want to review myself from the paper on how the current LLMs are trained:

**Pretraining(Cascade Distillation)** **->** **Post-training – Instruct(SFT + ODPO)** **->** **Post-training **–>** Reasoning(SFT(CoT) + GRPO +ODPO)**

$ODPO \space is \space online \space DPO$

So as we can see, now the LLM training taking a lot more steps now compare what we had before, for both instruct following and reasoning we need to do SFT and RL kind of work.

Webpage: https://mistral.ai/news/mistral-3

Models: https://huggingface.co/collections/mistralai/ministral-3

![My diagram notes](image_1.png)