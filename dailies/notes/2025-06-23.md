A very ambitious and creative paper I have not seen for a long time, the paper Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights try to avoid LLM finetuning from the beginning. For people who familiar with current LLM finetuning, we are not changing the weights of the original model, instead, the current industry standard is to use the PEFT method or LoRA method. Basically it is a layer we train and insert into the model so it performs on downstream task, this method is good due to its cost effective and flexibility of adapt to different task by plug the different adapters. However the paper still think training such LoRA adapters are not efficient, there should be some sort of underlining mapping between the prompt and the LoRA weights directly. Thus what they do is they collect a bunch of finetuning job's prompt and LoRA weights, they trained a model that can take your fintuning job prompt and predict your LoRA weight. Basically they covert a fintuning training problem into a inferecing problem, which accerate the process. IMO, I don't think this can work super well in general, as it can be hard to capture the relationship between prompt and the weights, but it can work as a quick test for sure, before we do the true training, have this run and get the taste of the final result.
Github: [link](https://jerryliang24.github.io/DnD/)
![My diagram notes](unnamed.png)
