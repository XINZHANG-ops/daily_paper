# For paper **mHC: Manifold-Constrained Hyper-Connections**

It is a paper from **DeepSeek** try to improve the residual connections. As we know the regular res connections are like **x + f(x)**. And there is another type called the **Hyper connections(HC)** that improves on the naive res connection. But in the LLM domain, we can see that if we do simple x + f(x), let's say language has semantic, grammar, all kinds of different meaning in the x, and our shallow and deep decoder blocks all modifying that same x, which may make this process all mixed together, since shallow layers maybe represents different language perspective of the language. Thus people change the simple residual connection into HC, which multiple a matrix **H** to the x, and each H is also a learnable matrix parameters. The goal of this H is we keep the idea of resnet, but also be able to have a couple different **streams**, so each stream itself is a sepreate stream of resnet, but represent different language meaning. However, this paper finds that without restriction on this matrix H, the training can be numerical instability because the H matrix can be accumulated over layers. So the paper created **Manifold-Constrained Hyper-Connections (mHC)**, the goal is to make sure this accumulated H multiplication will not blow-up. The most important feature of this mapping is we need if **H1**, **H2** in **M**, we also need **H2H1** in **M** as well, thus the chained multiplication will not blow-up the end result.

![My diagram notes](all_types.png)
![My diagram notes](HC_mHC.png)

