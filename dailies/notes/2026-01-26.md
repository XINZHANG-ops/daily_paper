# SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents

One more time, the context window issue is definitely going to be a hot zone. What this paper is doing is it looks at the question from a different angle, previous papers were talking about given the long chat history, what we should keep, how do we envolve them. However this paper looks at from being at the LLM or agent's perspective, if I am given a long chat, what I should focus on? And what is even nicer is, this paper looks at exact the same issue i'm facing, coding area. The paper mentions that 75% of token ussage are spent on agents trying to read the code from using  commands like: 
$$grep / cat / file \space read$$

so even before any chat history, a lot of the time, the LLM is already overwhelmed by the larage repository codes, and model itself does not know which code should be look at so it reads all of them.

The paper's solution is train a lightweight pruning model that takes the agentâ€™s explicit **intent** as input and returns **only the code lines that are semantically relevant** to the current goal.

thus the new work flow becomes:

$$
\begin{aligned}
\text{Environment (repo / files)} \\
\downarrow \\
\text{grep / cat / read} \\
\downarrow \\
\text{Raw Code Context} \\
\downarrow \\
\text{SWE-Pruner (goal)} \\
\downarrow \\
\text{Filtered Code Context} \\
\downarrow \\
\text{Agent}
\end{aligned}
$$

You might think isn't it just a RAG for codes? In fact it is more than that, so the **grep** step is like the RAG, retrieve the relavant code chunks, what SWE_Pruner does is one step further, it goes up to the line-level, it tells the agent which lines to look at for the goal instead of the entire chunk. This is something I want to try locally, so here is the github:

github: https://github.com/Ayanami1314/swe-pruner

![My diagram notes](image_1.png)