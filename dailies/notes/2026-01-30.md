# Scaling Embeddings Outperforms Scaling Experts in Language Models

This paper does teach me something new **Embedding Scaling**. So they talk about when traing MoE models, if the number of experts reach to a point, rather than keep increasing the expert parameters, increasing the embedding parameters has better return.

One thing we want to be careful here is, increase or scale the embedding does **not** mean increase the input dimension of the tokens, rather, **Embedding Scaling** means that we assign multiple embeddings to one token, and we combine the embeddings. 

Why this is intereting? If we think about it is that, MoE is about adding multiple different activatable experts - FFN in the original FFN layer. And this embedding scaling is instead of splitting that FFN layer, we should also split the embeddings into a couple **embedding "experts"**. This is purely my words and understanding, maybe not so accurate. The Expert in MoE is one expert is good at one type of task, like math, history etc, at lease in the conceptual level. And this a couple embeddings on tokens is also a way of multiple expert, maybe token has different meaning in different context.

But the paper also mentions that the number of embeddings has U shape curve too, the marginal return decreases after certain amount.

huggingface: https://huggingface.co/meituan-longcat/LongCat-Flash-Lite


![My diagram notes](image_1.png)