The paper SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation is talking about those visual human avatar models, however the perspective is from data. One thing I always curious about is how those researches collect their large scale dataset, this paper shows me an perfect example of that. In order to train good human avatar model, they need high quality video data. They collect interview, news, presentations, talk shows, lecture video data from Youtube, in total of 15.3k. Then use human to filter out make sure each one has human is doing the talking. Then they cut the videos into clipsto 3-14 seconds each, to make sure the stability of each clip. Then they use a tool to label the human in each clip with an ID to indicate who is talking, then use YoloV8 to box the human and the movements, then they use a tool called syncnet to make sure the voice is matching the mouth movement, if higher than threshold abandon the clip. Then they unify the human ID using face detection to make sure the IDs across different clips are one to one to human. Then filter by video quality on resolution, brightness, blur, noise etc. Then use multi-model to give each clip a descrition, including the camera angle, human action, scene, content etc. Finially they got 5200000 clips, in totall 8700 hours of single human talking, and 770000 clips in totall of 1800 hours of double human talking. From this I understand the complexity and all the detail to create a large dataset, it is indeed a hard engineering problem.
Github: [link](https://dorniwang.github.io/SpeakerVid-5M/)
![My diagram notes](unnamed.png)
