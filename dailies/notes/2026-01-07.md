# For paper **LTX-2: Efficient Joint Audio-Visual Foundation Model**

Nowadays, the text2video models like sora2 and Neo are open to people to use. However they are cloased models we do not really know how they work under the hood. What we know is in the past, text2video has 2 potential paths:

1. Text -> Video -> Audio

This approach is not great since we need to sync in sounds to the video, which means audio is not part of the role of what the video is showing, which make the video not natural, my phsical details is not matching like echo etc.

2. Text -> Audio -> Video

This approach is also not great since it is making video **play** with the sound, we can imagine is good for person talking like voice, but not good for complex sounds in natural etc.

And this paper proposed the Audio + Video together at once, the word **Joint** is joint of audio tokens and video tokens when generating the output, in other words, cross attention between those 2 types of tokens. 


If we want to express in mathematical way:

**Old method:**

$p(\text{video} | \text{text}) + p(\text{audio} | \text{video}, \text{text})$

in other words we do video, then audio is conditioned on both video and text.

---

**LTX-2:**

learn the joint distributionï¼š

$p(\text{video}, \text{audio} | \text{text})$

![My diagram notes](LTX-2.png)