The paper A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code creates a repository-level security check for code as the paper name suggested. It collects from github and inside company codes that has Common Vulnerabilities and Exposures logs, the code needs to have comments, fixing commits and runnable, it is labeled the vulnerable location, then they <mask> the locations combined with descriptions, readme and ask LLM to reproduce the fix. They also build a evaluation system on evaluating how good the generated fix it. But what I feel more interesting is how they make the model generate the code is from using mask. I'm thinking in our project is there any potential in generating SQLs, we do not need to generate from scratch, we can just ask the LLM fillin the blanks, that would hugely increase the speed and reduce the error rate and correctness, but we need to build a great template datasets to cover all kinds of questions.
![My diagram notes](unnamed.png)
