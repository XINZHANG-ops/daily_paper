The paper The Entropy Mechanism of Reinforcement Learning for Reasoning Language talk about the policy entropy collapse during reasoning training. What I found interesting is first of all the entropy concept in this, i. e the policy entropy. So we can simply understand it as for next token generation, the model almost certain to pick which token, or in other words, the 1 or 2 token has almost 1 probability and the rest are near 0, so this means low entropy, the model is super confident with itself, the output is almost deterministic. And this is quite interesting to think about, I never think training process is to reduce the entropy of a bundle of parameters, and that's indeed true. And next interesting thing is the paper found the the performance of model is from sacrificing entropy, and when entropy runs out, the model reaches its ceiling. This is also something quite innovative to think about, it is like our life, as we live our possibilities runs out as aging. 
Github: [link](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL)
![My diagram notes](unnamed.png)
