The paper MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention, brings a new model trained with a couple advanced techniques like MoE, new clipping strategies, more math and coding training data etc. What worth mentioning is the model increase its output length to 80k for the larger version model. But what I want to quickly mention is the lightening attention invented by the paper. The traditional attention need to do softmax(QK)V (I omitted the transpose of K and normalization by head size). So the softxmax step involves QK multiplication which is n x n, thus the complxicity is O(n2). It is also the traditional reason why attention is costly in terms of vram and money. So what lightening attention does is use the "kernal trick", similar to svm, which converts a dot product problem into a kernal function calculation issue, which reduce the calculation by large, which means our kernal function phi, we want to have softmax(QK)V roughly the same as phi(Q) * phi(K)V, the dot product between QK is gone, and approxiametd by the kernal, and this make the softmax step O(n).
Github: [link](https://github.com/MiniMax-AI/MiniMax-M1)
![My diagram notes](unnamed.png)
