# Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?

The idea of this paper is try to verify if SAE really be able to explain the networks. What they found is even if they just use random weights decoder, it can still perform almost no difference than the trained decoder.

So SAE is a new concept for me, I really want to quick mention it here.

How SAE works is it is encode-decoder system, it got plugged into the layers we want to understand so a layer of LLM. And we feed in setences into the LLM, that layer output signals and input to our SAE, of course it won't effect the LLM forward pass. So this is like getting the data of training the SAE, then what SAE does is, it first project the signal into a huge dimension, then use some really harsh activation to kill almost all features, and the only left input into the decoder, the job of decode is try to recover the original signal, at the same time try to keep as less as activated signal as possible. So way after some training, we can input one type of setence into LLM, see what features are activated since it is sparse.

And this paper here now saies, since the decode is trying so hard to recover the original feature, it also may not really understand the feature, but rather found some random mapping that explains the features. 

This paper really bring me this new SAE thing, which something in fact quite intereting.


![My diagram notes](image_1.png)