The paper Perspective-Aware Reasoning solves a problem that traditional model only reasoning the image from the camera perspective, they manage to ask the model take the perspective of any object or person in the image to reasoning the circumstances, i.e the model can really understand the spatial relationships inside the image. How they do this is using some sort of 3D transformation, but this is not what I interested. What I think interesting is if this works so large amount of applications can be applied, robotics, online touring, NPCs in gaming etc. Also I'm also start thinking, what is the "spatial" dimension in languages? can language jobs can learn from this?
Github: [link](https://github.com/KAIST-Visual-AI-Group/APC-VLM)
![My diagram notes](unnamed.png)
