Latent Flow Transformer is a hard but elegant method to simplify the architecture of traditional LLM. Instead of the traditional layers of layers decoder blocks, the paper uses something called Latent Flow Transformer to "approxiamate" the middle blocks flow, by learning what they call transport operator of the transformer blocks to mimic the transformer blocks. I don't fully get the math of the method, but it seems working. In short, they seems to find a way to replace large middle layers with much simpler substitutes, achieved much less model size for cost effective with minimum performance loss.
Github: [link](https://github.com/mtkresearch/latent-flow-transformer)
![My diagram notes](unnamed.png)
