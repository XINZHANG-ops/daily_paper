If the paper On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification is correct, I think they have shown something quite crucial in LLM finetuning, or not finetuning anymore. They showed mathematically SFT is simply a special case of policy gradient, a RL training method. Note that PPO is just a improvement on policy gradient, and GRPO is just a different version of PPO, and DPO simplified PPO. Which means SFT is just RL. How to understand this is regular RL method will reward the correct behavior, whereas SFT is extremely strict, the method will force the LLM almost to memory the exact output. But how they exact show it mathematically I might need more time to understand. 
Gitlab: [link](https://github.com/yongliang-wu/DFT)
![My diagram notes](unnamed.png)
