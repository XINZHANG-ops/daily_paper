A new gradient clipping method, ZClip is invented by the paper. The traditional PyTorch clip_grad_norm will just compute the sum of all gradient at the a step: ‖g‖₂ = √(∑g²ᵢ), and if ‖g‖₂ > c, we scale all gradient g = g * (c / ‖g‖₂), the issue is during the entire training the Conetsnt c is not changing. This paper says this this c might be too small during early phase but too big at late phase. Thus they create a new dynamic way of finding this threshold, it uses the Exponential Moving Average. 
μₜ = αμₜ₋₁ + (1-α)‖g‖₂
σ²ₜ = ασ²ₜ₋₁ + (1-α)(‖g‖₂-μₜ)² 
z = (‖g‖₂ - μₜ)/σₜ
and threshold isthreshold = μₜ + (z_thresh²/z)*σₜ, where z_thresh is hyperparam we input. From the plot we can see the training is more stable, and less spikes for all different learning rates.
Github: [link](https://github.com/bluorion-com/ZClip/tree/main)
![My diagram notes](unnamed.png)
