Scaling Law for Quantization-Aware Training is a rare study recently on quatilization of models given the recent new techniques to reduce training cost using methods from R1 paper. There is nothing fancy from tha paper but maybe a quite good finding to remember that: 
Quantization error decreases with larger models but increases with more training tokens and coarser quantization granularity, and identified that activation quantization in the FC2 layer is the primary bottleneck for W4A4 QAT performance.
The paper get the finding from 268 experiments. 
The result of more training tokens will increase the qualization error is quite counter intuitive, and the paper explains because as the model is exposed to more data, it learns finer distinctions and representations. These richer representations are more sensitive to quantization.
![My diagram notes](unnamed.png)
