# DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing

This is one kind of paper I love, since the model they proposed I can in fact get my hands on. So this paper talks about the issue the that traditional multi-model need to be so large, >10B params.  Where as they proposed this small model of 5B param, called DeepGen 1.0, from their test they can outperform 80B model. 

So how they do it is the most important thing. So unllike traditional VLM - Vision-Language Model, only use the last layer output as feature, they combined 6 layers, called Stacked Channel Bridging - SCB. From low, medium and high as different features. Here low, medium and high are not resolution, but the depth in the layers.

And they introduced a learnable token called think token, which plays the role as I  interpret it as latent CoT thinking. So this token itself represents the traditional reasoning steps.

And they also created new training strat like free the model only train the connector and think token, and better LoRA SFT etc.

But I pick this one as I can try, I checked their weights as avaliable at huggingface.

github: https://github.com/DeepGenTeam/DeepGen

huggingface: https://huggingface.co/deepgenteam/DeepGen-1.0

dataset: https://huggingface.co/datasets/deepgenteam/DeepGen-1.0


![My diagram notes](image_1.png)