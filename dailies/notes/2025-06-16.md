The paper ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning is a quite different paper, it is not giving a new model, nor a new algorithm but a new dataset in medical reasoning. The way they do it is first collect 19.5k medical questions from public medical question benchmarks. Then use Qwen, DeepSeek and HuaTuo (3 chinese LLMs) to generate 1750k CoTs, and use LLM as a judge for the CoTs to evaluate them, give them a hardness ranking based on how many correct traces are generated for the question. Then use openai-o1 to give a better reasoning CoT for those not performing well(Hard ones). Overall the process is not very hard, but this paper reminds me the ImageNet work done decades ago by Li Fei Fei. When everybody focusing on better model and better algorithms, there should be patience researches put their time on developing better datasets. People normally value such work less than the new inventions, but all true machine learning people know, the imporance of dataset is not lower than the models.
Dataset: [link](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed)
![My diagram notes](unnamed.png)
