The paper Rethinking Verification for LLM Code Generation: From Generation to Testing proposed a new method (dataset more accurately) to test out coding correctness. And this coding benchmark is for human writing code instead of AI coding. Traidtinally the evaluation is based on a small, similar evaluation test set, which over estimated the quality of human coding, also AI judgers tends to find errors that the AI itself easily make instead of human. What this paper does is they collected true human coding from for example Codeforces, AtCoder, which has a lot of real human error coding and correct coding, the model abstract the rules or patterns of human common mistakes, then based on these patterns, AI designed a couple test cases for each code piece. They make this high quality test set as CodeCompass, a new coding benchmark. This is quite informative for our relevancy benchmark, we should also collect our AI's common mistake, and design test cases for those.
Github: [link](https://github.com/open-compass/SAGA)
![My diagram notes](unnamed.png)
