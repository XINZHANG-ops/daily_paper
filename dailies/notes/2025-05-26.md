Interestingly, today papers are quite about handling long input context. QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning is about training the LLM is able to do good reasoning when receiving long context with a couple training phase gradually. But more interesting paper is QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization, this paper's method can be applied to all LLMs like RAG. The paper takes a transformer block and specifically train it to get the compressed information from a large context, as so, the compress is accurate to token level. And what we will send to the final LLM is the compressed info relevant to the question. This is like RAG but technically better than RAG as it also keeps the relationship of all the chunks naturally(i.e no chunks).
Github: [link](https://github.com/Tongyi-Zhiwen/QwenLong-CPRS)
![My diagram notes](unnamed.png)
