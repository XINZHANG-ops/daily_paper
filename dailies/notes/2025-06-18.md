The paper Reasoning with Exploration: An Entropy Perspective modified the advantage formula in the PPO or GRPO. And an intuitive way of understanding this is during regular RL process, the model can trap in the goodhartâ€™s law, which means the model is so smart to get the rewards from the RL, instead generate the good answer we expect, one such case is model refuse to think, but give the high score short reasoning directly. What the paper did is, or uncertain tokens (high entropy ones) encourage the model to think more,  basically we manually add a bit advantages to those tokens, make them more possible to be correct token, in other words, the model would explore more thinking paths.
![My diagram notes](unnamed.png)
