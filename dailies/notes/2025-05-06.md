The paper "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play" really answers a question I always have, is why GPT4o voice chat is so instant. What I always have in mind is the voice got transcript to text, then send text to LLM, then LLM response text, then convert text to voice, that is the slowest possible way. This paper reminds me the GPT4o is already a multi-modal model, thus what is happening is exactly like we type into LLM, it can output tokens immediately by "voice" token one by one. Which mean your voice input is exactly like when you typing text, and reponse is exactly like streaming text tokens, but instead everything is in voice. I was quite silly was not realize of this. And this paper further improve on that end2end model, it improved to hierarchy architecture, has more prebulit voice training, and lower latency.
![My diagram notes](unnamed.png)
