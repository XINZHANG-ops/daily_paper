The paper TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs try to solve the problem of multi-modal not performing in temporal grounding and highlight detection tasks, and SFT training is not flexible for such tasks. The paper changed the GRPO from using only on-policy samples combined with off-policy samples, which means the samples used for RL training is not only from the target policy model, and also in case of all off-policy sample has the highest reward and on-policy one has negative rewards, they restrict off-policy reward to the 80%. And they rescaled the rewards, like applying a transformation function, compress high rewards, and stretch low rewards. And they use both CoT answering training and direct response training for better performance on different tasks.
Github: [link](https://github.com/HVision-NKU/TempSamp-R1)
![My diagram notes](unnamed.png)
