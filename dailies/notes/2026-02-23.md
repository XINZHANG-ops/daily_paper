# ARCEE TRINITY LARGE TECHNICAL REPORT

Papers like this reminds me the LLM, in this case, the open source LLMs, will just keep getting faster and cheaper. This paper imo is a bit technical in terms of the methods, so I will not explain the details well since I do not think I fully understand. But briefly, they were mentioning about the unstability of training large MoE models, expensive of training long context models. 

So they invented extremely sparse MoE, a 400B model will only activate 13B params, this is amazing, what that really mean is consumer level GPU, something about 16GB vram can run 400B models, which is impossible before. 

So my really take away is, I feel in the future production level product for those companies they will use closed sourced api, but gradually, individuals like us can really host relative similar performace models locally for our day to day tasks, they can be on cars, fridge, game console, and everything.

Love to see such progress each day.

huggingface: https://huggingface.co/arcee-ai


![My diagram notes](image_1.png)