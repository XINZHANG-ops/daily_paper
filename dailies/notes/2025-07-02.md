For paper SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning, it seems like the paper found a similar fact as one of the paper I read a couple days ago, the fact is the learnt reasoning capability of LLMs are transferable. So this paper trying to solve the issue that traditional reasoning RL trainings are not scalable, which requires a ton of human labeled reasoning data, and human designed rewards for reasoning steps, and this need to be change from domain to domain. Instead, the paper ask the model to play those zero-sum games like TicTacToe, Kuhn Poker or Negotiation games, the model will play with itself in a online learning way, and since these games are zero-sum there will a winner and loser, and naturally assign rewards accordingly. The paper has done other works too but the above zero-sum self-play online learning I think is biggest thing they found. And what they showed is by playing these games, the model performs better on math problem's reasoning, and in general reasoning abilities, which shows that the reasoning capability is a generalizable skill the model can learn from one thing and apply in another thing.
Github: [link](https://github.com/spiral-rl/spiral)
![My diagram notes](unnamed.png)
