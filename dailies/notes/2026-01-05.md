# For paper **Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization**

My first glance of the paper title is quite confusing, why **agent** has anything to do with **policy optimization**. Since agent is basically a workflow of LLM to do certain tasks, whereas **policy optimization** is RL training method.
From quick reading of the paper, the author mentioned a 2 layer method, first one what they call it as **text lora**.
How it work is, for a task, we have a working agent, the goal is we want to improve this agent by giving it better prompt, in this paper is better memoery of experience. So we ask the simple agent genearte a couple rolls or routes to solve a task, some task perform good, some are bad, and these runs as treated like a group in **GRPO**. And these experience are summarized as long term memory, and when the agent are doing the task in the future, this memory will be given so the agent is expected to perform better.

Another layer of the method is in fact training the LLM as an agent. So similar to the above **practice** approach, we still roll out many runs of an agent, but now we collect those trajectories as a real group and train the LLM using **GRPO**. Which convert the LLM itself as the agent. 

So in fact the creativity of this paper is they redefined the term **agent** a bit, agent becomes a formula of a couple receipts: `Agent = env + tools + context + policy`, in this way we can autimate the production of agents, we can produce batches and batches of agents effciently, agents themselves can be trained instead of we build a good agent.


And why it is callded Youtu-Agent Framework is because it is from tencent Youtu lab.

Github: https://github.com/TencentCloudADP/youtu-agent

![My diagram notes](Automated_generation_mechanism.png)
![My diagram notes](Training_Free_GRPO.png)
![My diagram notes](End2End.png)
