The paper Pixels, Patterns, but No Poetry: To See The World like Humans is extremely interesting paper, especially for me, the paper aim to test MLLM's real vision tests, and created a Turing Eye Test(TET).  This is interesting for me since I personally is a green-red color blind. The tests includes:
HiddenText, texts blend into a scene
3DCaptcha, like those very noisy anti-bot characters recognization on websites
ChineseLigatures, this one is assemble a chinese phrase into a picture and ask model to tell the origins words, for this one even myself have a very hard time to recognize. 
ColorBlind, this is the common Ishihara Test people use to test for color blind.
And the paper compared gpt4o, claude, gemini-2.5 and QWEN, and all models, for all tests are all most all get 0% correctness, which mean the models purely cannot do any. What is interesting is the paper think the reasons are the models often focus on the wrong section of the image, especially for the color blind one, the model just cannot know which one is noise which one is information. 
And the paper also tried to fine-tune the model in 2 ways, one is purely fine-tune the language ability, does not improve the performance at all, second is purely fine-tune the vision ability, then the performance bump from all 0 to about 95%. And this shows the MLLM's brain is ok, but simily vision is bad.
Github: [link](https://turingeyetest.github.io/)
![My diagram notes](unnamed.png)
