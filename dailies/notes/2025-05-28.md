Have you ever thought about LLM can output multiple tokens at a time? we know there were paper doing this study. Exploring the Latent Capacity of LLMs for One-Step Text Generation proposed a quite unique approach, without autoregression, so traidtionally we know next token is based on previous ones, but this paper asked can we remove autoregression. So what they did is trained a special embedding, this embedding should be much more richful then token embeddings, and each embedding would output hundreds or thousands of tokens in one forward pass. So my understanding is, this new generation is like concatente a series of traditional tokens into one big "token", and each token now is a context piece already(Many words), so technically, it generate texts on a larger scale, not traditional token level, but paragraph level.
Github: [link](https://github.com/Glebzok/OneStepLLMGeneration)
![My diagram notes](unnamed.png)
