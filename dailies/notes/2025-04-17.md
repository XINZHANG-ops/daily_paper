Quite astonishing achievement by the BitNet b1.58 team. What they achieved is extreme quatilization technique, they managed to model weights are to log2(3) = 1.58 bits during the forward pass. Which means the weights have only 3 possible values {-1, 0, 1}. They make the model weights ternary! As we know quatilization is a method to reduce memory and increase inference speed when loading LLMs, what they have done is just so extreme.
They open sourced the model here: [link](https://huggingface.co/NousResearch/OLMo-Bitnet-1B)
![My diagram notes](unnamed.png)
