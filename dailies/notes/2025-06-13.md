The paper Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation paper gives me a better understanding of how LLM generate responses. The paper raised an issue that many questions for LLM can be decoupled into parallel questions, but the LLM still need to run linearly answer the questions step by step. Imagine I asked a simple question: "what is 1 + 1, and what is 2 + 2?". Obviously the question composed 2 questions can be parallelled generated, but LLM still need to generate the answer one by one since it is an autoregressive model. Thus the paper finetuned the model with TAGs 
<Parallel> 
  <Goal> 
    <Outline> 1 </Outline> 
    <Outline> 2 </Outline> 
      <Path> 1 </Path> 
      <Path> 2 </Path> 
  </Goal> 
  <Conclusion> </Conclusion> 
</Parallel>
And the when the algorithm detects the Parallel tags, it will use the same KV-Cache, send the different path as a batch to the LLM and generate each path in a parallel way(Since it is a batch use the same forward pass). Then when all path is done the answer will be concluded and returned. 
In another way, the paper convert the model from thinking like a cpu (linear way) to a gpu (parallel way). Which is very inteteresting. 
Github+demo: [link](https://multiverse4fm.github.io/)
![My diagram notes](unnamed.png)
