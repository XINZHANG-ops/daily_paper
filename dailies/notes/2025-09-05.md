For paper Towards a Unified View of Large Language Model Post-Training, the main idea is that SFT and RL share a common optimization goal and can be expressed under a unified policy gradient estimator. Building on this, the proposed Hybrid Post-Training (HPT) algorithm introduces a performance-based gating mechanism: for each question, the model samples several rollouts and checks their correctness with a verifier. If the success rate P is low, the update uses the SFT loss on demonstration data to provide guidance; if P is high, the update switches to RL loss (implemented with GRPO) to encourage exploration. In practice, HPT alternates automatically between these two modes, dynamically adjusting the balance of imitation and exploration, which leads to better reasoning performance and stability compared to fixed-stage or fixed-ratio methods.
Github: [link](https://github.com/TsinghuaC3I/Unify-Post-Training)
![My diagram notes](unnamed.png)
