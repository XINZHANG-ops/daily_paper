One of my fravorite paper recently, the paper "Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test" investigate the grokking behiavior of LLMs. The meaning of grokking is during training, people notice the loss function drops, but the real performance or the loss on val set not drop until a long time of training, and suddenly drops. What they conclude is at the beginning of training, the model simply memeorize the information from training, thus even if the loss in training drops, it doesn't mean it learnt the data, and as the model keep training, at a point it suddenly understand the data and be able to perform. What they ovberse is for QA data this happens earlier than coding and math data. And also for MoE models, we can also see when the model tend to use single expert to answer the question is also an indicator of the model learnt generalization. This also tells us the training loss is not a useful indicator, val loss is useful but it is expensive for large LLMs, there is also other metrics people can check and make sure model learnt to generalize.
![My diagram notes](unnamed.png)
