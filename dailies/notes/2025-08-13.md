The paper Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models draws my attention due to the words "Diffusion Language Models". This is in fact my first time know diffusion for language models, since I always thought it is only for image generation like stable diffusion. And it turns out other than the auto-regression LLM like the well-know models like GPT, LLAMA etc, there is another approach using diffusion models. How it works its exactly like image, we first randomize tokens, and we mask tokens, and diffusion model will predict these masked, and we will remask the lower confidence ones and run diffusion again, just like image generation we need to pass the model a couple of times. Obviously this way we can generate a couple tokens at a time not just one. And this paper focusing on putting time as a feature into the denoise steps, which I don't really care too much about, since what this paper really let me learnt is this new language model path.
Github: [link](https://github.com/aim-uofa/dLLM-MidTruth)
![My diagram notes](unnamed.png)
