GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation introduced a new GoT-R1 training to give image generation much accurate result from Prompt. They applied the GRPO loss, so what interest to me from this paper is the reward functions they used, which includes 4:
Prompt-Image Reward: Measure how the generated image matching the prompt instruction
Prompt-Reasoning Semantic Reward: Measure the genearted reasoning with prompt in terms of semantic meanings.
Prompt-Reasoning Spatial Reward: One of the crucial thing mentioned by the paper, is the spatial relation of the objects, so they also introduce this spatial reward between prompt and reasoning.
Reasoning-Image Reward: Lastly, to make sure the image generation follow its reasoning plan, measure the reward between the image and reasoning.
So given current date's models all using RL to training for better performance with reasoning, the reward function is something that distinct different approaches, so it is intereting to see what people apply in their papers.
Github: [link](https://github.com/gogoduan/GoT-R1)
![My diagram notes](unnamed.png)
