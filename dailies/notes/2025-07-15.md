Another favorite paper for a while. The paper Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination trying to investigate the RL indeed give model reasoning ability or just simply memorization, the results are very strong! 
So some previous research found for math reasoning training using data MATh-500 on QWEN2.5 improve the performance massively but not on llama using same RL method. However this paper proves that the reasoning is simply QWEN model have seen this dataset before whereas llama did not. The way they do it is truncate the dataset see if the model can tell what the rest of the testset, and it turns out that QWEN can recover 54% of the training data. 
The paper dive deeper, so they created a completely new dataset which make sure the model did not see before. And on this new set, the magical performance on QWEN is gone, which again proves previous research just make the model memory the data.
Lastly the paper chanllenges the ideas bring by some other paper that even random reward or negative(reward LLm when answer is wrong) can improve the performance, and they showed that using the pure data, only the correct reward can improve performace.
Thus they conclude that the MATH 500 dataset is highly leaked to training data, this is also my guess for a lot of those public benchmarks, and potentially a lot of benchmarking dataset are already in the training data of those LLMs.
![My diagram notes](unnamed.png)
