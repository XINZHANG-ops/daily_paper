This becomes interesting, as we can still recall the hype from Deepseek-R1 beginning this year, it showed the pure RL training give the model self-reflection and reasoning capabilities. However this paper from Tsinghua University showed that RL in fact not improving base model's reasoning capability, instead, models generate reasoning paths already within the base model's output distribution, meaning RL biases the model toward higher-rewarded solutions rather than creating entirely new reasoning abilities. And they also showed different RL algorithm has no big differences, i.e PPO, GRPO etc. Now I'm confused, from my own training, I do see the model can do self-reflection, but maybe that is the model can do itself already, RL just bait out that nature.
Their Post: [link](https://limit-of-rlvr.github.io/)
![My diagram notes](unnamed.png)
