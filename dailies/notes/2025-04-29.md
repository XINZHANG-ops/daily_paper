Has been reading about too much of reasoning methods, this time I want to focus on the paper "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models" which researches about image generation with causal transformers(The autoregression model). The main innovation of the paper is it reduce the cost and improve the speed of training by using a trick "Token Shuffle". The idea is they bundled the close image blocks as one single token, and after the transformer block un-bundle the token back into original blocks. This is smart, as we can imagine the training for tokens decrease dramatically, and it take advantages of image has the property of localization(I invented this), basically means close pixels or blocks in a location of the image are similar to each other, so they can train to get much high resolution due to this drop in cost. This reminds me of the early days of NLP, people believe words close to each other has similar meaning and context.
Github: [link](https://ma-xu.github.io/token-shuffle/)
![My diagram notes](unnamed.png)
