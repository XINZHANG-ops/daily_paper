Just a new knowledge piece for me to see how a multi-model architecture can be. Text, vision and audio tokens are all input together and processed in parallel together.
![My diagram notes](unnamed.png)
