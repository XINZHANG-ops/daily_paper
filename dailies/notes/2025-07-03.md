All 3 papers today are about vision models. The one I feel more interesting is Depth Anything at Any Condition. Depth image is a crucial image feature for many tasks, including classification and control net. However traditional depth model performances good only if when the lighting condition is good, i.e with enough lighting and no fog etc. However a lot of real life images including night condition and unclear environmental conditions, to solve the issue, the paper proposed 2 new methods, firstly, do not train unsupervised unlabeled data, instead, in addition to the good quality labeled data, manually add noise to mimic the bad lighting conditions, and the target is try to make the model predict the same depth image as the good condition. Secondly, add spatial information of the image, define the distance between image patches, now the model has a better spatial understanding before making the prediction for depth.
Github: [link](https://github.com/HVision-NKU/DepthAnythingAC)
![My diagram notes](unnamed.png)
