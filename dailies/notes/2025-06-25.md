The paper xSkywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs move one step further on better coding LLM agents. In order to do this, they collect data from 2531 Github repositories, collected 10196 real python bug fixing tasks, with human comments, patching and tests. Then they fintuned a 32B LLM using the data, which they found there is clear evidence for the data scaling law for the task, which means the more training data the better the perforamce, and they do not even observe it is saturated. I hope community can apply this model easily soon, which could be a very use tool for python programmer. 
Model: [link](https://huggingface.co/Skywork/Skywork-SWE-32B)
![My diagram notes](unnamed.png)
