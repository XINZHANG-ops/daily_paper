Oh wow, this is so cool to look at their demo the Hogwild! Inference. What they do is they share the attention cache(K-V) across LLMs, and the different LLM generate at the same time. More interestingly is, these LLMs are not just generate different answers like N generations, but each LLM like reading other LLMs minds, so they generate output in a collaborative way. Again, such a cool idea, the demo blows my mind.
Github: [link](https://github.com/eqimp/hogwild_llm)

![My diagram notes](unnamed1.png)
![My diagram notes](unnamed2.png)
