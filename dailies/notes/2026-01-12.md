# Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards

This paper mentions the issue regular RL training rewards is either 0 or 1 on the final answer correctness, which might not rewarding the agent entire reasoning behavior. The paper gives a good example of agent make a quite long back and forth and searching behavior chain, and give  with a number value as the final answer. The answer is correct, and got rewarded 1. However due to the quite long thought process, any mistakes, bad assumption, hallucinations the agent makes along the way will be equally rewarded. And obviously that is not what we want especially for those deep search agents. So the paper proposes a method called 

$$Citation-aware\space Group\space Relative\space Policy \space Optimization (C-GRPO)$$ 

Unlike the regular thinking that we try to reward the thought process of the agent, instead the paper try to reward the **evidence** the agent touched during the entire process. Which means if the agent searched a webpage, we will evaluate if that is a relevant search, if agent referenced a thing, we will evaluate that thing. The core idea is if our agent really find the supportive evidence to find the final answer. So instead of test if the agent thinking correctly, but evaluate if it finds the correct facts.

Github: https://github.com/THUDM/CaRR


![My diagram notes](image_1.png)