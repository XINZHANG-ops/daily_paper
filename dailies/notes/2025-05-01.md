The "UniversalRAG" paper is a quite informative paper from recent readings. It proposed a framework to retrieve from a multi-model database, including text, image, video. And a given user question will be categoried into which type of source data maybe relevavnt. For example:

"What is the birth date of Alan Turing?" is a text retrieval,
"Describe the appearance of a blue whale." is an image retrieval,
"Explain how Messi scored his goal in the 2022 World Cup final." is a video retrieval. 

this classification can be done using purely prompt or fintuned model. 
The framework in fact also consider different granularity of the retrieval which is also very useful, so in fact  for text it considers: paragraph, document, for image is image, for video is: clip(short) and video(long).
The technique is interesting and could be applied in ACE for future multi-model retrieval.
Github: [link](https://universalrag.github.io/)
![My diagram notes](unnamed.png)
