The paper Group Sequence Policy Optimization is a bit of technical paper, it creates a new RL loss method called GSPO, aim to replace GRPO loss. As we know, GRPO is a simplfied loss to replace PPO, since it does not require a value function. It generate multiple answers for same input, and calculate the importance ratio on the token level(we can understand the importance ratio is how much our new model like something), then do the optimization. The main difference of GSPO to the GRPO is it calculates the importance ratio of the whole sequence instead of the tokens, it make more sense since the reward is given an sequence level not token level, and it works better for the current trending MoE models, since MoE model each token will be produced by different expert, and the token level importance ratio will be very unstable since each can from different route, and using sequence level importance hugely improve the training stability.
![My diagram notes](unnamed.png)
