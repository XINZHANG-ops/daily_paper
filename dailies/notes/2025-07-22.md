The paper GR-3 Technical Report is from the Byte Dance company, the reason why I think it is interesting is it is not a common technical report and some new LLMs, however it is researching on the even "advanced" topic the robotic policy model GR-3 which focusing on robot control. Since it is robot based, the input is naturally from vision, it is a vision-language-action or VLA model. What it can do is based on some language inputs, plus vision inputs, output robot movements. What makes me think it really behave like human is, like how human kids learn, they mimic the adults hehavior, learn from observation, and this robot can do exactly the same, it learn from human trajectory data(one of the 3 sources), and it also can do high efficiency finetuning, which like a human kid learn a new skill, it can learn from 10 times of observation. I think researches like this is very remarkable, this really means in the future if you put a bot next to a human worker, it can learn from just observing what this worker does.
Project: [link](https://seed.bytedance.com/en/GR3)
![My diagram notes](unnamed.png)
