# OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models

This paper mainly talks about the inbalance between audio and video in traditional multi-models, when doing token compression, either seprete them and compress sepreately, or use audio to guide the video. And this pape proposed remove redundency in video first, then use the core video to guide the audio.

But to me I learnt this new term **token compression**, this is used in multi-model since a lot of the time video and audio data has a lot of noise, for example background, silence moment etc, and those are not the core info related to the task. And those information can spend thousands of tokens. Unlike language, our text language we do not input unrelated tokens, and often is our text is lacking of info. So I'm thinking, this is why for text tasks we often have tools like retrieval, graphs to make the context more, but in audio and video we prune noise which is interesting.

However, I'm also thinking the retrieval maybe add useless context too which could be noise, in video they compare frames to keep those unique key features - sounds like COLMAP to find key points from the images. Maybe we can apply similar thing in retrieval to limit tokens too?


![My diagram notes](image_1.png)