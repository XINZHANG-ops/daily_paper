The paper Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following states a fact that, the LLM's reasoning ability and instruction following is a trade off given current training methods, which means the LLM can either have good reasoning ability or it can follow the prompt nicely, it cannot have both, the underlying reason is these 2 concepts are contradict to each other, reasoning means the model can think "freely", following instructions means the model cannot think "freely", which is a hard problem for models. What the paper trying to solve is this trade off, what they do is they do not train the model with all restriction at the same time, for example if a prompt ask the LLM to do a, b and c, the paper will decouple these instructions into a, b and c, and train the model to accomplish a first, then add b, then add c. And based on this, they make a new rewarding, one answer finished a, one answer finished a and b, and they will ask the model itself to judge which is better, they embed this into GRPO and train the model to minimize the trade off.
Github: [link](https://github.com/Rainier-rq/verl-if)
![My diagram notes](unnamed.png)
