# LLaDA2.1: Speeding Up Text Diffusion via Token Editing

The paper title does not sounds existing, but after I dig into how diffusion language model works I really wantto get my hands on the model. So Compare to auto-regressive LLMs, diffusion language models are like diffusion image models, the tokens are generated with iterations. So the process is like, when receive a prompt, it extend it say 200 tokens, all are **[Mask]** tokens, and then the model runs iteratively predict what each mask should be, when the prediction confidence level above threshold, model replace the mask with real token. So the model do this block by block, untill it sees the end token in that block, the generation stops. So compare to autogressive way, the tokens are generated in a parallel way, which makes the model much faster.

But this paper addressd an issue that this prediction has one issue before, once the model replaced mask with a real token, it will not touch it anymore, which makes it fragile to errors, for example, the model predict postion 1 as walking, next position as running, both maybe make sense in the context, so both are correct, but when we have both it is not correct anymore, which makes future iteration harder, so harder to meet the confidence level threshold and the generation becomes slow. 

As the title suggested, now, they allow real token be able to be changed, not just the mask token. But it use a different confidence threshold than the mask changing threshold. And now if model think one token is not good it can change it.

Overall the the diffusion language idea is very cool, I want to test it.

![My diagram notes](image_1.png)