My recent focus on my weekly writing is about diffusion models and stable diffusion, and the paper Video World Models with Long-term Spatial Memory is something I feel quite interesting about. The current trend of image and video gen is moving away from the traditional SD1.5 architecture which implemented using Convolutional + cross attention. And the most recent SD3 and 3.5 changed to transformed(ViT) diffusion architecture. And the paper also show case the ViT diffusion on video generation. What is new is they want to solve the biggest issue in video gen - inconsistency. A while ago I had a project on video gen, that time we were using SD1.5 + animatediff, but we were facing inconsistency so much and the video quality was not impressive, so I know how important the consistency is in video Gen. They bring a 3 sources for consistency, most recent frame, static cloud and history, these works like prompt conditioning get injected into the generation process of new frames. 
Github: [link](https://spmem.github.io/)
![My diagram notes](unnamed.png)
