# Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering

The paper talks about the huge pain point when using those LLM agents, the limited context window with super long chat history. This paper is not aiming to increase the context window, since even me think thats a black hole, you never have long enough window size - not to mention the costs given unlimited long context. 

I am super interested in this topic since recently I apply the claude code so much on my projects, and one of the common issue is after a couple rounds of chats, the model out of contexts windown and it has to do a compact of the chats, i have to admit it does a good job most of the time, it can keep doing the tasks, however i have to say normally after a compact behavior, the new generated code indeed forgot a ton of details we chatted previously, and it will make some old mistakes again, back to the original naive approach, and I re-adressing those issues is a waste of context window again and the issue just repeats. 

This paper mentions the exactly same issue i was running into, the LLM overwhelmed by  a ton of details in not just a couple rounds of chats, but maybe even in days or even weeks of agent tasks. We can imagine how much important designs, details, architectures might be forgotten by the agent.

**This paper proposes: In long-term tasks, what information should be forgotten, what should be compressed, and what should be transformed into long-term capabilities?**

The paper calls it **Cognitive Accumulation**, which means a **Hierarchical Cognitive Caching (HCC)**, so basically, I really like this paragraph from the paper I want to paste it here:

**a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy.**

They have different levels or layers of memory, and one setence summary is the ask the LLM convert the current information into future conclusions.

But different from previous existing methods: **hierarchical memory**, **multi-layer context**, **retrieval abstraction**, previous methods more like how to manage to to store the information, but this paper talks more about how the **information envolve**, a piece of info, should we keep it now? should we upgrade it? should we change its level? should we remove it? 

I might apply this paper in my personal project to make my agents knows all my papers.

![My diagram notes](image_1.png)