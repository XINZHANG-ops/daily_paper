
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-10-15 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* Âç°ÁâáÂÆπÂô®Ê†∑Âºè - Êñ∞Â¢û */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            height: 600px; /* Âõ∫ÂÆöÈ´òÂ∫¶ */
            cursor: pointer; /* Â¢ûÂä†ÊåáÈíàÊ†∑ÂºèÊèêÁ§∫ÂèØÁÇπÂáª */
        }
        
        /* Âç°ÁâáÈÄöÁî®Ê†∑Âºè */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
        }
        
        /* ËΩÆÊí≠Âç°ÁâáÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%;
            transition: transform 0.5s ease, opacity 0.5s ease;
        }
        
        /* ÈùûÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* ÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* Á¨¨‰∏ÄÂº†Âç°ÁâáÔºàÊñáÊú¨ÂÜÖÂÆπÔºâ‰∏çÈúÄË¶ÅÊªöÂä® */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* Á¨¨‰∫åÂº†Âç°ÁâáÔºàÊµÅÁ®ãÂõæÔºâÊîØÊåÅÊªöÂä® */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow: auto !important;
            padding-bottom: 50px; /* Ê∑ªÂä†Â∫ïÈÉ®Â°´ÂÖÖ */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-height: none; /* ÁßªÈô§‰ªª‰ΩïÈ´òÂ∫¶ÈôêÂà∂ */
        }
        
        /* ‰º†ÁªüÂç°ÁâáÊ†∑Âºè */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        
        .paper-card p {
            margin: 5px 0;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* Âç°ÁâáËÆ°Êï∞Âô® - Êñ∞Â¢û */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* Êîπ‰∏∫Âõ∫ÂÆöÂÆö‰ΩçÔºå‰∏çÈöèÊªöÂä®ËÄåÁßªÂä® */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* Â±Ö‰∏≠ÊòæÁ§∫ */
            width: 90%;
            max-width: 500px; /* Â¢ûÂä†ÊúÄÂ§ßÂÆΩÂ∫¶ÔºåÈÄÇÂ∫îÈïøÂÜÖÂÆπ */
            max-height: 80vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶ */
            overflow-y: auto; /* ÂÜÖÂÆπËøáÂ§öÊó∂ÂèØÊªöÂä® */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* Á°Æ‰øùÊòæÁ§∫Âú®ÊúÄ‰∏äÂ±Ç */
        }
        
        /* Ê∑ªÂä†ÈÅÆÁΩ©Â±ÇÔºåÈò≤Ê≠¢ÈóÆÈ¢òÂç°Ë¢´ÂÖ∂‰ªñÂÜÖÂÆπÈÅÆÊå° */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* ‰ΩøÁî®JavaScriptÊéßÂà∂ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫ÂíåÈöêËóèÔºå‰∏çÂÜç‰ΩøÁî®hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            hyphens: auto; /* Âú®ÂøÖË¶ÅÊó∂‰ΩøÁî®ËøûÂ≠óÁ¨¶ */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* ÈïøÊñáÊú¨Â∑¶ÂØπÈΩê */
            display: block; /* Á°Æ‰øùÊòØÂùóÁ∫ßÂÖÉÁ¥† */
            white-space: normal; /* ÂÖÅËÆ∏Ëá™Âä®Êç¢Ë°å */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* ÈïøÊñáÊú¨ÈÄâÈ°πÁöÑÁâπÊÆäÊ†∑Âºè */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* Á°Æ‰øùÂºπÁ™ó‰∏≠ÁöÑÊåâÈíÆÊñáÊú¨‰∏ç‰ºöÊ∫¢Âá∫ */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* ÈÄÇÂ∫îË∂ÖÈïøÈÄâÈ°πÊñáÊú¨ */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        @media (max-width: 768px) {
            .paper-container {
                flex-direction: column;
            }
            
            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                height: 650px; /* ÁßªÂä®ËÆæÂ§á‰∏äÈ´òÂ∫¶Ë∞ÉÊï¥ */
            }
            
            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }
            
            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }
            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
            }
        }
    
        /* Personal Takeaways Section Styles */
        .takeaways-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .takeaways-section h2 {
            color: #ffffff;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid rgba(255, 255, 255, 0.3);
            padding-bottom: 15px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        .takeaways-content {
            background-color: rgba(255, 255, 255, 0.95);
            border-radius: 8px;
            padding: 25px;
            line-height: 1.8;
            color: #333;
        }

        .takeaways-content h3 {
            color: #667eea;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .takeaways-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .takeaways-content p {
            margin: 15px 0;
        }

        .takeaways-content ul, .takeaways-content ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        .takeaways-content li {
            margin: 8px 0;
        }

        .takeaways-content blockquote {
            border-left: 4px solid #667eea;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }

        .takeaways-content pre {
            background-color: #f6f8fa;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            border: 1px solid #e1e4e8;
        }

        .takeaways-content code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d73a49;
        }

        .takeaways-content pre code {
            background-color: transparent;
            padding: 0;
            color: #333;
        }
    
    </style>
</head>
<body>
    <h1>2025-10-15 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/woven.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>Spatial Forcing: Implicit Spatial Representation Alignment for
  Vision-language-action Model</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-10-14</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2510.12276" target="_blank">http://arxiv.org/pdf/2510.12276</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on improving spatial awareness in Vision-Language-Action (VLA) models for robotic manipulation through implicit spatial representation alignment.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on previous VLA models that rely on explicit 3D sensor inputs or depth estimators, this paper proposes a novel approach of implicitly developing spatial comprehension without relying on explicit 3D data.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the challenge of enabling VLA models to develop accurate spatial awareness without depending on explicit 3D sensor information or depth estimators, which are often unreliable or unavailable.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors introduce Spatial Forcing (SF), which aligns intermediate visual embeddings of VLAs with geometric representations from pretrained 3D foundation models through cosine similarity scoring and representation alignment.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> SF achieved state-of-the-art results on LIBERO and RoboTwin benchmarks, accelerated training by up to 3.8x, improved data efficiency, and demonstrated superior performance in both simulated and real-world robotic tasks.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Spatial Forcing: Implicit Spatial Representation Alignment for
  Vision-language-action Model</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background gradient -->
  <defs>
    <linearGradient id="bgGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0f8ff;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#e6f3ff;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="blueGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#4a90e2;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#357abd;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="greenGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#5cb85c;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#449d44;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="orangeGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0ad4e;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#ec971f;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="purpleGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#9b59b6;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#8e44ad;stop-opacity:1" />
    </linearGradient>
  </defs>
  
  <!-- Background -->
  <rect width="1000" height="800" fill="url(#bgGrad)"/>
  
  <!-- Title -->
  <text x="500" y="40" font-family="Arial, sans-serif" font-size="24" font-weight="bold" text-anchor="middle" fill="#2c3e50">Spatial Forcing (SF) Methodology Flow</text>
  
  <!-- Phase 1: Problem Analysis -->
  <rect x="50" y="80" width="180" height="120" rx="10" fill="url(#blueGrad)" stroke="#2980b9" stroke-width="2"/>
  <text x="140" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Problem Analysis</text>
  <text x="140" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Depth Probing</text>
  <text x="140" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Experiment</text>
  <text x="140" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">VLA lacks spatial</text>
  <text x="140" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">understanding</text>
  <text x="140" y="185" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">in embeddings</text>
  
  <!-- Phase 2: 3D Foundation Model -->
  <rect x="280" y="80" width="180" height="120" rx="10" fill="url(#greenGrad)" stroke="#27ae60" stroke-width="2"/>
  <text x="370" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">3D Foundation Model</text>
  <text x="370" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">VGGT Processing</text>
  <text x="370" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Multi-view Images</text>
  <text x="370" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">‚Üí Spatial</text>
  <text x="370" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Representations</text>
  <text x="370" y="185" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">f3D(I)</text>
  
  <!-- Phase 3: VLA Processing -->
  <rect x="510" y="80" width="180" height="120" rx="10" fill="url(#orangeGrad)" stroke="#e67e22" stroke-width="2"/>
  <text x="600" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">VLA Processing</text>
  <text x="600" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Vision Tokens</text>
  <text x="600" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Language Tokens</text>
  <text x="600" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Intermediate</text>
  <text x="600" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Visual Embeddings</text>
  <text x="600" y="185" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">xVi</text>
  
  <!-- Phase 4: Alignment Process -->
  <rect x="740" y="80" width="180" height="120" rx="10" fill="url(#purpleGrad)" stroke="#8e44ad" stroke-width="2"/>
  <text x="830" y="105" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Alignment Process</text>
  <text x="830" y="125" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Batch Normalization</text>
  <text x="830" y="140" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">+ MLP</text>
  <text x="830" y="155" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Cosine Similarity</text>
  <text x="830" y="170" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Alignment</text>
  
  <!-- Central Alignment Formula -->
  <rect x="250" y="250" width="500" height="80" rx="15" fill="#ecf0f1" stroke="#34495e" stroke-width="3"/>
  <text x="500" y="275" font-family="Arial, sans-serif" font-size="16" font-weight="bold" text-anchor="middle" fill="#2c3e50">Spatial Forcing Alignment Loss</text>
  <text x="500" y="300" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#34495e">Lalign = -1/N Œ£ S[MLP¬∑Œì(xVi), f3Di(I) + E]</text>
  <text x="500" y="320" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#7f8c8d">where S[¬∑,¬∑] is cosine similarity</text>
  
  <!-- Training Objective -->
  <rect x="350" y="370" width="300" height="60" rx="10" fill="#e74c3c" stroke="#c0392b" stroke-width="2"/>
  <text x="500" y="395" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Combined Training Loss</text>
  <text x="500" y="415" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="white">LSF = Laction + Œ±¬∑Lalign</text>
  
  <!-- Results Section -->
  <g transform="translate(0, 480)">
    <!-- Performance Results -->
    <rect x="50" y="0" width="200" height="100" rx="10" fill="#16a085" stroke="#138d75" stroke-width="2"/>
    <text x="150" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Performance Gains</text>
    <text x="150" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">LIBERO: 98.5% SR</text>
    <text x="150" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">RoboTwin: SOTA</text>
    <text x="150" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Real-world: +47.5%</text>
    
    <!-- Training Efficiency -->
    <rect x="280" y="0" width="200" height="100" rx="10" fill="#d35400" stroke="#ba4a00" stroke-width="2"/>
    <text x="380" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Training Efficiency</text>
    <text x="380" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">3.8√ó faster</text>
    <text x="380" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">convergence</text>
    <text x="380" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">Same success rates</text>
    
    <!-- Data Efficiency -->
    <rect x="510" y="0" width="200" height="100" rx="10" fill="#8e44ad" stroke="#7d3c98" stroke-width="2"/>
    <text x="610" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Data Efficiency</text>
    <text x="610" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">5.9√ó more efficient</text>
    <text x="610" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">75.8% SR with</text>
    <text x="610" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">only 5% data</text>
    
    <!-- Inference -->
    <rect x="740" y="0" width="200" height="100" rx="10" fill="#2980b9" stroke="#21618c" stroke-width="2"/>
    <text x="840" y="25" font-family="Arial, sans-serif" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Inference</text>
    <text x="840" y="45" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">No additional</text>
    <text x="840" y="60" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">computational</text>
    <text x="840" y="75" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="white">overhead</text>
  </g>
  
  <!-- Key Innovation Box -->
  <rect x="50" y="620" width="900" height="80" rx="15" fill="#f8f9fa" stroke="#495057" stroke-width="2" stroke-dasharray="5,5"/>
  <text x="500" y="645" font-family="Arial, sans-serif" font-size="16" font-weight="bold" text-anchor="middle" fill="#495057">Key Innovation</text>
  <text x="500" y="665" font-family="Arial, sans-serif" font-size="13" text-anchor="middle" fill="#6c757d">Implicit spatial comprehension without explicit 3D inputs or depth estimators</text>
  <text x="500" y="685" font-family="Arial, sans-serif" font-size="13" text-anchor="middle" fill="#6c757d">Aligns intermediate VLA embeddings with pretrained 3D foundation model representations</text>
  
  <!-- Connection lines -->
  <line x1="230" y1="140" x2="280" y2="140" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="460" y1="140" x2="510" y2="140" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="690" y1="140" x2="740" y2="140" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Vertical connections -->
  <line x1="370" y1="200" x2="370" y2="250" stroke="#27ae60" stroke-width="3"/>
  <line x1="600" y1="200" x2="600" y2="250" stroke="#e67e22" stroke-width="3"/>
  <line x1="500" y1="330" x2="500" y2="370" stroke="#34495e" stroke-width="3"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#34495e"/>
    </marker>
  </defs>
  
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Dependence on unreliable depth sensors and incomplete datasets">
                        <div class="quiz-question">1. What is the main limitation of existing 3D VLA approaches that Spatial Forcing aims to overcome?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="High computational cost of processing 3D data">High computational cost of processing 3D data</div><div class="quiz-choice" data-value="Dependence on unreliable depth sensors and incomplete datasets">Dependence on unreliable depth sensors and incomplete datasets</div><div class="quiz-choice" data-value="Inability to handle multiple camera views">Inability to handle multiple camera views</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="By aligning visual embeddings with pretrained 3D foundation model representations">
                        <div class="quiz-question">2. How does Spatial Forcing achieve spatial awareness in VLA models?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By adding extra 3D sensors to the robot">By adding extra 3D sensors to the robot</div><div class="quiz-choice" data-value="By training a separate depth estimation network">By training a separate depth estimation network</div><div class="quiz-choice" data-value="By aligning visual embeddings with pretrained 3D foundation model representations">By aligning visual embeddings with pretrained 3D foundation model representations</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Reduced training time by 3.8x">
                        <div class="quiz-question">3. What significant performance improvement did Spatial Forcing demonstrate in training efficiency?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Reduced training time by 3.8x">Reduced training time by 3.8x</div><div class="quiz-choice" data-value="Improved accuracy by 3.8%">Improved accuracy by 3.8%</div><div class="quiz-choice" data-value="Reduced memory usage by 3.8x">Reduced memory usage by 3.8x</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/black-linen-2.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>Advancing End-to-End Pixel Space Generative Modeling via Self-supervised
  Pre-training</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-10-14</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2510.12586" target="_blank">http://arxiv.org/pdf/2510.12586</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> Pixel-space generative modeling for image synthesis using diffusion and consistency models.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on self-supervised learning approaches and prior diffusion models, proposes a novel two-stage training framework with self-supervised pre-training instead of relying on VAEs.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> Addressing the persistent performance and efficiency gap between pixel-space generative models and their latent-space counterparts.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> Uses a two-stage approach: pre-training encoders to capture semantics from clean images while aligning them along deterministic sampling trajectories, then fine-tuning with a randomly initialized decoder end-to-end.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Achieved FID scores of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 NFEs for diffusion models, and 8.82 FID for one-step consistency model generation on ImageNet-256, surpassing previous pixel-space methods.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Advancing End-to-End Pixel Space Generative Modeling via Self-supervised
  Pre-training</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="18" font-weight="bold" fill="#2c3e50">
    EPG: End-to-End Pixel Space Generative Modeling Framework
  </text>
  
  <!-- Stage 1: Pre-training -->
  <rect x="50" y="80" width="400" height="320" fill="#e8f4fd" stroke="#3498db" stroke-width="2" rx="10"/>
  <text x="250" y="105" text-anchor="middle" font-size="16" font-weight="bold" fill="#2980b9">
    Stage 1: Self-Supervised Pre-training
  </text>
  
  <!-- Input Data -->
  <rect x="70" y="130" width="120" height="50" fill="#fff3cd" stroke="#f39c12" stroke-width="2" rx="5"/>
  <text x="130" y="160" text-anchor="middle" font-size="12" fill="#e67e22">Clean Images</text>
  
  <!-- Data Augmentation -->
  <rect x="210" y="130" width="120" height="50" fill="#d1ecf1" stroke="#17a2b8" stroke-width="2" rx="5"/>
  <text x="270" y="155" text-anchor="middle" font-size="11" fill="#138496">Data Augmentation</text>
  <text x="270" y="170" text-anchor="middle" font-size="11" fill="#138496">(y‚ÇÅ, y‚ÇÇ)</text>
  
  <!-- Noise Addition -->
  <rect x="350" y="130" width="80" height="50" fill="#f8d7da" stroke="#dc3545" stroke-width="2" rx="5"/>
  <text x="390" y="155" text-anchor="middle" font-size="11" fill="#721c24">Noise</text>
  <text x="390" y="170" text-anchor="middle" font-size="11" fill="#721c24">(x‚Çú‚Çô, x‚Çú‚Çô‚Çã‚ÇÅ)</text>
  
  <!-- Three Encoders -->
  <rect x="70" y="200" width="100" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="120" y="225" text-anchor="middle" font-size="11" fill="#155724">Encoder EŒ∏</text>
  <text x="120" y="240" text-anchor="middle" font-size="10" fill="#155724">(Online)</text>
  
  <rect x="190" y="200" width="100" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="240" y="225" text-anchor="middle" font-size="11" fill="#155724">Encoder EŒ∏‚Çã</text>
  <text x="240" y="240" text-anchor="middle" font-size="10" fill="#155724">(Momentum)</text>
  
  <rect x="310" y="200" width="100" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="360" y="225" text-anchor="middle" font-size="11" fill="#155724">Encoder Esg(Œ∏)</text>
  <text x="360" y="240" text-anchor="middle" font-size="10" fill="#155724">(Stop Grad)</text>
  
  <!-- Projectors -->
  <rect x="70" y="280" width="80" height="40" fill="#e2e3e5" stroke="#6c757d" stroke-width="2" rx="5"/>
  <text x="110" y="305" text-anchor="middle" font-size="11" fill="#495057">Projector LŒ∏</text>
  
  <rect x="190" y="280" width="80" height="40" fill="#e2e3e5" stroke="#6c757d" stroke-width="2" rx="5"/>
  <text x="230" y="305" text-anchor="middle" font-size="11" fill="#495057">Projector LŒ∏‚Çã</text>
  
  <!-- Loss Functions -->
  <rect x="70" y="340" width="140" height="40" fill="#fff3cd" stroke="#ffc107" stroke-width="2" rx="5"/>
  <text x="140" y="365" text-anchor="middle" font-size="11" fill="#856404">Contrastive Loss</text>
  
  <rect x="230" y="340" width="180" height="40" fill="#fff3cd" stroke="#ffc107" stroke-width="2" rx="5"/>
  <text x="320" y="365" text-anchor="middle" font-size="11" fill="#856404">Representation Consistency Loss</text>
  
  <!-- Stage 2: Fine-tuning -->
  <rect x="550" y="80" width="400" height="320" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2" rx="10"/>
  <text x="750" y="105" text-anchor="middle" font-size="16" font-weight="bold" fill="#7b1fa2">
    Stage 2: End-to-End Fine-tuning
  </text>
  
  <!-- Pre-trained Encoder -->
  <rect x="570" y="130" width="120" height="60" fill="#d4edda" stroke="#28a745" stroke-width="2" rx="5"/>
  <text x="630" y="155" text-anchor="middle" font-size="12" fill="#155724">Pre-trained</text>
  <text x="630" y="170" text-anchor="middle" font-size="12" fill="#155724">Encoder EŒ∏</text>
  
  <!-- Random Decoder -->
  <rect x="710" y="130" width="120" height="60" fill="#ffeaa7" stroke="#fdcb6e" stroke-width="2" rx="5"/>
  <text x="770" y="155" text-anchor="middle" font-size="12" fill="#e17055">Random Init</text>
  <text x="770" y="170" text-anchor="middle" font-size="12" fill="#e17055">Decoder DŒ∏</text>
  
  <!-- Complete Model -->
  <rect x="620" y="220" width="160" height="50" fill="#dda0dd" stroke="#9370db" stroke-width="2" rx="5"/>
  <text x="700" y="250" text-anchor="middle" font-size="12" fill="#4b0082">Complete Model fŒ∏</text>
  
  <!-- Training Options -->
  <rect x="570" y="300" width="140" height="50" fill="#ffe4e1" stroke="#ff6b6b" stroke-width="2" rx="5"/>
  <text x="640" y="320" text-anchor="middle" font-size="11" fill="#c0392b">Diffusion Training</text>
  <text x="640" y="335" text-anchor="middle" font-size="10" fill="#c0392b">(Equation 1)</text>
  
  <rect x="730" y="300" width="140" height="50" fill="#e4f3ff" stroke="#74b9ff" stroke-width="2" rx="5"/>
  <text x="800" y="320" text-anchor="middle" font-size="11" fill="#0984e3">Consistency Training</text>
  <text x="800" y="335" text-anchor="middle" font-size="10" fill="#0984e3">(Equation 5 + 9)</text>
  
  <!-- Key Components -->
  <rect x="50" y="450" width="900" height="120" fill="#f1f2f6" stroke="#57606f" stroke-width="2" rx="10"/>
  <text x="500" y="475" text-anchor="middle" font-size="16" font-weight="bold" fill="#2f3542">
    Key Technical Components
  </text>
  
  <!-- Component 1 -->
  <rect x="70" y="490" width="180" height="60" fill="#e8f8f5" stroke="#1dd1a1" stroke-width="2" rx="5"/>
  <text x="160" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#00a085">Representation Learning</text>
  <text x="160" y="525" text-anchor="middle" font-size="10" fill="#00a085">Visual semantics from</text>
  <text x="160" y="540" text-anchor="middle" font-size="10" fill="#00a085">clean & noisy images</text>
  
  <!-- Component 2 -->
  <rect x="270" y="490" width="180" height="60" fill="#fff4e6" stroke="#ff9f43" stroke-width="2" rx="5"/>
  <text x="360" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#e55039">Temperature Scheduling</text>
  <text x="360" y="525" text-anchor="middle" font-size="10" fill="#e55039">œÑ(t) = œÑ‚ÇÅ*(1-t) + œÑ‚ÇÇ*t</text>
  <text x="360" y="540" text-anchor="middle" font-size="10" fill="#e55039">Stable training</text>
  
  <!-- Component 3 -->
  <rect x="470" y="490" width="180" height="60" fill="#e8f4f8" stroke="#3c6382" stroke-width="2" rx="5"/>
  <text x="560" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#40739e">ODE Trajectory Alignment</text>
  <text x="560" y="525" text-anchor="middle" font-size="10" fill="#40739e">Points on same trajectory</text>
  <text x="560" y="540" text-anchor="middle" font-size="10" fill="#40739e">maintain consistency</text>
  
  <!-- Component 4 -->
  <rect x="670" y="490" width="180" height="60" fill="#f8e8ff" stroke="#8c7ae6" stroke-width="2" rx="5"/>
  <text x="760" y="510" text-anchor="middle" font-size="11" font-weight="bold" fill="#6c5ce7">Auxiliary Contrastive Loss</text>
  <text x="760" y="525" text-anchor="middle" font-size="10" fill="#6c5ce7">For consistency models</text>
  <text x="760" y="540" text-anchor="middle" font-size="10" fill="#6c5ce7">Equation 9</text>
  
  <!-- Results -->
  <rect x="50" y="600" width="900" height="120" fill="#f8f9fa" stroke="#495057" stroke-width="2" rx="10"/>
  <text x="500" y="625" text-anchor="middle" font-size="16" font-weight="bold" fill="#343a40">
    Achievements
  </text>
  
  <!-- Result 1 -->
  <rect x="70" y="645" width="200" height="60" fill="#d1f2eb" stroke="#27ae60" stroke-width="2" rx="5"/>
  <text x="170" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#27ae60">Diffusion Model</text>
  <text x="170" y="680" text-anchor="middle" font-size="10" fill="#27ae60">FID 2.04 (ImageNet-256)</text>
  <text x="170" y="695" text-anchor="middle" font-size="10" fill="#27ae60">75 NFE, SOTA pixel-space</text>
  
  <!-- Result 2 -->
  <rect x="290" y="645" width="200" height="60" fill="#ebf3fd" stroke="#3498db" stroke-width="2" rx="5"/>
  <text x="390" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#3498db">Consistency Model</text>
  <text x="390" y="680" text-anchor="middle" font-size="10" fill="#3498db">FID 8.82 (ImageNet-256)</text>
  <text x="390" y="695" text-anchor="middle" font-size="10" fill="#3498db">Single-step generation</text>
  
  <!-- Result 3 -->
  <rect x="510" y="645" width="200" height="60" fill="#fef9e7" stroke="#f39c12" stroke-width="2" rx="5"/>
  <text x="610" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#f39c12">High Resolution</text>
  <text x="610" y="680" text-anchor="middle" font-size="10" fill="#f39c12">FID 2.35 (ImageNet-512)</text>
  <text x="610" y="695" text-anchor="middle" font-size="10" fill="#f39c12">Efficient scaling</text>
  
  <!-- Result 4 -->
  <rect x="730" y="645" width="200" height="60" fill="#f4ecf7" stroke="#9b59b6" stroke-width="2" rx="5"/>
  <text x="830" y="665" text-anchor="middle" font-size="11" font-weight="bold" fill="#9b59b6">Training Efficiency</text>
  <text x="830" y="680" text-anchor="middle" font-size="10" fill="#9b59b6">Competitive with VAE-based</text>
  <text x="830" y="695" text-anchor="middle" font-size="10" fill="#9b59b6">No external models</text>
  
  <!-- Flow indicators -->
  <polygon points="450,240 470,250 450,260" fill="#34495e"/>
  <text x="480" y="255" font-size="12" fill="#34495e">Transfer</text>
  
  <!-- Vertical flow in pre-training -->
  <polygon points="245,190 250,210 255,190" fill="#3498db"/>
  <polygon points="245,270 250,290 255,270" fill="#3498db"/>
  <polygon points="245,330 250,350 255,330" fill="#3498db"/>
  
  <!-- Vertical flow in fine-tuning -->
  <polygon points="745,200 750,220 755,200" fill="#9c27b0"/>
  <polygon points="745,280 750,300 755,280" fill="#9c27b0"/>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Two-stage training with self-supervised pre-training of encoders">
                        <div class="quiz-question">1. What is the main innovation in the paper's training framework compared to traditional approaches?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Using VAEs for latent space compression">Using VAEs for latent space compression</div><div class="quiz-choice" data-value="Two-stage training with self-supervised pre-training of encoders">Two-stage training with self-supervised pre-training of encoders</div><div class="quiz-choice" data-value="Single-stage end-to-end training with larger models">Single-stage end-to-end training with larger models</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Successfully training without VAEs or pre-trained diffusion models">
                        <div class="quiz-question">2. What is the most impressive achievement of the paper's consistency model variant?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Achieving 2.04 FID score on ImageNet-256">Achieving 2.04 FID score on ImageNet-256</div><div class="quiz-choice" data-value="Successfully training without VAEs or pre-trained diffusion models">Successfully training without VAEs or pre-trained diffusion models</div><div class="quiz-choice" data-value="Using only 32 sampling steps for generation">Using only 32 sampling steps for generation</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="By decomposing training into semantic learning and pixel generation stages">
                        <div class="quiz-question">3. How does the paper's approach improve the training efficiency?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By using more powerful GPUs and larger batch sizes">By using more powerful GPUs and larger batch sizes</div><div class="quiz-choice" data-value="By compressing images into latent space representations">By compressing images into latent space representations</div><div class="quiz-choice" data-value="By decomposing training into semantic learning and pixel generation stages">By decomposing training into semantic learning and pixel generation stages</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/argyle.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>Scaling Language-Centric Omnimodal Representation Learning</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-10-13</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2510.11693" target="_blank">http://arxiv.org/pdf/2510.11693</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> Language-centric omnimodal representation learning in multimodal large language models (MLLMs), focusing on cross-modal alignment and embedding capabilities.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on previous CLIP-style and MLLM-based embedding approaches, proposing that MLLMs achieve implicit cross-modal alignment during generative pretraining, allowing for lightweight contrastive learning refinement.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> Understanding why MLLM-based embedding approaches outperform traditional CLIP-based models and developing more efficient methods for cross-modal representation learning.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> Developed LCO-EMB framework using language-centric paired data for contrastive learning refinement, analyzed through anisotropy and kernel similarity studies, and validated on various benchmarks.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Achieved state-of-the-art performance across diverse modalities and benchmarks, discovered a Generation-Representation Scaling Law showing representation capabilities scale with generative abilities, and validated findings on a challenging visual-document retrieval task.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Scaling Language-Centric Omnimodal Representation Learning</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="18" font-weight="bold" fill="#2c3e50">
    LCO-EMB: Language-Centric Omnimodal Representation Learning
  </text>
  
  <!-- Stage 1: Analysis -->
  <rect x="50" y="60" width="200" height="100" rx="10" fill="#e8f4f8" stroke="#3498db" stroke-width="2"/>
  <text x="150" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Analysis Phase</text>
  <text x="150" y="105" text-anchor="middle" font-size="10" fill="#34495e">Anisotropy Analysis</text>
  <text x="150" y="120" text-anchor="middle" font-size="10" fill="#34495e">Kernel Similarity</text>
  <text x="150" y="135" text-anchor="middle" font-size="10" fill="#34495e">Cross-modal Alignment</text>
  
  <!-- Stage 2: MLLM Backbone -->
  <rect x="300" y="60" width="180" height="100" rx="10" fill="#fff2e6" stroke="#e67e22" stroke-width="2"/>
  <text x="390" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">MLLM Backbone</text>
  <text x="390" y="105" text-anchor="middle" font-size="10" fill="#34495e">LLaVA-Next / Qwen2.5-VL</text>
  <text x="390" y="120" text-anchor="middle" font-size="10" fill="#34495e">Qwen2.5-Omni</text>
  <text x="390" y="135" text-anchor="middle" font-size="10" fill="#34495e">Pretrained Alignment</text>
  
  <!-- Stage 3: Text-only CL -->
  <rect x="530" y="60" width="180" height="100" rx="10" fill="#e8f5e8" stroke="#27ae60" stroke-width="2"/>
  <text x="620" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Text-only CL</text>
  <text x="620" y="105" text-anchor="middle" font-size="10" fill="#34495e">LoRA Fine-tuning</text>
  <text x="620" y="120" text-anchor="middle" font-size="10" fill="#34495e">NLI / Scale-1M</text>
  <text x="620" y="135" text-anchor="middle" font-size="10" fill="#34495e">Minimal Perturbation</text>
  
  <!-- Stage 4: Multimodal Refinement -->
  <rect x="750" y="60" width="180" height="100" rx="10" fill="#f4e8f8" stroke="#9b59b6" stroke-width="2"/>
  <text x="840" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Multimodal Refinement</text>
  <text x="840" y="105" text-anchor="middle" font-size="10" fill="#34495e">94k Synthetic Pairs</text>
  <text x="840" y="120" text-anchor="middle" font-size="10" fill="#34495e">Task Space Calibration</text>
  <text x="840" y="135" text-anchor="middle" font-size="10" fill="#34495e">Optional Enhancement</text>
  
  <!-- Data Sources -->
  <rect x="50" y="200" width="280" height="80" rx="8" fill="#fdf2e9" stroke="#d68910" stroke-width="1"/>
  <text x="190" y="220" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Training Data Sources</text>
  <text x="190" y="240" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ MNLI + SNLI (276k) ‚Ä¢ Scale-1M (1M pairs)</text>
  <text x="190" y="255" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Visual Documents ‚Ä¢ Retrieval & Compositionality</text>
  <text x="190" y="270" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Multilingual Data ‚Ä¢ Synthetic Samples</text>
  
  <!-- Generation-Representation Scaling Law -->
  <rect x="370" y="200" width="260" height="80" rx="8" fill="#eaf2f8" stroke="#2980b9" stroke-width="1"/>
  <text x="500" y="220" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Generation-Representation Scaling Law</text>
  <text x="500" y="240" text-anchor="middle" font-size="9" fill="#34495e">Generative Quality ‚àù Representation Performance</text>
  <text x="500" y="255" text-anchor="middle" font-size="9" fill="#34495e">PAC-Bayesian Bound: L_pop ‚â§ log(N) - I_P(X;Y) + Œµ_P</text>
  <text x="500" y="270" text-anchor="middle" font-size="9" fill="#34495e">Theoretical Justification</text>
  
  <!-- Evaluation Benchmarks -->
  <rect x="670" y="200" width="280" height="80" rx="8" fill="#e8f8f5" stroke="#16a085" stroke-width="1"/>
  <text x="810" y="220" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Evaluation Benchmarks</text>
  <text x="810" y="240" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ MIEB-Lite (51 tasks) ‚Ä¢ Audio-Text: AudioCaps, Clotho</text>
  <text x="810" y="255" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Video-Text: MSR-VTT, ActivityNet</text>
  <text x="810" y="270" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ SeaDoc (Cross-lingual Document Retrieval)</text>
  
  <!-- Key Components -->
  <rect x="100" y="320" width="160" height="60" rx="8" fill="#fff5f5" stroke="#e74c3c" stroke-width="1"/>
  <text x="180" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Modality Encoders</text>
  <text x="180" y="355" text-anchor="middle" font-size="9" fill="#34495e">Vision / Audio</text>
  <text x="180" y="370" text-anchor="middle" font-size="9" fill="#34495e">(Frozen)</text>
  
  <rect x="300" y="320" width="160" height="60" rx="8" fill="#f0f8ff" stroke="#3498db" stroke-width="1"/>
  <text x="380" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Projector</text>
  <text x="380" y="355" text-anchor="middle" font-size="9" fill="#34495e">Alignment Layer</text>
  <text x="380" y="370" text-anchor="middle" font-size="9" fill="#34495e">(Frozen)</text>
  
  <rect x="500" y="320" width="160" height="60" rx="8" fill="#f0fff0" stroke="#27ae60" stroke-width="1"/>
  <text x="580" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Language Decoder</text>
  <text x="580" y="355" text-anchor="middle" font-size="9" fill="#34495e">LLM Backbone</text>
  <text x="580" y="370" text-anchor="middle" font-size="9" fill="#34495e">(LoRA Tuned)</text>
  
  <rect x="700" y="320" width="160" height="60" rx="8" fill="#faf0e6" stroke="#d68910" stroke-width="1"/>
  <text x="780" y="340" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Embeddings</text>
  <text x="780" y="355" text-anchor="middle" font-size="9" fill="#34495e">Unified Space</text>
  <text x="780" y="370" text-anchor="middle" font-size="9" fill="#34495e">Similarity Matching</text>
  
  <!-- Results Section -->
  <rect x="50" y="420" width="900" height="100" rx="10" fill="#f8f9fa" stroke="#6c757d" stroke-width="2"/>
  <text x="500" y="445" text-anchor="middle" font-size="14" font-weight="bold" fill="#2c3e50">Key Results & Achievements</text>
  
  <text x="100" y="470" font-size="10" fill="#34495e">‚Ä¢ State-of-the-art on MIEB-Lite: 68.8% (LCO-EMB-Omni 7B)</text>
  <text x="100" y="485" font-size="10" fill="#34495e">‚Ä¢ 21√ó less training data than competing methods</text>
  <text x="100" y="500" font-size="10" fill="#34495e">‚Ä¢ Text-only variants outperform advanced baselines</text>
  
  <text x="500" y="470" font-size="10" fill="#34495e">‚Ä¢ Discovers latent cross-modal alignment in MLLMs</text>
  <text x="500" y="485" font-size="10" fill="#34495e">‚Ä¢ Establishes Generation-Representation Scaling Law</text>
  <text x="500" y="500" font-size="10" fill="#34495e">‚Ä¢ Generalizes across vision, audio, and video modalities</text>
  
  <!-- Theoretical Foundation -->
  <rect x="50" y="550" width="900" height="80" rx="10" fill="#f4f1fb" stroke="#8e44ad" stroke-width="2"/>
  <text x="500" y="575" text-anchor="middle" font-size="14" font-weight="bold" fill="#2c3e50">Theoretical Foundation</text>
  
  <text x="150" y="600" font-size="10" fill="#34495e">Generative Bottleneck:</text>
  <text x="150" y="615" font-size="9" fill="#34495e">log(N) - I_P(X;Y)</text>
  
  <text x="350" y="600" font-size="10" fill="#34495e">Optimization Inefficiency:</text>
  <text x="350" y="615" font-size="9" fill="#34495e">Œµ_P (minimized by strong prior)</text>
  
  <text x="550" y="600" font-size="10" fill="#34495e">Fine-tuning Cost:</text>
  <text x="550" y="615" font-size="9" fill="#34495e">‚àö(KL(Q||P) + log(1/Œ¥))/2n</text>
  
  <text x="750" y="600" font-size="10" fill="#34495e">LoRA Justification:</text>
  <text x="750" y="615" font-size="9" fill="#34495e">Keeps KL(Q||P) small</text>
  
  <!-- Innovation Highlights -->
  <rect x="50" y="660" width="280" height="100" rx="8" fill="#e8f5e8" stroke="#27ae60" stroke-width="1"/>
  <text x="190" y="680" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Key Innovations</text>
  <text x="190" y="700" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Language-centric training paradigm</text>
  <text x="190" y="715" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Minimal multimodal data requirement</text>
  <text x="190" y="730" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Preservation of generative capabilities</text>
  <text x="190" y="745" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Cross-modal generalization</text>
  
  <!-- Validation -->
  <rect x="360" y="660" width="280" height="100" rx="8" fill="#fdf2e9" stroke="#d68910" stroke-width="1"/>
  <text x="500" y="680" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Empirical Validation</text>
  <text x="500" y="700" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Multiple MLLM backbones tested</text>
  <text x="500" y="715" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Comprehensive benchmark evaluation</text>
  <text x="500" y="730" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ SeaDoc challenge task validation</text>
  <text x="500" y="745" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Ablation studies on LoRA settings</text>
  
  <!-- Future Directions -->
  <rect x="670" y="660" width="280" height="100" rx="8" fill="#eaf2f8" stroke="#2980b9" stroke-width="1"/>
  <text x="810" y="680" text-anchor="middle" font-size="11" font-weight="bold" fill="#2c3e50">Impact & Applications</text>
  <text x="810" y="700" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Efficient multimodal representation</text>
  <text x="810" y="715" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Low-resource language support</text>
  <text x="810" y="730" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Document understanding advances</text>
  <text x="810" y="745" text-anchor="middle" font-size="9" fill="#34495e">‚Ä¢ Scalable training paradigm</text>
  
  <!-- Flow connections (simplified lines) -->
  <line x1="250" y1="110" x2="300" y2="110" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="110" x2="530" y2="110" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="710" y1="110" x2="750" y2="110" stroke="#34495e" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="260" y1="320" x2="300" y2="320" stroke="#7f8c8d" stroke-width="1"/>
  <line x1="460" y1="320" x2="500" y2="320" stroke="#7f8c8d" stroke-width="1"/>
  <line x1="660" y1="320" x2="700" y2="320" stroke="#7f8c8d" stroke-width="1"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#34495e"/>
    </marker>
  </defs>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="MLLMs achieve implicit cross-modal alignment during generative pretraining">
                        <div class="quiz-question">1. What is the key insight about MLLMs that the paper discovers?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="MLLMs require extensive contrastive learning to achieve cross-modal alignment">MLLMs require extensive contrastive learning to achieve cross-modal alignment</div><div class="quiz-choice" data-value="MLLMs achieve implicit cross-modal alignment during generative pretraining">MLLMs achieve implicit cross-modal alignment during generative pretraining</div><div class="quiz-choice" data-value="MLLMs cannot perform as well as CLIP-based models in embedding tasks">MLLMs cannot perform as well as CLIP-based models in embedding tasks</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Representational capabilities scale positively with generative abilities">
                        <div class="quiz-question">2. What novel scaling law does the paper identify?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Model size directly correlates with embedding quality">Model size directly correlates with embedding quality</div><div class="quiz-choice" data-value="Training data size determines representation capabilities">Training data size determines representation capabilities</div><div class="quiz-choice" data-value="Representational capabilities scale positively with generative abilities">Representational capabilities scale positively with generative abilities</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="To preserve the latent cross-modal alignment while enhancing representation capability">
                        <div class="quiz-question">3. Why does the paper's LCO-EMB framework use LoRA for fine-tuning?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="To reduce computational costs during training">To reduce computational costs during training</div><div class="quiz-choice" data-value="To preserve the latent cross-modal alignment while enhancing representation capability">To preserve the latent cross-modal alignment while enhancing representation capability</div><div class="quiz-choice" data-value="To enable training on larger batch sizes">To enable training on larger batch sizes</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
    
    
    <!-- Personal Takeaways Section -->
    <div id="takeaways-container"></div>

        <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // ÂàõÂª∫ÈÅÆÁΩ©Â±Ç
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // Ëé∑ÂèñÊâÄÊúâÈóÆÈ¢òÊ†áÁ≠æ
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // ËÆæÁΩÆÁÇπÂáª‰∫ã‰ª∂Â§ÑÁêÜ
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // ÁÇπÂáªÊ†áÁ≠æÂàáÊç¢ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫Áä∂ÊÄÅ
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // ÈòªÊ≠¢‰∫ã‰ª∂ÂÜíÊ≥°
                    
                    // Â¶ÇÊûúÂΩìÂâçÈóÆÈ¢òÂç°Â∑≤ÁªèÊòæÁ§∫ÔºåÂàôÈöêËóèÂÆÉ
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // ÂÖàÈöêËóèÊâÄÊúâÂÖ∂‰ªñÈóÆÈ¢òÂç°
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // Â∞ÜÂºπÁ™óÂÜÖÂÆπÂ§çÂà∂Âà∞È°µÈù¢ÊúÄÂ§ñÂ±ÇÁöÑÂºπÁ™ó‰∏≠
                        document.body.appendChild(popup);
                        
                        // ÊòæÁ§∫ÂΩìÂâçÈóÆÈ¢òÂç°ÂíåËÉåÊôØÈÅÆÁΩ©
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // Á°Æ‰øùÁÇπÂáªÈóÆÈ¢òÂç°ÂÜÖÈÉ®Êó∂‰∏ç‰ºöÂÖ≥Èó≠ÈóÆÈ¢òÂç°
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // ÁÇπÂáªÈÅÆÁΩ©Â±ÇÊàñÈ°µÈù¢‰ªª‰ΩïÂÖ∂‰ªñ‰ΩçÁΩÆÊó∂ÈöêËóèÊâÄÊúâÈóÆÈ¢òÂç°
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // ‰∏∫ÊØè‰∏™ÈÄâÈ°πÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // ÈáçÁΩÆÊâÄÊúâÈÄâÈ°π
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // Ê†áËÆ∞ÂΩìÂâçÈÄâÈ°π‰∏∫Â∑≤ÈÄâ
                    this.classList.add('selected');
                    
                    // Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '‚úîÔ∏è CorrectÔºÅ';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '‚ùå WrongÔºÅ';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // Âç°ÁâáËΩÆÊí≠ÂäüËÉΩ - Êñ∞Â¢û
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // Êõ¥Êñ∞ËÆ°Êï∞Âô®ÊòæÁ§∫
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // ÊòæÁ§∫ÊåáÂÆöÁ¥¢ÂºïÁöÑÂç°Áâá
                function showCard(index) {
                    // Â§ÑÁêÜÂæ™ÁéØ
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // Êõ¥Êñ∞ÂΩìÂâçÁ¥¢Âºï
                    currentIndex = index;
                    
                    // Êõ¥Êñ∞Âç°ÁâáÊòæÁ§∫
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // Êõ¥Êñ∞ËÆ°Êï∞Âô®
                    updateCounter();
                }
                
                // ‰∏ã‰∏ÄÂº†Âç°Áâá
                function nextCard(e) {
                    e.stopPropagation(); // Èò≤Ê≠¢‰∫ã‰ª∂ÂÜíÊ≥°ÂØºËá¥ÈóÆÈ¢òÂç°ÂÖ≥Èó≠
                    showCard(currentIndex + 1);
                }
                
                // ‰∏∫Âç°ÁâáÂÆπÂô®Ê∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
                cardDeck.addEventListener('click', function(e) {
                    // Ê£ÄÊü•ÁÇπÂáªÊòØÂê¶ÂèëÁîüÂú®ÊµÅÁ®ãÂõæÂç°ÁâáÂÜÖÈÉ®ÁöÑÊªöÂä®Âå∫Âüü
                    // Â¶ÇÊûúÊòØÂú®ÊªöÂä®Êù°‰∏äÁÇπÂáªÔºå‰∏çÂàáÊç¢Âç°Áâá
                    const targetCard = e.target.closest('.paper-card');
                    if (targetCard && targetCard.classList.contains('flowchart-card')) {
                        // ËÆ°ÁÆóÁÇπÂáª‰ΩçÁΩÆÊòØÂê¶Âú®ÊªöÂä®Êù°Âå∫Âüü
                        const rect = targetCard.getBoundingClientRect();
                        const isScrollbarClick = 
                            (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                            (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);
                        
                        if (!isScrollbarClick) {
                            nextCard(e);
                        }
                    } else {
                        nextCard(e);
                    }
                });
                
                // ÈîÆÁõòÂØºËà™
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Load and render markdown takeaways
            const dateMatch = document.querySelector('h1').textContent.match(/(\d{4}-\d{2}-\d{2})/);
            if (dateMatch) {
                const date = dateMatch[1];
                const markdownPath = `../notes/${date}.md`;

                // Fetch the markdown file
                const xhr = new XMLHttpRequest();
                xhr.open('GET', markdownPath, true);
                xhr.onload = function() {
                    if (xhr.status === 200) {
                        const markdownContent = xhr.responseText;
                        if (!markdownContent.trim()) {
                            console.log('Markdown file is empty');
                            return;
                        }

                        // Convert markdown to HTML
                        let htmlContent = marked.parse(markdownContent);

                        // Fix image paths
                        const fixedContent = htmlContent.replace(
                            /src="(?!http:\/\/|https:\/\/|\/|\.\.\/)(.*?)"/g,
                            `src="../images/${date}/$1"`
                        );

                        // Wrap in styled divs
                        const wrappedHtml = `
                            <div class="takeaways-section">
                                <h2>üìù My Takeaways</h2>
                                <div class="takeaways-content">
                                    ${fixedContent}
                                </div>
                            </div>
                        `;

                        document.getElementById('takeaways-container').innerHTML = wrappedHtml;
                        console.log('Takeaways section rendered');
                    } else {
                        console.log('XHR failed - Status:', xhr.status);
                    }
                };
                xhr.onerror = function() {
                    console.log('No takeaway file found for this date');
                };
                xhr.send();
            }
        });
    </script>

</body>
</html>
