<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2026-01-28 Papers</title>
    <style>
        * {
            box-sizing: border-box; /* Á°Æ‰øùÊâÄÊúâÂÖÉÁ¥†‰ΩøÁî®border-boxÊ®°Âûã */
        }

        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('../../bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* Âç°ÁâáÂÆπÂô®Ê†∑Âºè - Êñ∞Â¢û */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            min-height: 600px; /* Êîπ‰∏∫ÊúÄÂ∞èÈ´òÂ∫¶ËÄå‰∏çÊòØÂõ∫ÂÆöÈ´òÂ∫¶ */
            max-height: 90vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶Èò≤Ê≠¢Ê∫¢Âá∫ */
            height: auto; /* Ëá™ÈÄÇÂ∫îÂÜÖÂÆπÈ´òÂ∫¶ */
            /* cursor removed - only cards should show pointer */
        }
        
        /* Âç°ÁâáÈÄöÁî®Ê†∑Âºè */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
            cursor: pointer; /* Show pointer on cards to indicate they're clickable */
        }
        
        /* ËΩÆÊí≠Âç°ÁâáÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%; /* ‰ΩøÁî®ÂÆπÂô®ÁöÑ100%È´òÂ∫¶ */
            transition: transform 0.5s ease, opacity 0.5s ease;
            overflow-y: auto; /* ÂÖÅËÆ∏ÂûÇÁõ¥ÊªöÂä® */
            overflow-x: hidden; /* Èò≤Ê≠¢Ê®™ÂêëÊªöÂä® */
        }
        
        /* ÈùûÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* ÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* Á¨¨‰∏ÄÂº†Âç°ÁâáÔºàÊñáÊú¨ÂÜÖÂÆπÔºâ‰∏çÈúÄË¶ÅÊªöÂä® */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* Á¨¨‰∫åÂº†Âç°ÁâáÔºàÊµÅÁ®ãÂõæÔºâÊîØÊåÅÊªöÂä® */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow-y: auto !important; /* ÂÖÅËÆ∏ÂûÇÁõ¥ÊªöÂä® */
            overflow-x: hidden !important; /* Èò≤Ê≠¢Ê®™ÂêëÊªöÂä® */
            padding: 20px; /* Ê∑ªÂä†ÂÜÖËæπË∑ù */
            padding-bottom: 50px; /* Ê∑ªÂä†Â∫ïÈÉ®Â°´ÂÖÖÁ°Æ‰øùËÉΩÁúãÂà∞Â∫ïÈÉ® */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-width: 100%; /* Á°Æ‰øù‰∏çË∂ÖÂá∫ÂÆπÂô®ÂÆΩÂ∫¶ */
            display: block; /* Èò≤Ê≠¢Â∫ïÈÉ®Á©∫ÁôΩ */
            margin: 0 auto; /* Â±Ö‰∏≠ÊòæÁ§∫ */
        }
        
        /* ‰º†ÁªüÂç°ÁâáÊ†∑Âºè */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
            word-wrap: break-word;
            overflow-wrap: break-word;
            hyphens: auto;
        }

        .paper-card p {
            margin: 5px 0;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
            word-wrap: break-word;
            overflow-wrap: break-word;
            overflow: hidden; /* Èò≤Ê≠¢ÂÜÖÂÆπÊ∫¢Âá∫ */
        }

        .category-chunk * {
            word-wrap: break-word;
            overflow-wrap: break-word;
            max-width: 100%;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* Âç°ÁâáËÆ°Êï∞Âô® - Êñ∞Â¢û */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* Êîπ‰∏∫Âõ∫ÂÆöÂÆö‰ΩçÔºå‰∏çÈöèÊªöÂä®ËÄåÁßªÂä® */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* Â±Ö‰∏≠ÊòæÁ§∫ */
            width: 90%;
            max-width: 500px; /* Â¢ûÂä†ÊúÄÂ§ßÂÆΩÂ∫¶ÔºåÈÄÇÂ∫îÈïøÂÜÖÂÆπ */
            max-height: 80vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶ */
            overflow-y: auto; /* ÂÜÖÂÆπËøáÂ§öÊó∂ÂèØÊªöÂä® */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* Á°Æ‰øùÊòæÁ§∫Âú®ÊúÄ‰∏äÂ±Ç */
        }
        
        /* Ê∑ªÂä†ÈÅÆÁΩ©Â±ÇÔºåÈò≤Ê≠¢ÈóÆÈ¢òÂç°Ë¢´ÂÖ∂‰ªñÂÜÖÂÆπÈÅÆÊå° */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* ‰ΩøÁî®JavaScriptÊéßÂà∂ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫ÂíåÈöêËóèÔºå‰∏çÂÜç‰ΩøÁî®hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            hyphens: auto; /* Âú®ÂøÖË¶ÅÊó∂‰ΩøÁî®ËøûÂ≠óÁ¨¶ */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* ÈïøÊñáÊú¨Â∑¶ÂØπÈΩê */
            display: block; /* Á°Æ‰øùÊòØÂùóÁ∫ßÂÖÉÁ¥† */
            white-space: normal; /* ÂÖÅËÆ∏Ëá™Âä®Êç¢Ë°å */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* ÈïøÊñáÊú¨ÈÄâÈ°πÁöÑÁâπÊÆäÊ†∑Âºè */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* Á°Æ‰øùÂºπÁ™ó‰∏≠ÁöÑÊåâÈíÆÊñáÊú¨‰∏ç‰ºöÊ∫¢Âá∫ */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* ÈÄÇÂ∫îË∂ÖÈïøÈÄâÈ°πÊñáÊú¨ */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        /* ‰∏≠Á≠âÂ±èÂπïËÆæÂ§áÔºàÂ¶ÇÂπ≥ÊùøÔºâ */
        @media (max-width: 1024px) {
            .card-deck {
                min-height: 500px;
            }

            .card-deck .paper-card {
                max-height: 88vh;
            }
        }

        /* ÁßªÂä®ËÆæÂ§áÂíåÂ∞èÂ±èÂπï */
        @media (max-width: 768px) {
            body {
                padding: 10px; /* ÂáèÂ∞ëÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂÜÖËæπË∑ù */
            }

            .paper-container {
                flex-direction: column;
            }

            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                min-height: 400px; /* ÁßªÂä®ËÆæÂ§á‰∏ä‰ΩøÁî®Êõ¥Â∞èÁöÑÊúÄÂ∞èÈ´òÂ∫¶ */
                height: auto; /* Ëá™ÈÄÇÂ∫îÈ´òÂ∫¶ */
            }

            .card-deck .paper-card {
                max-height: 85vh; /* ÁßªÂä®ËÆæÂ§á‰∏äÈôêÂà∂Êõ¥Â§ö */
                font-size: 0.95em; /* Á®çÂæÆÂáèÂ∞èÂ≠ó‰Ωì */
            }

            .paper-card h2 {
                font-size: 1.1em; /* ÁßªÂä®ËÆæÂ§á‰∏äË∞ÉÊï¥Ê†áÈ¢òÂ§ßÂ∞è */
            }

            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }

            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }

            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
                width: 45px; /* ÁßªÂä®ËÆæÂ§á‰∏äÁ®çÂ∞èÁöÑÊåâÈíÆ */
                height: 45px;
                font-size: 14px;
            }
        }

        /* ÊûÅÂ∞èÂ±èÂπïÔºàÂ¶ÇÂ∞èÊâãÊú∫Ôºâ */
        @media (max-width: 480px) {
            body {
                padding: 5px;
            }

            .card-deck {
                min-height: 300px;
            }

            .card-deck .paper-card {
                max-height: 80vh;
                font-size: 0.9em;
                padding: 10px;
            }

            .paper-card h2 {
                font-size: 1em;
            }

            .category-chunk {
                padding: 8px;
                font-size: 0.9em;
            }

            .quiz-tab {
                width: 40px;
                height: 40px;
                font-size: 12px;
            }
        }

        /* Personal Takeaways Section Styles */
        .takeaways-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .takeaways-section h2 {
            color: #ffffff;
            font-size: 2em;
            margin-bottom: 20px;
            text-align: center;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        .takeaways-content {
            background-color: rgba(255, 255, 255, 0.95);
            border-radius: 8px;
            padding: 25px;
            line-height: 1.8;
            color: #333;
        }

        .takeaways-content h3 {
            color: #667eea;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .takeaways-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .takeaways-content p {
            margin: 15px 0;
        }

        .takeaways-content ul, .takeaways-content ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        .takeaways-content li {
            margin: 8px 0;
        }

        .takeaways-content blockquote {
            border-left: 4px solid #667eea;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
            background-color: #f8f9fa;
            padding: 15px 20px;
            border-radius: 4px;
        }

        .takeaways-content code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d73a49;
        }

        .takeaways-content pre {
            background-color: #f6f8fa;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            border: 1px solid #e1e4e8;
        }

        .takeaways-content pre code {
            background-color: transparent;
            padding: 0;
            color: #333;
        }
    </style>
</head>
<body>
    <h1>2026-01-28 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/type.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>daVinci-Dev: Agent-native Mid-training for Software Engineering</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2026-01-26</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2601.18418" target="_blank">http://arxiv.org/pdf/2601.18418</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on agentic mid-training for software engineering, specifically developing large language models that can autonomously navigate, edit, and test complex code repositories.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing post-training approaches like SFT and RL for code agents, but proposes a novel "agent-native mid-training" paradigm that uses contextually-native trajectories (preserving complete information flow) and environmentally-native trajectories (from actual tool invocations) to instill foundational agentic behaviors earlier in training.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper addresses the distribution mismatch between static training data (showing only final code outcomes) and the dynamic, interactive nature of real software development where agents must iteratively navigate, edit, and test code based on feedback.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors synthesize two types of agent-native data from GitHub PRs: contextually-native trajectories (68.6B tokens) that reconstruct complete workflows, and environmentally-native trajectories (3.1B tokens) from actual Docker environment interactions, then perform mid-training on Qwen2.5 base models followed by supervised fine-tuning.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> On SWE-Bench Verified, their 32B and 72B models achieve 56.1% and 58.5% resolution rates respectively, surpassing the previous KIMI-DEV baseline while using less than half the mid-training tokens, with additional improvements on general code generation and scientific benchmarks.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>daVinci-Dev: Agent-native Mid-training for Software Engineering</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Title -->
  <text x="500" y="30" font-family="Arial, sans-serif" font-size="24" font-weight="bold" text-anchor="middle" fill="#2C3E50">daVinci-Dev: Agent-native Mid-training Workflow</text>
  
  <!-- Data Sources -->
  <rect x="50" y="70" width="180" height="80" rx="10" fill="#3498DB" stroke="#2980B9" stroke-width="2"/>
  <text x="140" y="100" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">GitHub Pull Requests</text>
  <text x="140" y="120" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Top 10K repos</text>
  <text x="140" y="135" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Python repos</text>
  
  <!-- Data Collection & Filtering -->
  <rect x="300" y="70" width="160" height="80" rx="10" fill="#9B59B6" stroke="#8E44AD" stroke-width="2"/>
  <text x="380" y="100" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Collection &amp;</text>
  <text x="380" y="115" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Filtering</text>
  <text x="380" y="135" font-family="Arial" font-size="11" text-anchor="middle" fill="white">4M General + 6M Python</text>
  
  <!-- Contextually-Native Branch -->
  <rect x="100" y="200" width="180" height="100" rx="10" fill="#E74C3C" stroke="#C0392B" stroke-width="2"/>
  <text x="190" y="225" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Contextually-Native</text>
  <text x="190" y="245" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Trajectories</text>
  <text x="190" y="265" font-family="Arial" font-size="12" text-anchor="middle" fill="white">68.6B tokens</text>
  <text x="190" y="285" font-family="Arial" font-size="11" text-anchor="middle" fill="white">Coverage &amp; Diversity</text>
  
  <!-- Reconstruction Steps -->
  <rect x="320" y="200" width="140" height="40" rx="5" fill="#F39C12" stroke="#E67E22" stroke-width="1"/>
  <text x="390" y="225" font-family="Arial" font-size="12" text-anchor="middle" fill="white">File Identification</text>
  
  <rect x="320" y="250" width="140" height="40" rx="5" fill="#F39C12" stroke="#E67E22" stroke-width="1"/>
  <text x="390" y="275" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Content Rewrite</text>
  
  <rect x="320" y="300" width="140" height="40" rx="5" fill="#F39C12" stroke="#E67E22" stroke-width="1"/>
  <text x="390" y="325" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Template Organization</text>
  
  <!-- Environmentally-Native Branch -->
  <rect x="550" y="200" width="180" height="100" rx="10" fill="#16A085" stroke="#138D75" stroke-width="2"/>
  <text x="640" y="225" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Environmentally-Native</text>
  <text x="640" y="245" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Trajectories</text>
  <text x="640" y="265" font-family="Arial" font-size="12" text-anchor="middle" fill="white">3.1B tokens</text>
  <text x="640" y="285" font-family="Arial" font-size="11" text-anchor="middle" fill="white">Interaction Authenticity</text>
  
  <!-- Docker Environment -->
  <rect x="770" y="200" width="160" height="100" rx="10" fill="#34495E" stroke="#2C3E50" stroke-width="2"/>
  <text x="850" y="225" font-family="Arial" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Docker Environment</text>
  <text x="850" y="245" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Unit Tests</text>
  <text x="850" y="265" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Real Execution</text>
  <text x="850" y="285" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Feedback Loop</text>
  
  <!-- Mid-Training -->
  <rect x="300" y="400" width="400" height="80" rx="10" fill="#2ECC71" stroke="#27AE60" stroke-width="2"/>
  <text x="500" y="430" font-family="Arial" font-size="18" font-weight="bold" text-anchor="middle" fill="white">Mid-Training</text>
  <text x="500" y="455" font-family="Arial" font-size="14" text-anchor="middle" fill="white">Qwen2.5 Base Models (32B/72B)</text>
  
  <!-- Post-Training -->
  <rect x="300" y="520" width="400" height="60" rx="10" fill="#E67E22" stroke="#D35400" stroke-width="2"/>
  <text x="500" y="545" font-family="Arial" font-size="16" font-weight="bold" text-anchor="middle" fill="white">Post-Training (SFT)</text>
  <text x="500" y="565" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Supervised Fine-tuning on Agentic Trajectories</text>
  
  <!-- Final Model -->
  <rect x="350" y="620" width="300" height="80" rx="10" fill="#8E44AD" stroke="#7D3C98" stroke-width="2"/>
  <text x="500" y="650" font-family="Arial" font-size="18" font-weight="bold" text-anchor="middle" fill="white">daVinci-Dev Models</text>
  <text x="500" y="675" font-family="Arial" font-size="14" text-anchor="middle" fill="white">56.1% (32B) | 58.5% (72B) on SWE-Bench</text>
  
  <!-- Workflow Actions -->
  <ellipse cx="100" y="370" rx="80" ry="30" fill="#3498DB" stroke="#2980B9" stroke-width="2"/>
  <text x="100" y="375" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Localize &amp; Read</text>
  
  <ellipse cx="250" y="370" rx="60" ry="30" fill="#9B59B6" stroke="#8E44AD" stroke-width="2"/>
  <text x="250" y="375" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Edit &amp; Diff</text>
  
  <ellipse cx="750" y="370" rx="80" ry="30" fill="#16A085" stroke="#138D75" stroke-width="2"/>
  <text x="750" y="375" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Test &amp; Verify</text>
  
  <ellipse cx="900" y="370" rx="60" ry="30" fill="#E74C3C" stroke="#C0392B" stroke-width="2"/>
  <text x="900" y="375" font-family="Arial" font-size="12" text-anchor="middle" fill="white">Revise</text>
  
  <!-- Arrows -->
  <path d="M 230 110 L 300 110" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 380 150 L 190 200" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 380 150 L 390 200" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 460 110 L 640 200" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 730 250 L 770 250" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 190 300 L 400 400" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 640 300 L 500 400" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 500 480 L 500 520" stroke="#34495E" stroke-width="2" fill="none"/>
  <path d="M 500 580 L 500 620" stroke="#34495E" stroke-width="2" fill="none"/>
  
  <!-- Legend -->
  <rect x="50" y="730" width="900" height="50" rx="5" fill="#ECF0F1" stroke="#BDC3C7" stroke-width="1"/>
  <text x="70" y="755" font-family="Arial" font-size="12" font-weight="bold" fill="#2C3E50">Key Insights:</text>
  <text x="180" y="755" font-family="Arial" font-size="11" fill="#2C3E50">‚Ä¢ Agent-native data preserves complete workflows</text>
  <text x="480" y="755" font-family="Arial" font-size="11" fill="#2C3E50">‚Ä¢ Two complementary trajectory types</text>
  <text x="720" y="755" font-family="Arial" font-size="11" fill="#2C3E50">‚Ä¢ 73.1B tokens total</text>
  <text x="180" y="770" font-family="Arial" font-size="11" fill="#2C3E50">‚Ä¢ Contextual: broad coverage</text>
  <text x="480" y="770" font-family="Arial" font-size="11" fill="#2C3E50">‚Ä¢ Environmental: authentic feedback</text>
  <text x="720" y="770" font-family="Arial" font-size="11" fill="#2C3E50">‚Ä¢ SOTA performance</text>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Using GitHub Pull Requests to create agent-native data that preserves the complete action-observation loop structure">
                        <div class="quiz-question">1. What is the key innovation that distinguishes daVinci-Dev's approach from traditional code model training?</div>
                        <div class="quiz-choices"><div class="quiz-choice long-text" data-value="Using GitHub Pull Requests to create agent-native data that preserves the complete action-observation loop structure">Using GitHub Pull Requests to create agent-native data that preserves the complete action-observation loop structure</div><div class="quiz-choice" data-value="Training exclusively on Python repositories with more than 10,000 stars">Training exclusively on Python repositories with more than 10,000 stars</div><div class="quiz-choice" data-value="Replacing supervised fine-tuning with unsupervised pre-training on raw code files">Replacing supervised fine-tuning with unsupervised pre-training on raw code files</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="It achieves superior performance using less than half the mid-training tokens (73.1B vs ~150B)">
                        <div class="quiz-question">2. How does daVinci-Dev's token efficiency compare to the previous state-of-the-art KIMI-DEV approach?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It uses approximately the same number of tokens (150B) but achieves better performance">It uses approximately the same number of tokens (150B) but achieves better performance</div><div class="quiz-choice" data-value="It achieves superior performance using less than half the mid-training tokens (73.1B vs ~150B)">It achieves superior performance using less than half the mid-training tokens (73.1B vs ~150B)</div><div class="quiz-choice" data-value="It requires 3x more tokens but compensates with better data quality">It requires 3x more tokens but compensates with better data quality</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="The model showed improved performance on scientific reasoning benchmarks like GPQA and SciBench">
                        <div class="quiz-question">3. What unexpected benefit did the authors observe from their agentic mid-training approach?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="The model became significantly faster at inference time due to optimized attention patterns">The model became significantly faster at inference time due to optimized attention patterns</div><div class="quiz-choice" data-value="The model showed improved performance on scientific reasoning benchmarks like GPQA and SciBench">The model showed improved performance on scientific reasoning benchmarks like GPQA and SciBench</div><div class="quiz-choice" data-value="The model automatically learned to write documentation without explicit training">The model automatically learned to write documentation without explicit training</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/diagmonds.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2026-01-26</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2601.18491" target="_blank">http://arxiv.org/pdf/2601.18491</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on AI agent safety and security, specifically developing a diagnostic guardrail framework for monitoring and evaluating risks in autonomous AI agents.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing guardrail models (LlamaGuard, Qwen3Guard, ShieldGemma) but proposes a novel three-dimensional safety taxonomy (risk source, failure mode, real-world harm) and introduces fine-grained risk diagnosis beyond binary safe/unsafe classification.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the lack of agentic risk awareness in current guardrail models and the absence of transparency in understanding why agents take unsafe or seemingly safe but unreasonable actions.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors use a taxonomy-guided data synthesis pipeline to generate agent trajectories, train AgentDoG models through supervised fine-tuning on multiple model families (Qwen, Llama), and implement an Agentic XAI framework for hierarchical attribution analysis.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> AgentDoG achieves state-of-the-art performance on R-Judge (91.84% accuracy), ASSE-Safety (81.10% accuracy), and ATBench (92.80% accuracy), significantly outperforming existing guard models in both binary safety classification and fine-grained risk diagnosis tasks.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Title -->
  <text x="500" y="30" font-size="24" font-weight="bold" text-anchor="middle" fill="#2C3E50">AgentDoG: Diagnostic Guardrail Framework Workflow</text>
  
  <!-- Phase 1: Safety Taxonomy -->
  <rect x="50" y="70" width="900" height="120" fill="#E8F4FD" stroke="#3498DB" stroke-width="2" rx="10"/>
  <text x="500" y="95" font-size="18" font-weight="bold" text-anchor="middle" fill="#2C3E50">1. Safety Taxonomy Development</text>
  
  <!-- Three Dimensions -->
  <rect x="100" y="110" width="250" height="60" fill="#FFF3CD" stroke="#FFC107" stroke-width="2" rx="5"/>
  <text x="225" y="135" font-size="14" font-weight="bold" text-anchor="middle" fill="#856404">Risk Source</text>
  <text x="225" y="155" font-size="12" text-anchor="middle" fill="#856404">(Where risk comes from)</text>
  
  <rect x="375" y="110" width="250" height="60" fill="#D1ECF1" stroke="#17A2B8" stroke-width="2" rx="5"/>
  <text x="500" y="135" font-size="14" font-weight="bold" text-anchor="middle" fill="#0C5460">Failure Mode</text>
  <text x="500" y="155" font-size="12" text-anchor="middle" fill="#0C5460">(How agent fails)</text>
  
  <rect x="650" y="110" width="250" height="60" fill="#F8D7DA" stroke="#DC3545" stroke-width="2" rx="5"/>
  <text x="775" y="135" font-size="14" font-weight="bold" text-anchor="middle" fill="#721C24">Real-world Harm</text>
  <text x="775" y="155" font-size="12" text-anchor="middle" fill="#721C24">(What harm occurs)</text>
  
  <!-- Arrow down -->
  <path d="M 500 190 L 500 220" stroke="#34495E" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>
  
  <!-- Phase 2: Data Synthesis -->
  <rect x="50" y="230" width="900" height="180" fill="#E8F8F5" stroke="#27AE60" stroke-width="2" rx="10"/>
  <text x="500" y="255" font-size="18" font-weight="bold" text-anchor="middle" fill="#2C3E50">2. Taxonomy-Guided Data Synthesis</text>
  
  <!-- Three Stages -->
  <rect x="100" y="280" width="250" height="100" fill="#D5E8D4" stroke="#82C341" stroke-width="2" rx="5"/>
  <text x="225" y="305" font-size="14" font-weight="bold" text-anchor="middle" fill="#2D5016">Stage 1: Planning</text>
  <text x="225" y="325" font-size="11" text-anchor="middle" fill="#2D5016">‚Ä¢ Sample risk configuration</text>
  <text x="225" y="345" font-size="11" text-anchor="middle" fill="#2D5016">‚Ä¢ Design multi-step task</text>
  <text x="225" y="365" font-size="11" text-anchor="middle" fill="#2D5016">‚Ä¢ Create execution plan</text>
  
  <rect x="375" y="280" width="250" height="100" fill="#FFE6CC" stroke="#FF9800" stroke-width="2" rx="5"/>
  <text x="500" y="305" font-size="14" font-weight="bold" text-anchor="middle" fill="#663C00">Stage 2: Trajectory Synthesis</text>
  <text x="500" y="325" font-size="11" text-anchor="middle" fill="#663C00">‚Ä¢ Generate user queries</text>
  <text x="500" y="345" font-size="11" text-anchor="middle" fill="#663C00">‚Ä¢ Simulate tool interactions</text>
  <text x="500" y="365" font-size="11" text-anchor="middle" fill="#663C00">‚Ä¢ Inject risks at trigger points</text>
  
  <rect x="650" y="280" width="250" height="100" fill="#E1BEE7" stroke="#9C27B0" stroke-width="2" rx="5"/>
  <text x="775" y="305" font-size="14" font-weight="bold" text-anchor="middle" fill="#4A148C">Stage 3: Quality Control</text>
  <text x="775" y="325" font-size="11" text-anchor="middle" fill="#4A148C">‚Ä¢ Structural validation</text>
  <text x="775" y="345" font-size="11" text-anchor="middle" fill="#4A148C">‚Ä¢ Label consistency check</text>
  <text x="775" y="365" font-size="11" text-anchor="middle" fill="#4A148C">‚Ä¢ ~52% pass rate</text>
  
  <!-- Arrow down -->
  <path d="M 500 410 L 500 440" stroke="#34495E" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>
  
  <!-- Phase 3: Model Training -->
  <rect x="50" y="450" width="900" height="100" fill="#FCE4EC" stroke="#E91E63" stroke-width="2" rx="10"/>
  <text x="500" y="475" font-size="18" font-weight="bold" text-anchor="middle" fill="#2C3E50">3. AgentDoG Training</text>
  
  <rect x="200" y="490" width="600" height="40" fill="#F48FB1" stroke="#C2185B" stroke-width="2" rx="5"/>
  <text x="500" y="515" font-size="14" text-anchor="middle" fill="#FFFFFF">Supervised Fine-Tuning on Qwen (4B, 7B) and Llama (8B) models</text>
  
  <!-- Arrow down -->
  <path d="M 500 550 L 500 580" stroke="#34495E" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>
  
  <!-- Phase 4: Evaluation -->
  <rect x="50" y="590" width="900" height="150" fill="#F3E5F5" stroke="#7B1FA2" stroke-width="2" rx="10"/>
  <text x="500" y="615" font-size="18" font-weight="bold" text-anchor="middle" fill="#2C3E50">4. Evaluation & Attribution</text>
  
  <!-- Two main capabilities -->
  <rect x="100" y="640" width="350" height="80" fill="#E8EAF6" stroke="#3F51B5" stroke-width="2" rx="5"/>
  <text x="275" y="665" font-size="14" font-weight="bold" text-anchor="middle" fill="#1A237E">Trajectory-level Safety</text>
  <text x="275" y="685" font-size="12" text-anchor="middle" fill="#1A237E">‚Ä¢ Binary classification (safe/unsafe)</text>
  <text x="275" y="705" font-size="12" text-anchor="middle" fill="#1A237E">‚Ä¢ State-of-the-art performance</text>
  
  <rect x="550" y="640" width="350" height="80" fill="#FFF9C4" stroke="#F57C00" stroke-width="2" rx="5"/>
  <text x="725" y="665" font-size="14" font-weight="bold" text-anchor="middle" fill="#E65100">Fine-grained Diagnosis + XAI</text>
  <text x="725" y="685" font-size="12" text-anchor="middle" fill="#E65100">‚Ä¢ Risk taxonomy classification</text>
  <text x="725" y="705" font-size="12" text-anchor="middle" fill="#E65100">‚Ä¢ Hierarchical attribution analysis</text>
  
  <!-- Results box -->
  <rect x="200" y="750" width="600" height="30" fill="#C8E6C9" stroke="#4CAF50" stroke-width="2" rx="5"/>
  <text x="500" y="770" font-size="14" font-weight="bold" text-anchor="middle" fill="#1B5E20">Output: Safe/Unsafe verdict + Root cause diagnosis + Attribution scores</text>
  
  <!-- Arrow definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#34495E"/>
    </marker>
  </defs>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Risk source (where), failure mode (how), and real-world harm (what)">
                        <div class="quiz-question">1. What are the three orthogonal dimensions in AgentDoG's unified safety taxonomy?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Risk source (where), failure mode (how), and real-world harm (what)">Risk source (where), failure mode (how), and real-world harm (what)</div><div class="quiz-choice" data-value="Security threats, safety violations, and performance metrics">Security threats, safety violations, and performance metrics</div><div class="quiz-choice" data-value="User inputs, model outputs, and environmental factors">User inputs, model outputs, and environmental factors</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="AgentDoG's tool library is 40-86√ó larger than existing benchmarks, containing ~10,000 tools">
                        <div class="quiz-question">2. How does AgentDoG's tool coverage compare to existing agent safety benchmarks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="AgentDoG uses approximately the same number of tools as R-Judge and ASSE-Safety">AgentDoG uses approximately the same number of tools as R-Judge and ASSE-Safety</div><div class="quiz-choice" data-value="AgentDoG's tool library is 40-86√ó larger than existing benchmarks, containing ~10,000 tools">AgentDoG's tool library is 40-86√ó larger than existing benchmarks, containing ~10,000 tools</div><div class="quiz-choice" data-value="AgentDoG focuses on quality over quantity with only 100 carefully curated tools">AgentDoG focuses on quality over quantity with only 100 carefully curated tools</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="AgentDoG performs trajectory-level analysis and can diagnose root causes of unsafe behaviors throughout the execution">
                        <div class="quiz-question">3. What distinguishes AgentDoG's approach from traditional guardrail models when evaluating agent safety?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="AgentDoG only checks the final output of an agent's response for safety violations">AgentDoG only checks the final output of an agent's response for safety violations</div><div class="quiz-choice long-text" data-value="AgentDoG performs trajectory-level analysis and can diagnose root causes of unsafe behaviors throughout the execution">AgentDoG performs trajectory-level analysis and can diagnose root causes of unsafe behaviors throughout the execution</div><div class="quiz-choice" data-value="AgentDoG relies exclusively on rule-based heuristics without any machine learning components">AgentDoG relies exclusively on rule-based heuristics without any machine learning components</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/type.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2026-01-27</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2601.19834" target="_blank">http://arxiv.org/pdf/2601.19834</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper investigates visual generation for multimodal reasoning in AI, specifically examining when and how visual world modeling enhances chain-of-thought reasoning compared to purely verbal approaches.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Building on chain-of-thought reasoning in LLMs/VLMs and unified multimodal models (UMMs), the paper proposes the "visual superiority hypothesis" that visual generation serves as superior world models for physical/spatial tasks due to richer representations and complementary prior knowledge.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> Current multimodal AI systems excel at abstract domains but lag behind humans in physical and spatial reasoning tasks, potentially due to their reliance on verbal reasoning pathways without dedicated visual world modeling capabilities.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors formalize world modeling with two capabilities (reconstruction and simulation), create VisWorld-Eval benchmark with 7 tasks, and conduct controlled experiments using BAGEL (a state-of-the-art UMM) comparing implicit, verbal, and visual world modeling approaches.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Visual world modeling significantly outperforms verbal approaches on physical/spatial tasks (paper folding, multi-hop manipulation, ball tracking) with 4√ó better sample efficiency, while showing no advantage on simple grid-world tasks (maze, Sokoban) where implicit modeling suffices.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Title -->
  <text x="500" y="40" text-anchor="middle" font-size="24" font-weight="bold" fill="#2C3E50">Visual Generation Unlocks Human-Like Reasoning</text>
  <text x="500" y="65" text-anchor="middle" font-size="18" fill="#34495E">Multimodal World Models Workflow</text>
  
  <!-- Main Flow Container -->
  <rect x="50" y="90" width="900" height="680" fill="none" stroke="#BDC3C7" stroke-width="2" rx="10"/>
  
  <!-- Step 1: Problem Formulation -->
  <rect x="100" y="120" width="350" height="100" fill="#3498DB" rx="10"/>
  <text x="275" y="150" text-anchor="middle" font-size="16" font-weight="bold" fill="white">1. Problem Formulation</text>
  <text x="275" y="175" text-anchor="middle" font-size="12" fill="white">Multi-Observable Markov Decision Process</text>
  <text x="275" y="195" text-anchor="middle" font-size="12" fill="white">(MOMDP): States, Actions, Observations</text>
  
  <!-- Step 2: World Model Capabilities -->
  <rect x="550" y="120" width="350" height="100" fill="#E74C3C" rx="10"/>
  <text x="725" y="150" text-anchor="middle" font-size="16" font-weight="bold" fill="white">2. World Model Capabilities</text>
  <text x="725" y="175" text-anchor="middle" font-size="12" fill="white">‚Ä¢ World Reconstruction</text>
  <text x="725" y="195" text-anchor="middle" font-size="12" fill="white">‚Ä¢ World Simulation</text>
  
  <!-- Connection 1-2 -->
  <path d="M 450 170 L 550 170" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  
  <!-- Step 3: CoT Formulations -->
  <rect x="100" y="260" width="800" height="120" fill="#9B59B6" rx="10"/>
  <text x="500" y="290" text-anchor="middle" font-size="16" font-weight="bold" fill="white">3. Chain-of-Thought Formulations</text>
  
  <!-- Three sub-boxes for CoT types -->
  <rect x="130" y="310" width="220" height="50" fill="#8E44AD" rx="5"/>
  <text x="240" y="340" text-anchor="middle" font-size="12" fill="white">Implicit World Modeling</text>
  
  <rect x="390" y="310" width="220" height="50" fill="#8E44AD" rx="5"/>
  <text x="500" y="340" text-anchor="middle" font-size="12" fill="white">Verbal World Modeling</text>
  
  <rect x="650" y="310" width="220" height="50" fill="#8E44AD" rx="5"/>
  <text x="760" y="340" text-anchor="middle" font-size="12" fill="white">Visual World Modeling</text>
  
  <!-- Connections to CoT -->
  <path d="M 275 220 L 275 260" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  <path d="M 725 220 L 725 260" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  
  <!-- Step 4: Visual Superiority Hypothesis -->
  <rect x="200" y="420" width="600" height="80" fill="#F39C12" rx="10"/>
  <text x="500" y="450" text-anchor="middle" font-size="16" font-weight="bold" fill="white">4. Visual Superiority Hypothesis</text>
  <text x="500" y="475" text-anchor="middle" font-size="12" fill="white">Visual generation provides richer informativeness &amp; complementary knowledge</text>
  
  <!-- Connection from CoT to Hypothesis -->
  <path d="M 500 380 L 500 420" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  
  <!-- Step 5: Evaluation Suite -->
  <rect x="100" y="540" width="350" height="100" fill="#16A085" rx="10"/>
  <text x="275" y="570" text-anchor="middle" font-size="16" font-weight="bold" fill="white">5. VisWorld-Eval Suite</text>
  <text x="275" y="595" text-anchor="middle" font-size="12" fill="white">7 tasks spanning synthetic &amp;</text>
  <text x="275" y="615" text-anchor="middle" font-size="12" fill="white">real-world domains</text>
  
  <!-- Step 6: Model Training -->
  <rect x="550" y="540" width="350" height="100" fill="#27AE60" rx="10"/>
  <text x="725" y="570" text-anchor="middle" font-size="16" font-weight="bold" fill="white">6. Model Training</text>
  <text x="725" y="595" text-anchor="middle" font-size="12" fill="white">BAGEL UMM with SFT + RLVR</text>
  <text x="725" y="615" text-anchor="middle" font-size="12" fill="white">Interleaved verbal-visual generation</text>
  
  <!-- Connections from Hypothesis -->
  <path d="M 350 500 L 275 540" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  <path d="M 650 500 L 725 540" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  
  <!-- Step 7: Results -->
  <rect x="300" y="680" width="400" height="70" fill="#E67E22" rx="10"/>
  <text x="500" y="705" text-anchor="middle" font-size="16" font-weight="bold" fill="white">7. Experimental Results</text>
  <text x="500" y="730" text-anchor="middle" font-size="12" fill="white">Visual world modeling significantly outperforms verbal CoT</text>
  
  <!-- Final connections -->
  <path d="M 275 640 L 350 680" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  <path d="M 725 640 L 650 680" stroke="#7F8C8D" stroke-width="3" fill="none"/>
  
  <!-- Decorative elements -->
  <circle cx="150" cy="170" r="25" fill="#2980B9" opacity="0.3"/>
  <circle cx="850" cy="170" r="25" fill="#C0392B" opacity="0.3"/>
  <circle cx="500" cy="340" r="30" fill="#7D3C98" opacity="0.3"/>
  <circle cx="500" cy="460" r="25" fill="#D68910" opacity="0.3"/>
  <circle cx="275" cy="590" r="25" fill="#138D75" opacity="0.3"/>
  <circle cx="725" cy="590" r="25" fill="#229954" opacity="0.3"/>
  <circle cx="500" cy="715" r="25" fill="#CA6F1E" opacity="0.3"/>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="The model developed emergent implicit world representations that could predict masked coordinates with near-perfect accuracy after fine-tuning">
                        <div class="quiz-question">1. What surprising finding did the researchers discover when probing the internal representations of BAGEL on maze tasks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="The model completely failed to track maze states internally without explicit coordinates">The model completely failed to track maze states internally without explicit coordinates</div><div class="quiz-choice long-text" data-value="The model developed emergent implicit world representations that could predict masked coordinates with near-perfect accuracy after fine-tuning">The model developed emergent implicit world representations that could predict masked coordinates with near-perfect accuracy after fine-tuning</div><div class="quiz-choice" data-value="Visual world modeling was essential for the model to solve even simple 5√ó5 mazes">Visual world modeling was essential for the model to solve even simple 5√ó5 mazes</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Visual representations provide richer informativeness and leverage complementary prior knowledge from visual pre-training data">
                        <div class="quiz-question">2. According to the visual superiority hypothesis, why does visual world modeling outperform verbal world modeling for physical tasks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Visual models are computationally more efficient and require less memory than verbal models">Visual models are computationally more efficient and require less memory than verbal models</div><div class="quiz-choice long-text" data-value="Visual representations provide richer informativeness and leverage complementary prior knowledge from visual pre-training data">Visual representations provide richer informativeness and leverage complementary prior knowledge from visual pre-training data</div><div class="quiz-choice" data-value="Humans prefer visual explanations over verbal descriptions in all reasoning scenarios">Humans prefer visual explanations over verbal descriptions in all reasoning scenarios</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="It achieved 4√ó better sample efficiency, reaching comparable performance with significantly less training data">
                        <div class="quiz-question">3. In the paper folding task, what advantage did visual world modeling demonstrate compared to verbal world modeling?</div>
                        <div class="quiz-choices"><div class="quiz-choice long-text" data-value="It achieved 4√ó better sample efficiency, reaching comparable performance with significantly less training data">It achieved 4√ó better sample efficiency, reaching comparable performance with significantly less training data</div><div class="quiz-choice" data-value="It completely eliminated all reasoning errors while verbal models failed entirely">It completely eliminated all reasoning errors while verbal models failed entirely</div><div class="quiz-choice" data-value="It reduced computational costs by avoiding complex matrix representations">It reduced computational costs by avoiding complex matrix representations</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            

    <!-- Personal Takeaways Section -->
    <div id="takeaways-container"></div>

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

    <!-- MathJax for LaTeX rendering (only for takeaways section) -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            },
            startup: {
                pageReady: () => {
                    // Disable automatic processing - we'll only process takeaways manually
                    return Promise.resolve();
                }
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Load and render markdown takeaways
            const dateMatch = document.querySelector('h1').textContent.match(/(\d{4}-\d{2}-\d{2})/);
            if (dateMatch) {
                const date = dateMatch[1];
                const mdPath = `../notes/${date}.md`;

                // Use XMLHttpRequest for better file:// protocol support
                const xhr = new XMLHttpRequest();
                xhr.onreadystatechange = function() {
                    if (xhr.readyState === 4) {
                        console.log('XHR Status:', xhr.status, 'Response length:', xhr.responseText.length);

                        if (xhr.status === 200 || xhr.status === 0) {  // status 0 for file://
                            const markdown = xhr.responseText;

                            if (!markdown || markdown.trim().length === 0) {
                                console.log('Empty markdown file');
                                return;
                            }

                            console.log('Markdown loaded, length:', markdown.length);

                            // Check if marked is loaded
                            if (typeof marked === 'undefined') {
                                console.error('marked.js library not loaded');
                                return;
                            }

                            // Convert markdown to HTML
                            const htmlContent = marked.parse(markdown);
                            console.log('HTML converted, length:', htmlContent.length);

                            // Fix image paths
                            const fixedContent = htmlContent.replace(
                                /src="(?!http:\/\/|https:\/\/|\/|\.\.\/)([^"]+)"/g,
                                `src="../images/${date}/$1"`
                            );

                            // Wrap in styled divs
                            const wrappedHtml = `
                                <div class="takeaways-section">
                                    <h2>üìù My Takeaways</h2>
                                    <div class="takeaways-content">
                                        ${fixedContent}
                                    </div>
                                </div>
                            `;

                            document.getElementById('takeaways-container').innerHTML = wrappedHtml;
                            console.log('Takeaways section rendered');

                            // Trigger MathJax to render LaTeX equations
                            if (typeof MathJax !== 'undefined') {
                                MathJax.typesetPromise([document.getElementById('takeaways-container')])
                                    .then(() => {
                                        console.log('MathJax rendering complete');
                                    })
                                    .catch((err) => console.error('MathJax rendering error:', err));
                            }
                        } else {
                            console.log('XHR failed - Status:', xhr.status);
                        }
                    }
                };
                xhr.open('GET', mdPath, true);
                console.log('Loading markdown from:', mdPath);
                xhr.send();
            }

            // ÂàõÂª∫ÈÅÆÁΩ©Â±Ç
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // Ëé∑ÂèñÊâÄÊúâÈóÆÈ¢òÊ†áÁ≠æ
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // ËÆæÁΩÆÁÇπÂáª‰∫ã‰ª∂Â§ÑÁêÜ
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // ÁÇπÂáªÊ†áÁ≠æÂàáÊç¢ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫Áä∂ÊÄÅ
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // ÈòªÊ≠¢‰∫ã‰ª∂ÂÜíÊ≥°
                    
                    // Â¶ÇÊûúÂΩìÂâçÈóÆÈ¢òÂç°Â∑≤ÁªèÊòæÁ§∫ÔºåÂàôÈöêËóèÂÆÉ
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // ÂÖàÈöêËóèÊâÄÊúâÂÖ∂‰ªñÈóÆÈ¢òÂç°
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // Â∞ÜÂºπÁ™óÂÜÖÂÆπÂ§çÂà∂Âà∞È°µÈù¢ÊúÄÂ§ñÂ±ÇÁöÑÂºπÁ™ó‰∏≠
                        document.body.appendChild(popup);
                        
                        // ÊòæÁ§∫ÂΩìÂâçÈóÆÈ¢òÂç°ÂíåËÉåÊôØÈÅÆÁΩ©
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // Á°Æ‰øùÁÇπÂáªÈóÆÈ¢òÂç°ÂÜÖÈÉ®Êó∂‰∏ç‰ºöÂÖ≥Èó≠ÈóÆÈ¢òÂç°
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // ÁÇπÂáªÈÅÆÁΩ©Â±ÇÊàñÈ°µÈù¢‰ªª‰ΩïÂÖ∂‰ªñ‰ΩçÁΩÆÊó∂ÈöêËóèÊâÄÊúâÈóÆÈ¢òÂç°
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // ‰∏∫ÊØè‰∏™ÈÄâÈ°πÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // ÈáçÁΩÆÊâÄÊúâÈÄâÈ°π
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // Ê†áËÆ∞ÂΩìÂâçÈÄâÈ°π‰∏∫Â∑≤ÈÄâ
                    this.classList.add('selected');
                    
                    // Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '‚úîÔ∏è CorrectÔºÅ';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '‚ùå WrongÔºÅ';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // Âç°ÁâáËΩÆÊí≠ÂäüËÉΩ - Êñ∞Â¢û
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // Êõ¥Êñ∞ËÆ°Êï∞Âô®ÊòæÁ§∫
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // ÊòæÁ§∫ÊåáÂÆöÁ¥¢ÂºïÁöÑÂç°Áâá
                function showCard(index) {
                    // Â§ÑÁêÜÂæ™ÁéØ
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // Êõ¥Êñ∞ÂΩìÂâçÁ¥¢Âºï
                    currentIndex = index;
                    
                    // Êõ¥Êñ∞Âç°ÁâáÊòæÁ§∫
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // Êõ¥Êñ∞ËÆ°Êï∞Âô®
                    updateCounter();
                }
                
                // ‰∏ã‰∏ÄÂº†Âç°Áâá
                function nextCard(e) {
                    e.stopPropagation(); // Èò≤Ê≠¢‰∫ã‰ª∂ÂÜíÊ≥°ÂØºËá¥ÈóÆÈ¢òÂç°ÂÖ≥Èó≠
                    showCard(currentIndex + 1);
                }
                
                // ‰∏∫ÊØè‰∏™Âç°ÁâáÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂ÔºàËÄå‰∏çÊòØÊï¥‰∏™ÂÆπÂô®Ôºâ
                cards.forEach(card => {
                    card.addEventListener('click', function(e) {
                        // Âè™ÊúâÁÇπÂáªÂú®Âç°ÁâáÂÜÖÈÉ®Êó∂ÊâçÂàáÊç¢
                        // Ê£ÄÊü•ÊòØÂê¶ÊòØÊµÅÁ®ãÂõæÂç°ÁâáÁöÑÊªöÂä®Êù°Âå∫Âüü
                        if (this.classList.contains('flowchart-card')) {
                            const rect = this.getBoundingClientRect();
                            const isScrollbarClick =
                                (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                                (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);

                            if (!isScrollbarClick) {
                                nextCard(e);
                            }
                        } else {
                            nextCard(e);
                        }
                    });
                });
                
                // ÈîÆÁõòÂØºËà™
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
</body>
</html>
