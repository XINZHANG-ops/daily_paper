<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- AI Assistant Styles -->
    <link rel="stylesheet" href="../../css/ai-assistant.css">
    <title>2026-02-03 Papers</title>
    <style>
        * {
            box-sizing: border-box; /* Á°Æ‰øùÊâÄÊúâÂÖÉÁ¥†‰ΩøÁî®border-boxÊ®°Âûã */
        }

        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('../../bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* Âç°ÁâáÂÆπÂô®Ê†∑Âºè - Êñ∞Â¢û */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            min-height: 600px; /* Êîπ‰∏∫ÊúÄÂ∞èÈ´òÂ∫¶ËÄå‰∏çÊòØÂõ∫ÂÆöÈ´òÂ∫¶ */
            max-height: 90vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶Èò≤Ê≠¢Ê∫¢Âá∫ */
            height: auto; /* Ëá™ÈÄÇÂ∫îÂÜÖÂÆπÈ´òÂ∫¶ */
            /* cursor removed - only cards should show pointer */
        }
        
        /* Âç°ÁâáÈÄöÁî®Ê†∑Âºè */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
            cursor: pointer; /* Show pointer on cards to indicate they're clickable */
        }
        
        /* ËΩÆÊí≠Âç°ÁâáÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%; /* ‰ΩøÁî®ÂÆπÂô®ÁöÑ100%È´òÂ∫¶ */
            transition: transform 0.5s ease, opacity 0.5s ease;
            overflow-y: auto; /* ÂÖÅËÆ∏ÂûÇÁõ¥ÊªöÂä® */
            overflow-x: hidden; /* Èò≤Ê≠¢Ê®™ÂêëÊªöÂä® */
        }
        
        /* ÈùûÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* ÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* Á¨¨‰∏ÄÂº†Âç°ÁâáÔºàÊñáÊú¨ÂÜÖÂÆπÔºâ‰∏çÈúÄË¶ÅÊªöÂä® */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* Á¨¨‰∫åÂº†Âç°ÁâáÔºàÊµÅÁ®ãÂõæÔºâÊîØÊåÅÊªöÂä® */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow-y: auto !important; /* ÂÖÅËÆ∏ÂûÇÁõ¥ÊªöÂä® */
            overflow-x: hidden !important; /* Èò≤Ê≠¢Ê®™ÂêëÊªöÂä® */
            padding: 20px; /* Ê∑ªÂä†ÂÜÖËæπË∑ù */
            padding-bottom: 50px; /* Ê∑ªÂä†Â∫ïÈÉ®Â°´ÂÖÖÁ°Æ‰øùËÉΩÁúãÂà∞Â∫ïÈÉ® */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-width: 100%; /* Á°Æ‰øù‰∏çË∂ÖÂá∫ÂÆπÂô®ÂÆΩÂ∫¶ */
            display: block; /* Èò≤Ê≠¢Â∫ïÈÉ®Á©∫ÁôΩ */
            margin: 0 auto; /* Â±Ö‰∏≠ÊòæÁ§∫ */
        }
        
        /* ‰º†ÁªüÂç°ÁâáÊ†∑Âºè */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
            word-wrap: break-word;
            overflow-wrap: break-word;
            hyphens: auto;
        }

        .paper-card p {
            margin: 5px 0;
            word-wrap: break-word;
            overflow-wrap: break-word;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
            word-wrap: break-word;
            overflow-wrap: break-word;
            overflow: hidden; /* Èò≤Ê≠¢ÂÜÖÂÆπÊ∫¢Âá∫ */
        }

        .category-chunk * {
            word-wrap: break-word;
            overflow-wrap: break-word;
            max-width: 100%;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* Âç°ÁâáËÆ°Êï∞Âô® - Êñ∞Â¢û */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* Êîπ‰∏∫Âõ∫ÂÆöÂÆö‰ΩçÔºå‰∏çÈöèÊªöÂä®ËÄåÁßªÂä® */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* Â±Ö‰∏≠ÊòæÁ§∫ */
            width: 90%;
            max-width: 500px; /* Â¢ûÂä†ÊúÄÂ§ßÂÆΩÂ∫¶ÔºåÈÄÇÂ∫îÈïøÂÜÖÂÆπ */
            max-height: 80vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶ */
            overflow-y: auto; /* ÂÜÖÂÆπËøáÂ§öÊó∂ÂèØÊªöÂä® */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* Á°Æ‰øùÊòæÁ§∫Âú®ÊúÄ‰∏äÂ±Ç */
        }
        
        /* Ê∑ªÂä†ÈÅÆÁΩ©Â±ÇÔºåÈò≤Ê≠¢ÈóÆÈ¢òÂç°Ë¢´ÂÖ∂‰ªñÂÜÖÂÆπÈÅÆÊå° */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* ‰ΩøÁî®JavaScriptÊéßÂà∂ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫ÂíåÈöêËóèÔºå‰∏çÂÜç‰ΩøÁî®hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            hyphens: auto; /* Âú®ÂøÖË¶ÅÊó∂‰ΩøÁî®ËøûÂ≠óÁ¨¶ */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* ÈïøÊñáÊú¨Â∑¶ÂØπÈΩê */
            display: block; /* Á°Æ‰øùÊòØÂùóÁ∫ßÂÖÉÁ¥† */
            white-space: normal; /* ÂÖÅËÆ∏Ëá™Âä®Êç¢Ë°å */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* ÈïøÊñáÊú¨ÈÄâÈ°πÁöÑÁâπÊÆäÊ†∑Âºè */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* Á°Æ‰øùÂºπÁ™ó‰∏≠ÁöÑÊåâÈíÆÊñáÊú¨‰∏ç‰ºöÊ∫¢Âá∫ */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* ÈÄÇÂ∫îË∂ÖÈïøÈÄâÈ°πÊñáÊú¨ */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        /* ‰∏≠Á≠âÂ±èÂπïËÆæÂ§áÔºàÂ¶ÇÂπ≥ÊùøÔºâ */
        @media (max-width: 1024px) {
            .card-deck {
                min-height: 500px;
            }

            .card-deck .paper-card {
                max-height: 88vh;
            }
        }

        /* ÁßªÂä®ËÆæÂ§áÂíåÂ∞èÂ±èÂπï */
        @media (max-width: 768px) {
            body {
                padding: 10px; /* ÂáèÂ∞ëÁßªÂä®ËÆæÂ§á‰∏äÁöÑÂÜÖËæπË∑ù */
            }

            .paper-container {
                flex-direction: column;
            }

            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                min-height: 400px; /* ÁßªÂä®ËÆæÂ§á‰∏ä‰ΩøÁî®Êõ¥Â∞èÁöÑÊúÄÂ∞èÈ´òÂ∫¶ */
                height: auto; /* Ëá™ÈÄÇÂ∫îÈ´òÂ∫¶ */
            }

            .card-deck .paper-card {
                max-height: 85vh; /* ÁßªÂä®ËÆæÂ§á‰∏äÈôêÂà∂Êõ¥Â§ö */
                font-size: 0.95em; /* Á®çÂæÆÂáèÂ∞èÂ≠ó‰Ωì */
            }

            .paper-card h2 {
                font-size: 1.1em; /* ÁßªÂä®ËÆæÂ§á‰∏äË∞ÉÊï¥Ê†áÈ¢òÂ§ßÂ∞è */
            }

            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }

            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }

            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
                width: 45px; /* ÁßªÂä®ËÆæÂ§á‰∏äÁ®çÂ∞èÁöÑÊåâÈíÆ */
                height: 45px;
                font-size: 14px;
            }
        }

        /* ÊûÅÂ∞èÂ±èÂπïÔºàÂ¶ÇÂ∞èÊâãÊú∫Ôºâ */
        @media (max-width: 480px) {
            body {
                padding: 5px;
            }

            .card-deck {
                min-height: 300px;
            }

            .card-deck .paper-card {
                max-height: 80vh;
                font-size: 0.9em;
                padding: 10px;
            }

            .paper-card h2 {
                font-size: 1em;
            }

            .category-chunk {
                padding: 8px;
                font-size: 0.9em;
            }

            .quiz-tab {
                width: 40px;
                height: 40px;
                font-size: 12px;
            }
        }

        /* Personal Takeaways Section Styles */
        .takeaways-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .takeaways-section h2 {
            color: #ffffff;
            font-size: 2em;
            margin-bottom: 20px;
            text-align: center;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        .takeaways-content {
            background-color: rgba(255, 255, 255, 0.95);
            border-radius: 8px;
            padding: 25px;
            line-height: 1.8;
            color: #333;
        }

        .takeaways-content h3 {
            color: #667eea;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .takeaways-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .takeaways-content p {
            margin: 15px 0;
        }

        .takeaways-content ul, .takeaways-content ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        .takeaways-content li {
            margin: 8px 0;
        }

        .takeaways-content blockquote {
            border-left: 4px solid #667eea;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
            background-color: #f8f9fa;
            padding: 15px 20px;
            border-radius: 4px;
        }

        .takeaways-content code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d73a49;
        }

        .takeaways-content pre {
            background-color: #f6f8fa;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            border: 1px solid #e1e4e8;
        }

        .takeaways-content pre code {
            background-color: transparent;
            padding: 0;
            color: #333;
        }
    </style>
</head>
<body>
    <h1>2026-02-03 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/buried.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>Kimi K2.5: Visual Agentic Intelligence</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2026-02-02</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2602.02276" target="_blank">http://arxiv.org/pdf/2602.02276</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper presents Kimi K2.5, focusing on multimodal agentic intelligence that combines vision and language capabilities with parallel agent orchestration for complex task execution.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Building on previous LLMs and agentic models like GPT-5.2 and Claude Opus 4.5, the paper proposes joint text-vision optimization throughout training and introduces Agent Swarm, a framework for dynamic parallel agent orchestration that decomposes tasks into concurrent subtasks.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper addresses the limitations of sequential agent execution in existing models, which suffer from linear scaling of inference time and inability to handle complex, heterogeneous tasks efficiently.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors employ joint text-vision pre-training with early fusion and constant mixing ratios, zero-vision SFT for activating visual capabilities, joint multimodal reinforcement learning, and Parallel-Agent Reinforcement Learning (PARL) for training an orchestrator to manage multiple specialized sub-agents.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Kimi K2.5 achieves state-of-the-art results across multiple domains including 96.1% on AIME 2025, 78.5% on MMMU-Pro, and 76.8% on SWE-Bench Verified, while Agent Swarm reduces inference latency by up to 4.5√ó and improves task performance by up to 17.8% on complex agentic benchmarks.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Kimi K2.5: Visual Agentic Intelligence</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background gradient -->
  <defs>
    <linearGradient id="bgGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#e8f4f8;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#f0e8ff;stop-opacity:1" />
    </linearGradient>
    
    <!-- Arrow marker -->
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="5" orient="auto">
      <polygon points="0 0, 10 5, 0 10" fill="#4a90e2" />
    </marker>
  </defs>
  
  <rect width="1000" height="800" fill="url(#bgGrad)" />
  
  <!-- Title -->
  <text x="500" y="40" text-anchor="middle" font-size="28" font-weight="bold" fill="#2c3e50">
    Kimi K2.5: Visual Agentic Intelligence Workflow
  </text>
  
  <!-- Foundation: Kimi K2 Base -->
  <rect x="50" y="80" width="900" height="60" rx="10" fill="#3498db" opacity="0.8"/>
  <text x="500" y="115" text-anchor="middle" font-size="18" fill="white" font-weight="bold">
    Foundation: Kimi K2 Base Model (1T parameters MoE)
  </text>
  
  <!-- Joint Optimization Branch -->
  <g transform="translate(200, 180)">
    <rect x="-150" y="0" width="300" height="50" rx="8" fill="#e74c3c"/>
    <text x="0" y="30" text-anchor="middle" font-size="16" fill="white" font-weight="bold">
      Joint Text-Vision Optimization
    </text>
    
    <!-- Pre-training -->
    <rect x="-140" y="70" width="280" height="40" rx="5" fill="#f39c12"/>
    <text x="0" y="95" text-anchor="middle" font-size="14" fill="white">
      Native Multimodal Pre-training (15T tokens)
    </text>
    
    <!-- Zero-vision SFT -->
    <rect x="-140" y="130" width="280" height="40" rx="5" fill="#27ae60"/>
    <text x="0" y="155" text-anchor="middle" font-size="14" fill="white">
      Zero-Vision SFT (Text-only activation)
    </text>
    
    <!-- Joint RL -->
    <rect x="-140" y="190" width="280" height="40" rx="5" fill="#8e44ad"/>
    <text x="0" y="215" text-anchor="middle" font-size="14" fill="white">
      Joint Multimodal RL
    </text>
  </g>
  
  <!-- Agent Swarm Branch -->
  <g transform="translate(800, 180)">
    <rect x="-150" y="0" width="300" height="50" rx="8" fill="#16a085"/>
    <text x="0" y="30" text-anchor="middle" font-size="16" fill="white" font-weight="bold">
      Agent Swarm Framework
    </text>
    
    <!-- PARL -->
    <rect x="-140" y="70" width="280" height="40" rx="5" fill="#d35400"/>
    <text x="0" y="95" text-anchor="middle" font-size="14" fill="white">
      Parallel-Agent RL (PARL)
    </text>
    
    <!-- Dynamic Decomposition -->
    <rect x="-140" y="130" width="280" height="40" rx="5" fill="#c0392b"/>
    <text x="0" y="155" text-anchor="middle" font-size="14" fill="white">
      Dynamic Task Decomposition
    </text>
    
    <!-- Parallel Execution -->
    <rect x="-140" y="190" width="280" height="40" rx="5" fill="#7f8c8d"/>
    <text x="0" y="215" text-anchor="middle" font-size="14" fill="white">
      Parallel Sub-agent Execution
    </text>
  </g>
  
  <!-- Architecture Components -->
  <g transform="translate(500, 480)">
    <text x="0" y="-20" text-anchor="middle" font-size="20" fill="#2c3e50" font-weight="bold">
      Architecture Components
    </text>
    
    <!-- MoonViT-3D -->
    <rect x="-300" y="0" width="180" height="60" rx="8" fill="#3498db"/>
    <text x="-210" y="25" text-anchor="middle" font-size="14" fill="white">
      MoonViT-3D
    </text>
    <text x="-210" y="45" text-anchor="middle" font-size="12" fill="white">
      Vision Encoder
    </text>
    
    <!-- MLP Projector -->
    <rect x="-60" y="0" width="120" height="60" rx="8" fill="#9b59b6"/>
    <text x="0" y="35" text-anchor="middle" font-size="14" fill="white">
      MLP Projector
    </text>
    
    <!-- LLM -->
    <rect x="120" y="0" width="180" height="60" rx="8" fill="#e67e22"/>
    <text x="210" y="25" text-anchor="middle" font-size="14" fill="white">
      Kimi K2 MoE
    </text>
    <text x="210" y="45" text-anchor="middle" font-size="12" fill="white">
      Language Model
    </text>
  </g>
  
  <!-- Training Pipeline -->
  <g transform="translate(500, 600)">
    <text x="0" y="-20" text-anchor="middle" font-size="20" fill="#2c3e50" font-weight="bold">
      Training Pipeline Stages
    </text>
    
    <!-- Stage 1 -->
    <ellipse cx="-250" cy="40" rx="80" ry="35" fill="#1abc9c"/>
    <text x="-250" y="35" text-anchor="middle" font-size="13" fill="white">
      ViT Training
    </text>
    <text x="-250" y="50" text-anchor="middle" font-size="11" fill="white">
      (1T tokens)
    </text>
    
    <!-- Stage 2 -->
    <ellipse cx="0" cy="40" rx="80" ry="35" fill="#f39c12"/>
    <text x="0" y="35" text-anchor="middle" font-size="13" fill="white">
      Joint Pre-training
    </text>
    <text x="0" y="50" text-anchor="middle" font-size="11" fill="white">
      (15T tokens)
    </text>
    
    <!-- Stage 3 -->
    <ellipse cx="250" cy="40" rx="80" ry="35" fill="#e74c3c"/>
    <text x="250" y="35" text-anchor="middle" font-size="13" fill="white">
      Long-context
    </text>
    <text x="250" y="50" text-anchor="middle" font-size="11" fill="white">
      Mid-training
    </text>
  </g>
  
  <!-- Results -->
  <rect x="150" y="720" width="700" height="60" rx="10" fill="#2ecc71" opacity="0.8"/>
  <text x="500" y="755" text-anchor="middle" font-size="18" fill="white" font-weight="bold">
    SOTA Results: Coding, Vision, Reasoning, Agentic Tasks (4.5√ó latency reduction)
  </text>
  
  <!-- Connecting lines -->
  <line x1="500" y1="140" x2="200" y2="180" stroke="#4a90e2" stroke-width="3" marker-end="url(#arrowhead)"/>
  <line x1="500" y1="140" x2="800" y2="180" stroke="#4a90e2" stroke-width="3" marker-end="url(#arrowhead)"/>
  
  <line x1="200" y1="410" x2="200" y2="460" stroke="#4a90e2" stroke-width="2" stroke-dasharray="5,5"/>
  <line x1="800" y1="410" x2="500" y2="460" stroke="#4a90e2" stroke-width="2" stroke-dasharray="5,5"/>
  
  <line x1="170" y1="640" x2="250" y2="640" stroke="#4a90e2" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="420" y1="640" x2="500" y2="640" stroke="#4a90e2" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="500" y1="680" x2="500" y2="720" stroke="#4a90e2" stroke-width="3" marker-end="url(#arrowhead)"/>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Early fusion with lower vision ratios (10-20%) outperforms late fusion with higher ratios">
                        <div class="quiz-question">1. What surprising finding did the authors discover about the optimal vision-text training strategy for multimodal models?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Late fusion with 50% vision tokens yields the best performance">Late fusion with 50% vision tokens yields the best performance</div><div class="quiz-choice" data-value="Early fusion with lower vision ratios (10-20%) outperforms late fusion with higher ratios">Early fusion with lower vision ratios (10-20%) outperforms late fusion with higher ratios</div><div class="quiz-choice" data-value="Vision tokens should only be introduced after completing text-only pretraining">Vision tokens should only be introduced after completing text-only pretraining</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="By using a trainable orchestrator that dynamically creates frozen sub-agents and executes subtasks in parallel">
                        <div class="quiz-question">2. How does Agent Swarm address the fundamental challenge of sequential agent execution in complex tasks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By training all agents end-to-end with shared parameters for better coordination">By training all agents end-to-end with shared parameters for better coordination</div><div class="quiz-choice long-text" data-value="By using a trainable orchestrator that dynamically creates frozen sub-agents and executes subtasks in parallel">By using a trainable orchestrator that dynamically creates frozen sub-agents and executes subtasks in parallel</div><div class="quiz-choice" data-value="By pre-defining a fixed set of specialized agents that take turns processing the input">By pre-defining a fixed set of specialized agents that take turns processing the input</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Visual RL improved text-only benchmarks like MMLU-Pro (+1.7%) and GPQA-Diamond (+2.1%)">
                        <div class="quiz-question">3. What unexpected cross-modal benefit did the authors observe when applying visual reinforcement learning to Kimi K2.5?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Visual RL degraded text performance but significantly improved video understanding">Visual RL degraded text performance but significantly improved video understanding</div><div class="quiz-choice" data-value="Visual RL had no measurable impact on text-only benchmarks">Visual RL had no measurable impact on text-only benchmarks</div><div class="quiz-choice" data-value="Visual RL improved text-only benchmarks like MMLU-Pro (+1.7%) and GPQA-Diamond (+2.1%)">Visual RL improved text-only benchmarks like MMLU-Pro (+1.7%) and GPQA-Diamond (+2.1%)</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/black-orchid.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2026-02-02</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2602.02185" target="_blank">http://arxiv.org/pdf/2602.02185</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on evaluating multimodal large language models (MLLMs) in vision-based deep research tasks, specifically their visual and textual search capabilities for complex visual-textual fact-finding.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing multimodal search benchmarks (SimpleVQA, LiveVQA, FVQA, etc.) but identifies their limitations - they allow text-only shortcuts and rely on idealized whole-image retrieval; it proposes VDR-Bench with visual-search-centric design and multi-round cropped-search workflow.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> Current benchmarks fail to properly evaluate MLLMs' visual search abilities because answers can often be inferred from text cues or prior knowledge without genuine visual verification, and evaluation scenarios are unrealistically idealized.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors created VDR-Bench through a multi-stage pipeline involving manual image cropping, visual entity extraction/verification, seed VQA generation, knowledge-graph-based complexity expansion, and rigorous human review, evaluated using answer accuracy and entity recall metrics.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Models achieved low direct-answer scores (3.8-9.5%), confirming visual search necessity; with search tools, open-source models showed surprisingly strong performance (up to 21.2%), and the proposed Multi-turn Visual Forcing strategy significantly improved results (e.g., Gemini: 16.2‚Üí30.0%).</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Title -->
  <text x="500" y="30" font-size="24" font-weight="bold" text-anchor="middle" fill="#2C3E50">Vision-DeepResearch Benchmark Workflow</text>
  
  <!-- Step 0: Image Pre-Filtering -->
  <rect x="50" y="60" width="180" height="80" rx="10" fill="#3498DB" stroke="#2C3E50" stroke-width="2"/>
  <text x="140" y="90" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Step 0:</text>
  <text x="140" y="110" font-size="12" text-anchor="middle" fill="white">Multi-Domain Image</text>
  <text x="140" y="130" font-size="12" text-anchor="middle" fill="white">Pre-Filtering</text>
  
  <!-- Step 1: Manual Cropping -->
  <rect x="280" y="60" width="180" height="80" rx="10" fill="#E74C3C" stroke="#2C3E50" stroke-width="2"/>
  <text x="370" y="90" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Step 1:</text>
  <text x="370" y="110" font-size="12" text-anchor="middle" fill="white">Manual Cropping &</text>
  <text x="370" y="130" font-size="12" text-anchor="middle" fill="white">Visual Search</text>
  
  <!-- Step 2: Entity Extraction -->
  <rect x="510" y="60" width="180" height="80" rx="10" fill="#9B59B6" stroke="#2C3E50" stroke-width="2"/>
  <text x="600" y="90" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Step 2:</text>
  <text x="600" y="110" font-size="12" text-anchor="middle" fill="white">Visual Entity</text>
  <text x="600" y="130" font-size="12" text-anchor="middle" fill="white">Extraction & Verification</text>
  
  <!-- Step 3: Seed VQA Generation -->
  <rect x="740" y="60" width="180" height="80" rx="10" fill="#1ABC9C" stroke="#2C3E50" stroke-width="2"/>
  <text x="830" y="90" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Step 3:</text>
  <text x="830" y="110" font-size="12" text-anchor="middle" fill="white">Seed VQA</text>
  <text x="830" y="130" font-size="12" text-anchor="middle" fill="white">Generation</text>
  
  <!-- Connection lines for top row -->
  <line x1="230" y1="100" x2="280" y2="100" stroke="#34495E" stroke-width="3"/>
  <line x1="460" y1="100" x2="510" y2="100" stroke="#34495E" stroke-width="3"/>
  <line x1="690" y1="100" x2="740" y2="100" stroke="#34495E" stroke-width="3"/>
  
  <!-- Step 4: Knowledge Graph Expansion -->
  <rect x="280" y="200" width="200" height="80" rx="10" fill="#F39C12" stroke="#2C3E50" stroke-width="2"/>
  <text x="380" y="230" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Step 4:</text>
  <text x="380" y="250" font-size="12" text-anchor="middle" fill="white">Knowledge-Graph-Based</text>
  <text x="380" y="270" font-size="12" text-anchor="middle" fill="white">Complexity Expansion</text>
  
  <!-- Step 5: Solvability Verification -->
  <rect x="530" y="200" width="200" height="80" rx="10" fill="#E67E22" stroke="#2C3E50" stroke-width="2"/>
  <text x="630" y="230" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Step 5:</text>
  <text x="630" y="250" font-size="12" text-anchor="middle" fill="white">Solvability & Quality</text>
  <text x="630" y="270" font-size="12" text-anchor="middle" fill="white">Verification</text>
  
  <!-- Connection from Step 3 to Step 4 -->
  <line x1="830" y1="140" x2="830" y2="170" stroke="#34495E" stroke-width="3"/>
  <line x1="830" y1="170" x2="380" y2="170" stroke="#34495E" stroke-width="3"/>
  <line x1="380" y1="170" x2="380" y2="200" stroke="#34495E" stroke-width="3"/>
  
  <!-- Connection from Step 4 to Step 5 -->
  <line x1="480" y1="240" x2="530" y2="240" stroke="#34495E" stroke-width="3"/>
  
  <!-- Key Components -->
  <rect x="50" y="340" width="150" height="60" rx="8" fill="#16A085" stroke="#2C3E50" stroke-width="2"/>
  <text x="125" y="365" font-size="12" font-weight="bold" text-anchor="middle" fill="white">Cropped Image</text>
  <text x="125" y="385" font-size="12" text-anchor="middle" fill="white">Search (CIS)</text>
  
  <rect x="250" y="340" width="150" height="60" rx="8" fill="#2980B9" stroke="#2C3E50" stroke-width="2"/>
  <text x="325" y="365" font-size="12" font-weight="bold" text-anchor="middle" fill="white">Text Search</text>
  <text x="325" y="385" font-size="12" text-anchor="middle" fill="white">(TS)</text>
  
  <rect x="450" y="340" width="150" height="60" rx="8" fill="#8E44AD" stroke="#2C3E50" stroke-width="2"/>
  <text x="525" y="365" font-size="12" font-weight="bold" text-anchor="middle" fill="white">Multi-turn Visual</text>
  <text x="525" y="385" font-size="12" text-anchor="middle" fill="white">Forcing (MVF)</text>
  
  <rect x="650" y="340" width="150" height="60" rx="8" fill="#C0392B" stroke="#2C3E50" stroke-width="2"/>
  <text x="725" y="365" font-size="12" font-weight="bold" text-anchor="middle" fill="white">MLLM</text>
  <text x="725" y="385" font-size="12" text-anchor="middle" fill="white">Verification</text>
  
  <rect x="850" y="340" width="120" height="60" rx="8" fill="#D35400" stroke="#2C3E50" stroke-width="2"/>
  <text x="910" y="365" font-size="12" font-weight="bold" text-anchor="middle" fill="white">Human</text>
  <text x="910" y="385" font-size="12" text-anchor="middle" fill="white">Review</text>
  
  <!-- Final Output -->
  <rect x="350" y="460" width="300" height="80" rx="15" fill="#27AE60" stroke="#2C3E50" stroke-width="3"/>
  <text x="500" y="490" font-size="18" font-weight="bold" text-anchor="middle" fill="white">VDR-Bench</text>
  <text x="500" y="515" font-size="14" text-anchor="middle" fill="white">2,000 Visual-Search-Centric</text>
  <text x="500" y="535" font-size="14" text-anchor="middle" fill="white">VQA Instances</text>
  
  <!-- Connection to final output -->
  <line x1="630" y1="280" x2="630" y2="420" stroke="#34495E" stroke-width="3"/>
  <line x1="630" y1="420" x2="500" y2="420" stroke="#34495E" stroke-width="3"/>
  <line x1="500" y1="420" x2="500" y2="460" stroke="#34495E" stroke-width="3"/>
  
  <!-- Evaluation Metrics -->
  <rect x="150" y="600" width="180" height="60" rx="8" fill="#34495E" stroke="#2C3E50" stroke-width="2"/>
  <text x="240" y="625" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Answer Accuracy</text>
  <text x="240" y="645" font-size="12" text-anchor="middle" fill="white">Final answer correctness</text>
  
  <rect x="380" y="600" width="180" height="60" rx="8" fill="#34495E" stroke="#2C3E50" stroke-width="2"/>
  <text x="470" y="625" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Entity Recall</text>
  <text x="470" y="645" font-size="12" text-anchor="middle" fill="white">Visual entity discovery</text>
  
  <rect x="610" y="600" width="240" height="60" rx="8" fill="#34495E" stroke="#2C3E50" stroke-width="2"/>
  <text x="730" y="625" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Multi-hop Reasoning</text>
  <text x="730" y="645" font-size="12" text-anchor="middle" fill="white">Cross-modal evidence aggregation</text>
  
  <!-- Title for metrics -->
  <text x="500" y="580" font-size="16" font-weight="bold" text-anchor="middle" fill="#2C3E50">Evaluation Metrics</text>
  
  <!-- Connection from VDR-Bench to metrics -->
  <line x1="500" y1="540" x2="500" y2="560" stroke="#34495E" stroke-width="2" stroke-dasharray="5,5"/>
  
  <!-- Decorative elements -->
  <circle cx="140" cy="100" r="35" fill="none" stroke="#3498DB" stroke-width="1" opacity="0.3"/>
  <circle cx="370" cy="100" r="35" fill="none" stroke="#E74C3C" stroke-width="1" opacity="0.3"/>
  <circle cx="600" cy="100" r="35" fill="none" stroke="#9B59B6" stroke-width="1" opacity="0.3"/>
  <circle cx="830" cy="100" r="35" fill="none" stroke="#1ABC9C" stroke-width="1" opacity="0.3"/>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Lazy search - models relied on prior knowledge instead of actively using search tools">
                        <div class="quiz-question">1. What phenomenon did the researchers identify when strong MLLMs were equipped with search tools for vision-deep research tasks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Perfect retrieval bias - models retrieved exact duplicates too easily">Perfect retrieval bias - models retrieved exact duplicates too easily</div><div class="quiz-choice" data-value="Lazy search - models relied on prior knowledge instead of actively using search tools">Lazy search - models relied on prior knowledge instead of actively using search tools</div><div class="quiz-choice" data-value="Cross-modal confusion - models mixed up visual and textual information">Cross-modal confusion - models mixed up visual and textual information</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="By starting with manual cropping of visual entities and multi-stage verification to avoid text-only shortcuts">
                        <div class="quiz-question">2. How does VDR-Bench ensure that visual search is genuinely required to answer questions?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By using extremely high-resolution images that require zooming">By using extremely high-resolution images that require zooming</div><div class="quiz-choice long-text" data-value="By starting with manual cropping of visual entities and multi-stage verification to avoid text-only shortcuts">By starting with manual cropping of visual entities and multi-stage verification to avoid text-only shortcuts</div><div class="quiz-choice" data-value="By limiting the time allowed for each question response">By limiting the time allowed for each question response</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Open-source models with weaker priors showed stronger search capabilities than closed-source models">
                        <div class="quiz-question">3. What surprising finding emerged when comparing open-source and closed-source models on VDR-Bench with search tools?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Closed-source models consistently outperformed open-source models by 50%">Closed-source models consistently outperformed open-source models by 50%</div><div class="quiz-choice" data-value="All models performed equally poorly regardless of their capabilities">All models performed equally poorly regardless of their capabilities</div><div class="quiz-choice" data-value="Open-source models with weaker priors showed stronger search capabilities than closed-source models">Open-source models with weaker priors showed stronger search capabilities than closed-source models</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/tileable-wood.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2026-02-02</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2602.02437" target="_blank">http://arxiv.org/pdf/2602.02437</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on unified multimodal reasoning for world knowledge-aligned image generation and editing in computer vision and AI.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Building on existing unified multimodal models and prompt enhancement strategies, the paper proposes unifying T2I generation and image editing through dual reasoning paradigms that infer implicit world knowledge and enable iterative visual refinement.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper addresses the limitation of current unified models that struggle with complex synthesis tasks requiring deep reasoning beyond surface-level pixels and treat generation and editing as isolated capabilities.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors use a two-stage training strategy with world knowledge-enhanced textual reasoning for initial synthesis and fine-grained editing-like visual refinement for iterative improvement, supported by systematically constructed datasets across five knowledge domains.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> UniReason achieves state-of-the-art performance on reasoning-intensive benchmarks (WISE: 0.78, KrisBench: 68.23, UniREditBench: 70.06) while maintaining superior general synthesis capabilities on GenEval (0.90) and DPGBench (86.21).</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Title -->
  <text x="500" y="30" font-size="24" font-weight="bold" text-anchor="middle" fill="#1a1a1a">UniReason: Unified Reasoning Framework Workflow</text>
  
  <!-- Input Section -->
  <rect x="50" y="60" width="200" height="60" rx="10" fill="#4CAF50" stroke="#2E7D32" stroke-width="2"/>
  <text x="150" y="85" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Input</text>
  <text x="150" y="105" font-size="12" text-anchor="middle" fill="white">Text/Image Instructions</text>
  
  <!-- Phase 1: World Knowledge-Enhanced Textual Reasoning -->
  <g transform="translate(0, 40)">
    <rect x="320" y="80" width="300" height="140" rx="15" fill="#2196F3" stroke="#1565C0" stroke-width="3"/>
    <text x="470" y="105" font-size="16" font-weight="bold" text-anchor="middle" fill="white">Phase 1</text>
    <text x="470" y="125" font-size="14" text-anchor="middle" fill="white">World Knowledge-Enhanced</text>
    <text x="470" y="145" font-size="14" text-anchor="middle" fill="white">Textual Reasoning</text>
    
    <!-- Knowledge Categories -->
    <circle cx="370" cy="180" r="15" fill="#FFC107"/>
    <text x="370" y="185" font-size="10" text-anchor="middle" fill="#333">Culture</text>
    
    <circle cx="420" cy="180" r="15" fill="#FF9800"/>
    <text x="420" y="185" font-size="10" text-anchor="middle" fill="#333">Science</text>
    
    <circle cx="470" cy="180" r="15" fill="#FF5722"/>
    <text x="470" y="185" font-size="10" text-anchor="middle" fill="#333">Spatial</text>
    
    <circle cx="520" cy="180" r="15" fill="#E91E63"/>
    <text x="520" y="185" font-size="10" text-anchor="middle" fill="#333">Temporal</text>
    
    <circle cx="570" cy="180" r="15" fill="#9C27B0"/>
    <text x="570" y="185" font-size="10" text-anchor="middle" fill="#333">Logical</text>
  </g>
  
  <!-- Initial Synthesis -->
  <rect x="700" y="120" width="200" height="80" rx="10" fill="#673AB7" stroke="#4527A0" stroke-width="2"/>
  <text x="800" y="150" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Initial Synthesis</text>
  <text x="800" y="175" font-size="12" text-anchor="middle" fill="white">Draft Image Generation</text>
  
  <!-- Phase 2: Fine-grained Editing-like Visual Refinement -->
  <g transform="translate(0, 180)">
    <rect x="320" y="160" width="300" height="140" rx="15" fill="#FF5722" stroke="#D32F2F" stroke-width="3"/>
    <text x="470" y="185" font-size="16" font-weight="bold" text-anchor="middle" fill="white">Phase 2</text>
    <text x="470" y="205" font-size="14" text-anchor="middle" fill="white">Fine-grained Editing-like</text>
    <text x="470" y="225" font-size="14" text-anchor="middle" fill="white">Visual Refinement</text>
    
    <!-- Refinement Steps -->
    <rect x="350" y="245" width="60" height="40" rx="5" fill="#FFEB3B" stroke="#F9A825" stroke-width="1"/>
    <text x="380" y="265" font-size="10" text-anchor="middle" fill="#333">Verify</text>
    
    <rect x="420" y="245" width="60" height="40" rx="5" fill="#8BC34A" stroke="#689F38" stroke-width="1"/>
    <text x="450" y="265" font-size="10" text-anchor="middle" fill="#333">Reflect</text>
    
    <rect x="490" y="245" width="60" height="40" rx="5" fill="#00BCD4" stroke="#0097A7" stroke-width="1"/>
    <text x="520" y="265" font-size="10" text-anchor="middle" fill="#333">Refine</text>
  </g>
  
  <!-- Final Output -->
  <rect x="700" y="340" width="200" height="80" rx="10" fill="#4CAF50" stroke="#2E7D32" stroke-width="2"/>
  <text x="800" y="370" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Final Output</text>
  <text x="800" y="395" font-size="12" text-anchor="middle" fill="white">Refined Image</text>
  
  <!-- Training Strategy -->
  <g transform="translate(0, 250)">
    <rect x="50" y="230" width="200" height="120" rx="15" fill="#607D8B" stroke="#455A64" stroke-width="2"/>
    <text x="150" y="255" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Training Strategy</text>
    
    <rect x="70" y="270" width="160" height="30" rx="5" fill="#90A4AE"/>
    <text x="150" y="290" font-size="11" text-anchor="middle" fill="white">Stage 1: Foundation</text>
    
    <rect x="70" y="310" width="160" height="30" rx="5" fill="#78909C"/>
    <text x="150" y="330" font-size="11" text-anchor="middle" fill="white">Stage 2: Interleaved</text>
  </g>
  
  <!-- Data Creation Pipeline -->
  <g transform="translate(0, 350)">
    <rect x="50" y="230" width="200" height="100" rx="15" fill="#795548" stroke="#5D4037" stroke-width="2"/>
    <text x="150" y="255" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Data Creation</text>
    <text x="150" y="275" font-size="11" text-anchor="middle" fill="white">‚Ä¢ LLM Generation</text>
    <text x="150" y="295" font-size="11" text-anchor="middle" fill="white">‚Ä¢ Multi-dim Filtering</text>
    <text x="150" y="315" font-size="11" text-anchor="middle" fill="white">‚Ä¢ Agent Pipeline</text>
  </g>
  
  <!-- Arrows -->
  <path d="M 250 90 L 320 150" stroke="#333" stroke-width="3" fill="none"/>
  <path d="M 620 160 L 700 160" stroke="#333" stroke-width="3" fill="none"/>
  <path d="M 800 200 L 800 260 L 620 340" stroke="#333" stroke-width="3" fill="none"/>
  <path d="M 620 380 L 700 380" stroke="#333" stroke-width="3" fill="none"/>
  
  <!-- Bidirectional arrow between T2I and Editing -->
  <g transform="translate(650, 500)">
    <rect x="-50" y="-30" width="100" height="60" rx="10" fill="#E91E63" stroke="#AD1457" stroke-width="2"/>
    <text x="0" y="-5" font-size="12" font-weight="bold" text-anchor="middle" fill="white">T2I Generation</text>
    <text x="0" y="10" font-size="12" font-weight="bold" text-anchor="middle" fill="white">‚ü∑</text>
    <text x="0" y="25" font-size="12" font-weight="bold" text-anchor="middle" fill="white">Image Editing</text>
  </g>
  
  <!-- Key Insight -->
  <rect x="300" y="520" width="400" height="60" rx="10" fill="#9E9E9E" stroke="#616161" stroke-width="2"/>
  <text x="500" y="545" font-size="14" font-weight="bold" text-anchor="middle" fill="white">Key Insight</text>
  <text x="500" y="565" font-size="12" text-anchor="middle" fill="white">Unified framework: Generation & Editing share reasoning patterns</text>
  
  <!-- Model Architecture Note -->
  <rect x="50" y="620" width="900" height="40" rx="10" fill="#37474F" stroke="#263238" stroke-width="2"/>
  <text x="500" y="645" font-size="12" text-anchor="middle" fill="white">Architecture: BAGEL-based Mixture-of-Transformers with ViT encoder for unified multimodal processing</text>
  
  <!-- Loss Function -->
  <rect x="350" y="680" width="300" height="50" rx="10" fill="#546E7A" stroke="#37474F" stroke-width="2"/>
  <text x="500" y="700" font-size="12" text-anchor="middle" fill="white">Loss: L = Œª_text * L_text + Œª_img * L_img</text>
  <text x="500" y="720" font-size="11" text-anchor="middle" fill="white">(Œª_text = 2, Œª_img = 1)</text>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Refinement in T2I generation and image editing share the same reasoning pattern, enabling bidirectional capability transfer">
                        <div class="quiz-question">1. What key insight enables UniReason to unify text-to-image generation and image editing tasks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Both tasks require identical neural network architectures for processing visual features">Both tasks require identical neural network architectures for processing visual features</div><div class="quiz-choice long-text" data-value="Refinement in T2I generation and image editing share the same reasoning pattern, enabling bidirectional capability transfer">Refinement in T2I generation and image editing share the same reasoning pattern, enabling bidirectional capability transfer</div><div class="quiz-choice" data-value="Text-to-image generation is simply a special case of image editing with blank canvas input">Text-to-image generation is simply a special case of image editing with blank canvas input</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Cultural commonsense, natural science, spatial reasoning, temporal reasoning, and logical reasoning">
                        <div class="quiz-question">2. Which five major knowledge domains does UniReason's training data cover for world knowledge-enhanced reasoning?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Cultural commonsense, natural science, spatial reasoning, temporal reasoning, and logical reasoning">Cultural commonsense, natural science, spatial reasoning, temporal reasoning, and logical reasoning</div><div class="quiz-choice" data-value="Mathematics, linguistics, computer science, art history, and social psychology">Mathematics, linguistics, computer science, art history, and social psychology</div><div class="quiz-choice long-text" data-value="Visual perception, semantic understanding, geometric transformation, color theory, and style transfer">Visual perception, semantic understanding, geometric transformation, color theory, and style transfer</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Performance gains from refinement increase monotonically with higher editing proficiency">
                        <div class="quiz-question">3. What correlation did the authors discover between image editing capability and refinement effectiveness?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="There is no significant correlation between editing proficiency and refinement gains">There is no significant correlation between editing proficiency and refinement gains</div><div class="quiz-choice" data-value="Higher editing proficiency leads to diminishing returns in refinement effectiveness">Higher editing proficiency leads to diminishing returns in refinement effectiveness</div><div class="quiz-choice" data-value="Performance gains from refinement increase monotonically with higher editing proficiency">Performance gains from refinement increase monotonically with higher editing proficiency</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            

    <!-- Personal Takeaways Section -->
    <div id="takeaways-container"></div>

    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

    <!-- MathJax for LaTeX rendering (only for takeaways section) -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            },
            startup: {
                pageReady: () => {
                    // Disable automatic processing - we'll only process takeaways manually
                    return Promise.resolve();
                }
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Load and render markdown takeaways
            const dateMatch = document.querySelector('h1').textContent.match(/(\d{4}-\d{2}-\d{2})/);
            if (dateMatch) {
                const date = dateMatch[1];
                const mdPath = `../notes/${date}.md`;

                // Use XMLHttpRequest for better file:// protocol support
                const xhr = new XMLHttpRequest();
                xhr.onreadystatechange = function() {
                    if (xhr.readyState === 4) {
                        console.log('XHR Status:', xhr.status, 'Response length:', xhr.responseText.length);

                        if (xhr.status === 200 || xhr.status === 0) {  // status 0 for file://
                            const markdown = xhr.responseText;

                            if (!markdown || markdown.trim().length === 0) {
                                console.log('Empty markdown file');
                                return;
                            }

                            console.log('Markdown loaded, length:', markdown.length);

                            // Check if marked is loaded
                            if (typeof marked === 'undefined') {
                                console.error('marked.js library not loaded');
                                return;
                            }

                            // Convert markdown to HTML
                            const htmlContent = marked.parse(markdown);
                            console.log('HTML converted, length:', htmlContent.length);

                            // Fix image paths
                            const fixedContent = htmlContent.replace(
                                /src="(?!http:\/\/|https:\/\/|\/|\.\.\/)([^"]+)"/g,
                                `src="../images/${date}/$1"`
                            );

                            // Wrap in styled divs
                            const wrappedHtml = `
                                <div class="takeaways-section">
                                    <h2>üìù My Takeaways</h2>
                                    <div class="takeaways-content">
                                        ${fixedContent}
                                    </div>
                                </div>
                            `;

                            document.getElementById('takeaways-container').innerHTML = wrappedHtml;
                            console.log('Takeaways section rendered');

                            // Trigger MathJax to render LaTeX equations
                            if (typeof MathJax !== 'undefined') {
                                MathJax.typesetPromise([document.getElementById('takeaways-container')])
                                    .then(() => {
                                        console.log('MathJax rendering complete');
                                    })
                                    .catch((err) => console.error('MathJax rendering error:', err));
                            }
                        } else {
                            console.log('XHR failed - Status:', xhr.status);
                        }
                    }
                };
                xhr.open('GET', mdPath, true);
                console.log('Loading markdown from:', mdPath);
                xhr.send();
            }

            // ÂàõÂª∫ÈÅÆÁΩ©Â±Ç
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // Ëé∑ÂèñÊâÄÊúâÈóÆÈ¢òÊ†áÁ≠æ
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // ËÆæÁΩÆÁÇπÂáª‰∫ã‰ª∂Â§ÑÁêÜ
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // ÁÇπÂáªÊ†áÁ≠æÂàáÊç¢ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫Áä∂ÊÄÅ
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // ÈòªÊ≠¢‰∫ã‰ª∂ÂÜíÊ≥°
                    
                    // Â¶ÇÊûúÂΩìÂâçÈóÆÈ¢òÂç°Â∑≤ÁªèÊòæÁ§∫ÔºåÂàôÈöêËóèÂÆÉ
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // ÂÖàÈöêËóèÊâÄÊúâÂÖ∂‰ªñÈóÆÈ¢òÂç°
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // Â∞ÜÂºπÁ™óÂÜÖÂÆπÂ§çÂà∂Âà∞È°µÈù¢ÊúÄÂ§ñÂ±ÇÁöÑÂºπÁ™ó‰∏≠
                        document.body.appendChild(popup);
                        
                        // ÊòæÁ§∫ÂΩìÂâçÈóÆÈ¢òÂç°ÂíåËÉåÊôØÈÅÆÁΩ©
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // Á°Æ‰øùÁÇπÂáªÈóÆÈ¢òÂç°ÂÜÖÈÉ®Êó∂‰∏ç‰ºöÂÖ≥Èó≠ÈóÆÈ¢òÂç°
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // ÁÇπÂáªÈÅÆÁΩ©Â±ÇÊàñÈ°µÈù¢‰ªª‰ΩïÂÖ∂‰ªñ‰ΩçÁΩÆÊó∂ÈöêËóèÊâÄÊúâÈóÆÈ¢òÂç°
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // ‰∏∫ÊØè‰∏™ÈÄâÈ°πÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // ÈáçÁΩÆÊâÄÊúâÈÄâÈ°π
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // Ê†áËÆ∞ÂΩìÂâçÈÄâÈ°π‰∏∫Â∑≤ÈÄâ
                    this.classList.add('selected');
                    
                    // Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '‚úîÔ∏è CorrectÔºÅ';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '‚ùå WrongÔºÅ';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // Âç°ÁâáËΩÆÊí≠ÂäüËÉΩ - Êñ∞Â¢û
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // Êõ¥Êñ∞ËÆ°Êï∞Âô®ÊòæÁ§∫
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // ÊòæÁ§∫ÊåáÂÆöÁ¥¢ÂºïÁöÑÂç°Áâá
                function showCard(index) {
                    // Â§ÑÁêÜÂæ™ÁéØ
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // Êõ¥Êñ∞ÂΩìÂâçÁ¥¢Âºï
                    currentIndex = index;
                    
                    // Êõ¥Êñ∞Âç°ÁâáÊòæÁ§∫
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // Êõ¥Êñ∞ËÆ°Êï∞Âô®
                    updateCounter();
                }
                
                // ‰∏ã‰∏ÄÂº†Âç°Áâá
                function nextCard(e) {
                    e.stopPropagation(); // Èò≤Ê≠¢‰∫ã‰ª∂ÂÜíÊ≥°ÂØºËá¥ÈóÆÈ¢òÂç°ÂÖ≥Èó≠
                    showCard(currentIndex + 1);
                }
                
                // ‰∏∫ÊØè‰∏™Âç°ÁâáÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂ÔºàËÄå‰∏çÊòØÊï¥‰∏™ÂÆπÂô®Ôºâ
                cards.forEach(card => {
                    card.addEventListener('click', function(e) {
                        // Âè™ÊúâÁÇπÂáªÂú®Âç°ÁâáÂÜÖÈÉ®Êó∂ÊâçÂàáÊç¢
                        // Ê£ÄÊü•ÊòØÂê¶ÊòØÊµÅÁ®ãÂõæÂç°ÁâáÁöÑÊªöÂä®Êù°Âå∫Âüü
                        if (this.classList.contains('flowchart-card')) {
                            const rect = this.getBoundingClientRect();
                            const isScrollbarClick =
                                (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                                (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);

                            if (!isScrollbarClick) {
                                nextCard(e);
                            }
                        } else {
                            nextCard(e);
                        }
                    });
                });
                
                // ÈîÆÁõòÂØºËà™
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>

    <!-- AI Assistant Scripts - Load in correct order (relative paths for subpages) -->
    <script src="../../js/ai-assistant-constants.js"></script>
    <script src="../../js/ai-assistant-storage.js"></script>
    <script src="../../js/ai-assistant-positioning.js"></script>
    <script src="../../js/ai-assistant-templates.js"></script>
    <script src="../../js/ai-assistant-dom-utils.js"></script>
    <script src="../../js/ai-assistant-config.js"></script>
    <script src="../../js/ai-assistant.js"></script>
</body>
</html>
