
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-09-12 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* Âç°ÁâáÂÆπÂô®Ê†∑Âºè - Êñ∞Â¢û */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            height: 600px; /* Âõ∫ÂÆöÈ´òÂ∫¶ */
            cursor: pointer; /* Â¢ûÂä†ÊåáÈíàÊ†∑ÂºèÊèêÁ§∫ÂèØÁÇπÂáª */
        }
        
        /* Âç°ÁâáÈÄöÁî®Ê†∑Âºè */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
        }
        
        /* ËΩÆÊí≠Âç°ÁâáÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%;
            transition: transform 0.5s ease, opacity 0.5s ease;
        }
        
        /* ÈùûÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* ÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* Á¨¨‰∏ÄÂº†Âç°ÁâáÔºàÊñáÊú¨ÂÜÖÂÆπÔºâ‰∏çÈúÄË¶ÅÊªöÂä® */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* Á¨¨‰∫åÂº†Âç°ÁâáÔºàÊµÅÁ®ãÂõæÔºâÊîØÊåÅÊªöÂä® */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow: auto !important;
            padding-bottom: 50px; /* Ê∑ªÂä†Â∫ïÈÉ®Â°´ÂÖÖ */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-height: none; /* ÁßªÈô§‰ªª‰ΩïÈ´òÂ∫¶ÈôêÂà∂ */
        }
        
        /* ‰º†ÁªüÂç°ÁâáÊ†∑Âºè */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        
        .paper-card p {
            margin: 5px 0;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* Âç°ÁâáËÆ°Êï∞Âô® - Êñ∞Â¢û */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* Êîπ‰∏∫Âõ∫ÂÆöÂÆö‰ΩçÔºå‰∏çÈöèÊªöÂä®ËÄåÁßªÂä® */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* Â±Ö‰∏≠ÊòæÁ§∫ */
            width: 90%;
            max-width: 500px; /* Â¢ûÂä†ÊúÄÂ§ßÂÆΩÂ∫¶ÔºåÈÄÇÂ∫îÈïøÂÜÖÂÆπ */
            max-height: 80vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶ */
            overflow-y: auto; /* ÂÜÖÂÆπËøáÂ§öÊó∂ÂèØÊªöÂä® */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* Á°Æ‰øùÊòæÁ§∫Âú®ÊúÄ‰∏äÂ±Ç */
        }
        
        /* Ê∑ªÂä†ÈÅÆÁΩ©Â±ÇÔºåÈò≤Ê≠¢ÈóÆÈ¢òÂç°Ë¢´ÂÖ∂‰ªñÂÜÖÂÆπÈÅÆÊå° */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* ‰ΩøÁî®JavaScriptÊéßÂà∂ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫ÂíåÈöêËóèÔºå‰∏çÂÜç‰ΩøÁî®hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            hyphens: auto; /* Âú®ÂøÖË¶ÅÊó∂‰ΩøÁî®ËøûÂ≠óÁ¨¶ */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* ÈïøÊñáÊú¨Â∑¶ÂØπÈΩê */
            display: block; /* Á°Æ‰øùÊòØÂùóÁ∫ßÂÖÉÁ¥† */
            white-space: normal; /* ÂÖÅËÆ∏Ëá™Âä®Êç¢Ë°å */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* ÈïøÊñáÊú¨ÈÄâÈ°πÁöÑÁâπÊÆäÊ†∑Âºè */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* Á°Æ‰øùÂºπÁ™ó‰∏≠ÁöÑÊåâÈíÆÊñáÊú¨‰∏ç‰ºöÊ∫¢Âá∫ */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* ÈÄÇÂ∫îË∂ÖÈïøÈÄâÈ°πÊñáÊú¨ */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        @media (max-width: 768px) {
            .paper-container {
                flex-direction: column;
            }
            
            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                height: 650px; /* ÁßªÂä®ËÆæÂ§á‰∏äÈ´òÂ∫¶Ë∞ÉÊï¥ */
            }
            
            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }
            
            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }
            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
            }
        }
    
        /* Personal Takeaways Section Styles */
        .takeaways-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .takeaways-section h2 {
            color: #ffffff;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid rgba(255, 255, 255, 0.3);
            padding-bottom: 15px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        .takeaways-content {
            background-color: rgba(255, 255, 255, 0.95);
            border-radius: 8px;
            padding: 25px;
            line-height: 1.8;
            color: #333;
        }

        .takeaways-content h3 {
            color: #667eea;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .takeaways-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .takeaways-content p {
            margin: 15px 0;
        }

        .takeaways-content ul, .takeaways-content ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        .takeaways-content li {
            margin: 8px 0;
        }

        .takeaways-content blockquote {
            border-left: 4px solid #667eea;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }

        .takeaways-content pre {
            background-color: #f6f8fa;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            border: 1px solid #e1e4e8;
        }

        .takeaways-content code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d73a49;
        }

        .takeaways-content pre code {
            background-color: transparent;
            padding: 0;
            color: #333;
        }
    
    </style>
</head>
<body>
    <h1>2025-09-12 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/shattered-dark.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action
  Model</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-09-11</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2509.09372" target="_blank">http://arxiv.org/pdf/2509.09372</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> Vision-Language-Action (VLA) modeling for robotic control, focusing on bridging visual-language perception with action generation.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on previous VLA models that rely on large pre-trained vision-language models and extensive robotic data pre-training; proposes a novel lightweight "VLA-Adapter" paradigm that reduces reliance on large models.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> Current VLA models face bottlenecks including dependence on large-scale vision-language models, slow fine-tuning, high GPU memory usage, and low inference efficiency.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> Introduces a Policy module with Bridge Attention that uses both Raw features and ActionQuery features from all layers of a small backbone model to effectively bridge perception and action spaces, without requiring robotic pre-training.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Achieves state-of-the-art performance (97.3% success rate) using only a 0.5B parameter backbone (vs 7B for previous methods), with 3x faster inference speed, 1/38x training cost, and successful real-world robot deployment, evaluated on LIBERO and CALVIN benchmarks.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action
  Model</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="18" font-weight="bold" fill="#2c3e50">VLA-Adapter: Vision-Language-Action Model Workflow</text>
  
  <!-- Input Stage -->
  <rect x="50" y="70" width="200" height="80" rx="10" fill="#e3f2fd" stroke="#1976d2" stroke-width="2"/>
  <text x="150" y="95" text-anchor="middle" font-size="12" font-weight="bold" fill="#1976d2">Input Data</text>
  <text x="150" y="110" text-anchor="middle" font-size="10" fill="#1976d2">3rd-view Image (X_v)</text>
  <text x="150" y="125" text-anchor="middle" font-size="10" fill="#1976d2">Gripper Image (X_g)</text>
  <text x="150" y="140" text-anchor="middle" font-size="10" fill="#1976d2">Instruction (L_t)</text>
  
  <!-- Vision Feature Extraction -->
  <rect x="300" y="70" width="180" height="80" rx="10" fill="#fff3e0" stroke="#f57c00" stroke-width="2"/>
  <text x="390" y="95" text-anchor="middle" font-size="12" font-weight="bold" fill="#f57c00">Vision Feature Extraction</text>
  <text x="390" y="110" text-anchor="middle" font-size="10" fill="#f57c00">DINOv2 + SigLIP</text>
  <text x="390" y="125" text-anchor="middle" font-size="10" fill="#f57c00">Vision Embeddings</text>
  
  <!-- VLM Processing -->
  <rect x="150" y="200" width="250" height="100" rx="10" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="2"/>
  <text x="275" y="220" text-anchor="middle" font-size="12" font-weight="bold" fill="#7b1fa2">VLM (Qwen2.5-0.5B)</text>
  <text x="275" y="240" text-anchor="middle" font-size="10" fill="#7b1fa2">M layers processing</text>
  <text x="275" y="255" text-anchor="middle" font-size="10" fill="#7b1fa2">Prismatic-VLMs architecture</text>
  <text x="275" y="270" text-anchor="middle" font-size="10" fill="#7b1fa2">Raw latent (C_R) extraction</text>
  <text x="275" y="285" text-anchor="middle" font-size="10" fill="#7b1fa2">ActionQuery latent (C_AQ) extraction</text>
  
  <!-- ActionQuery -->
  <rect x="450" y="200" width="150" height="100" rx="10" fill="#e8f5e8" stroke="#388e3c" stroke-width="2"/>
  <text x="525" y="220" text-anchor="middle" font-size="12" font-weight="bold" fill="#388e3c">ActionQuery</text>
  <text x="525" y="240" text-anchor="middle" font-size="10" fill="#388e3c">64 learnable tokens</text>
  <text x="525" y="255" text-anchor="middle" font-size="10" fill="#388e3c">Multimodal aggregation</text>
  <text x="525" y="270" text-anchor="middle" font-size="10" fill="#388e3c">Deep-layer performs best</text>
  <text x="525" y="285" text-anchor="middle" font-size="10" fill="#388e3c">All-layer features used</text>
  
  <!-- Condition Analysis -->
  <rect x="50" y="350" width="300" height="80" rx="10" fill="#fff8e1" stroke="#ffa000" stroke-width="2"/>
  <text x="200" y="375" text-anchor="middle" font-size="12" font-weight="bold" fill="#ffa000">Condition Analysis</text>
  <text x="200" y="390" text-anchor="middle" font-size="10" fill="#ffa000">Key Finding 1: Middle-layer Raw features better</text>
  <text x="200" y="405" text-anchor="middle" font-size="10" fill="#ffa000">Key Finding 2: Deep-layer ActionQuery better</text>
  <text x="200" y="420" text-anchor="middle" font-size="10" fill="#ffa000">Key Finding 3: Multi-layer features optimal</text>
  
  <!-- Policy Network -->
  <rect x="400" y="350" width="250" height="120" rx="10" fill="#ffebee" stroke="#d32f2f" stroke-width="2"/>
  <text x="525" y="375" text-anchor="middle" font-size="12" font-weight="bold" fill="#d32f2f">Policy Network (97M params)</text>
  <text x="525" y="395" text-anchor="middle" font-size="10" fill="#d32f2f">L1-based architecture</text>
  <text x="525" y="410" text-anchor="middle" font-size="10" fill="#d32f2f">M layers (same as VLM)</text>
  <text x="525" y="425" text-anchor="middle" font-size="10" fill="#d32f2f">Bridge Attention modules</text>
  <text x="525" y="440" text-anchor="middle" font-size="10" fill="#d32f2f">FFN layers</text>
  <text x="525" y="455" text-anchor="middle" font-size="10" fill="#d32f2f">Trained from scratch</text>
  
  <!-- Bridge Attention Detail -->
  <rect x="700" y="200" width="250" height="150" rx="10" fill="#e1f5fe" stroke="#0277bd" stroke-width="2"/>
  <text x="825" y="220" text-anchor="middle" font-size="12" font-weight="bold" fill="#0277bd">Bridge Attention</text>
  <text x="825" y="240" text-anchor="middle" font-size="10" fill="#0277bd">Cross Attention 1: C_R features</text>
  <text x="825" y="255" text-anchor="middle" font-size="10" fill="#0277bd">Cross Attention 2: C_AQ + Proprio</text>
  <text x="825" y="270" text-anchor="middle" font-size="10" fill="#0277bd">Self Attention: Action latent</text>
  <text x="825" y="285" text-anchor="middle" font-size="10" fill="#0277bd">Learnable Ratio parameter</text>
  <text x="825" y="300" text-anchor="middle" font-size="10" fill="#0277bd">tanh activation for stability</text>
  <text x="825" y="315" text-anchor="middle" font-size="10" fill="#0277bd">Selective injection of C_R</text>
  <text x="825" y="330" text-anchor="middle" font-size="10" fill="#0277bd">Full injection of C_AQ</text>
  
  <!-- Training -->
  <rect x="100" y="520" width="200" height="80" rx="10" fill="#f1f8e9" stroke="#689f38" stroke-width="2"/>
  <text x="200" y="545" text-anchor="middle" font-size="12" font-weight="bold" fill="#689f38">Training</text>
  <text x="200" y="560" text-anchor="middle" font-size="10" fill="#689f38">End-to-end training</text>
  <text x="200" y="575" text-anchor="middle" font-size="10" fill="#689f38">L1 loss objective</text>
  <text x="200" y="590" text-anchor="middle" font-size="10" fill="#689f38">8 hours on single GPU</text>
  
  <!-- Output -->
  <rect x="400" y="520" width="200" height="80" rx="10" fill="#fce4ec" stroke="#c2185b" stroke-width="2"/>
  <text x="500" y="545" text-anchor="middle" font-size="12" font-weight="bold" fill="#c2185b">Action Output</text>
  <text x="500" y="560" text-anchor="middle" font-size="10" fill="#c2185b">H-step action chunk</text>
  <text x="500" y="575" text-anchor="middle" font-size="10" fill="#c2185b">7D continuous actions</text>
  <text x="500" y="590" text-anchor="middle" font-size="10" fill="#c2185b">219.2Hz inference speed</text>
  
  <!-- Performance Metrics -->
  <rect x="700" y="400" width="250" height="120" rx="10" fill="#f9fbe7" stroke="#827717" stroke-width="2"/>
  <text x="825" y="425" text-anchor="middle" font-size="12" font-weight="bold" fill="#827717">Performance Achievements</text>
  <text x="825" y="445" text-anchor="middle" font-size="10" fill="#827717">LIBERO: 97.3% success rate</text>
  <text x="825" y="460" text-anchor="middle" font-size="10" fill="#827717">CALVIN: 4.42 avg length</text>
  <text x="825" y="475" text-anchor="middle" font-size="10" fill="#827717">1/14√ó backbone size vs SOTA</text>
  <text x="825" y="490" text-anchor="middle" font-size="10" fill="#827717">1/38√ó training cost</text>
  <text x="825" y="505" text-anchor="middle" font-size="10" fill="#827717">3√ó faster inference</text>
  
  <!-- Key Innovation Box -->
  <rect x="700" y="550" width="250" height="100" rx="10" fill="#fff" stroke="#ff5722" stroke-width="3" stroke-dasharray="5,5"/>
  <text x="825" y="575" text-anchor="middle" font-size="12" font-weight="bold" fill="#ff5722">Key Innovation</text>
  <text x="825" y="595" text-anchor="middle" font-size="10" fill="#ff5722">Effective VL‚ÜíA bridging</text>
  <text x="825" y="610" text-anchor="middle" font-size="10" fill="#ff5722">Tiny-scale backbone (0.5B)</text>
  <text x="825" y="625" text-anchor="middle" font-size="10" fill="#ff5722">No robotic pre-training needed</text>
  <text x="825" y="640" text-anchor="middle" font-size="10" fill="#ff5722">Autonomous condition injection</text>
  
  <!-- Flow connections with colored lines -->
  <line x1="250" y1="110" x2="300" y2="110" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="480" y1="110" x2="525" y2="150" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="275" y1="170" x2="275" y2="200" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="525" y1="200" x2="525" y2="170" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="400" y1="250" x2="400" y2="350" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="600" y1="250" x2="700" y2="275" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="350" y1="390" x2="400" y2="390" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="525" y1="470" x2="525" y2="520" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="300" y1="560" x2="400" y2="560" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="650" y1="410" x2="700" y2="450" stroke="#666" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#666"/>
    </marker>
  </defs>
  
  <!-- Bottom summary -->
  <text x="500" y="750" text-anchor="middle" font-size="14" font-weight="bold" fill="#34495e">VLA-Adapter: Efficient VL‚ÜíA bridging with tiny-scale backbone achieving SOTA performance</text>
  <text x="500" y="770" text-anchor="middle" font-size="12" fill="#7f8c8d">Lower training costs, faster inference, maintained performance quality</text>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="It eliminates the need for robotic pre-training while using a smaller backbone">
                        <div class="quiz-question">1. What is the main innovation of VLA-Adapter compared to previous VLA models?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It uses a much larger vision-language model">It uses a much larger vision-language model</div><div class="quiz-choice" data-value="It eliminates the need for robotic pre-training while using a smaller backbone">It eliminates the need for robotic pre-training while using a smaller backbone</div><div class="quiz-choice" data-value="It focuses only on action generation without visual inputs">It focuses only on action generation without visual inputs</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Bridge Attention with both Raw and ActionQuery features">
                        <div class="quiz-question">2. What key component does VLA-Adapter use to bridge perception and action spaces?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Bridge Attention with both Raw and ActionQuery features">Bridge Attention with both Raw and ActionQuery features</div><div class="quiz-choice" data-value="Only last-layer features from the vision model">Only last-layer features from the vision model</div><div class="quiz-choice" data-value="Random feature selection from different layers">Random feature selection from different layers</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="It can be trained in just 8 hours on a single consumer GPU">
                        <div class="quiz-question">3. What is the most significant practical advantage of VLA-Adapter?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It achieves 100% accuracy on all tasks">It achieves 100% accuracy on all tasks</div><div class="quiz-choice" data-value="It can only work in simulation environments">It can only work in simulation environments</div><div class="quiz-choice" data-value="It can be trained in just 8 hours on a single consumer GPU">It can be trained in just 8 hours on a single consumer GPU</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/woven.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-09-11</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2509.09674" target="_blank">http://arxiv.org/pdf/2509.09674</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on developing SimpleVLA-RL, an efficient reinforcement learning framework for Vision-Language-Action (VLA) models in robotic manipulation tasks.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on veRL (Volcano Engine Reinforcement Learning for LLMs), the paper proposes new VLA-specific trajectory sampling, parallel rendering, and optimized loss computation for robotic applications.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper addresses two key challenges in VLA models: the scarcity of large-scale human-operated robotic trajectories required for training, and limited generalization to tasks involving distribution shift.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The paper implements an end-to-end online RL framework with dynamic sampling, higher rollout temperature, and modified clipping range, using binary outcome rewards (1 for success, 0 for failure) for training.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> The framework achieved state-of-the-art performance on LIBERO and RoboTwin benchmarks, improving success rates by 10-15%, demonstrating strong generalization capabilities, and effectively transferring from simulation to real-world tasks.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <defs>
    <linearGradient id="bgGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f8f9fa;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#e9ecef;stop-opacity:1" />
    </linearGradient>
    <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
      <feDropShadow dx="3" dy="3" stdDeviation="3" flood-color="#00000020"/>
    </filter>
  </defs>
  
  <rect width="1000" height="800" fill="url(#bgGrad)"/>
  
  <!-- Title -->
  <text x="500" y="40" text-anchor="middle" font-family="Arial, sans-serif" font-size="24" font-weight="bold" fill="#2c3e50">
    SimpleVLA-RL: Vision-Language-Action Model Training via Reinforcement Learning
  </text>
  
  <!-- Input Stage -->
  <rect x="50" y="80" width="180" height="60" rx="10" fill="#3498db" filter="url(#shadow)"/>
  <text x="140" y="105" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Input Query
  </text>
  <text x="140" y="120" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    (Visual + Language)
  </text>
  
  <!-- Interactive VLA Rollout -->
  <rect x="50" y="180" width="180" height="100" rx="10" fill="#e74c3c" filter="url(#shadow)"/>
  <text x="140" y="205" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Interactive VLA Rollout
  </text>
  <text x="140" y="220" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Token-based sampling
  </text>
  <text x="140" y="235" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Environment interaction
  </text>
  <text x="140" y="250" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Temperature = 1.6
  </text>
  <text x="140" y="265" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Dynamic sampling
  </text>
  
  <!-- Multiple Trajectories -->
  <g transform="translate(320, 180)">
    <rect x="0" y="0" width="120" height="25" rx="5" fill="#9b59b6" filter="url(#shadow)"/>
    <text x="60" y="17" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" font-weight="bold" fill="white">
      Trajectory œÑ‚ÇÅ
    </text>
    
    <rect x="0" y="35" width="120" height="25" rx="5" fill="#9b59b6" filter="url(#shadow)"/>
    <text x="60" y="52" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" font-weight="bold" fill="white">
      Trajectory œÑ‚ÇÇ
    </text>
    
    <text x="60" y="75" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="#7f8c8d">
      ...
    </text>
    
    <rect x="0" y="80" width="120" height="25" rx="5" fill="#9b59b6" filter="url(#shadow)"/>
    <text x="60" y="97" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" font-weight="bold" fill="white">
      Trajectory œÑG
    </text>
  </g>
  
  <!-- Environment Feedback -->
  <rect x="520" y="180" width="160" height="80" rx="10" fill="#27ae60" filter="url(#shadow)"/>
  <text x="600" y="205" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Environment
  </text>
  <text x="600" y="220" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    Parallel rendering
  </text>
  <text x="600" y="235" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    State transitions
  </text>
  <text x="600" y="250" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    Multi-environment
  </text>
  
  <!-- Outcome Reward Modeling -->
  <rect x="750" y="180" width="160" height="80" rx="10" fill="#f39c12" filter="url(#shadow)"/>
  <text x="830" y="205" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Outcome Rewards
  </text>
  <text x="830" y="220" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    R = 1 if success
  </text>
  <text x="830" y="235" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    R = 0 if failure
  </text>
  <text x="830" y="250" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    Binary feedback
  </text>
  
  <!-- Exploration Enhancements -->
  <rect x="50" y="330" width="300" height="100" rx="10" fill="#8e44ad" filter="url(#shadow)"/>
  <text x="200" y="355" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Exploration Enhancements
  </text>
  <text x="80" y="375" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Dynamic Sampling: Exclude uniform reward groups
  </text>
  <text x="80" y="390" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Clip Higher: [0.8, 1.28] vs [0.8, 1.2]
  </text>
  <text x="80" y="405" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Higher Temperature: 1.6 vs 1.0
  </text>
  <text x="80" y="420" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Remove KL regularization
  </text>
  
  <!-- GRPO Training -->
  <rect x="420" y="330" width="200" height="100" rx="10" fill="#34495e" filter="url(#shadow)"/>
  <text x="520" y="355" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    GRPO Training
  </text>
  <text x="520" y="375" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    Group advantage:
  </text>
  <text x="520" y="390" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    √Ç = (R - mean(R)) / std(R)
  </text>
  <text x="520" y="405" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    PPO-style clipping
  </text>
  <text x="520" y="420" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    Policy optimization
  </text>
  
  <!-- Results -->
  <rect x="680" y="330" width="250" height="100" rx="10" fill="#16a085" filter="url(#shadow)"/>
  <text x="805" y="355" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Key Achievements
  </text>
  <text x="690" y="375" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ LIBERO: 91.0% ‚Üí 99.1% success rate
  </text>
  <text x="690" y="390" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Data efficiency: 1 demo ‚Üí 96.9% performance
  </text>
  <text x="690" y="405" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Generalization across spatial/object/goal
  </text>
  <text x="690" y="420" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Real-world sim2real transfer
  </text>
  
  <!-- Novel Phenomena -->
  <rect x="50" y="480" width="400" height="80" rx="10" fill="#e67e22" filter="url(#shadow)"/>
  <text x="250" y="505" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    "Pushcut" Phenomenon Discovery
  </text>
  <text x="60" y="525" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ RL discovers novel pushing strategies beyond demonstration data
  </text>
  <text x="60" y="540" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Move-can-pot: Push instead of grasp-move-place
  </text>
  <text x="60" y="555" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Emergent efficient behaviors through exploration
  </text>
  
  <!-- Framework Integration -->
  <rect x="520" y="480" width="380" height="80" rx="10" fill="#2980b9" filter="url(#shadow)"/>
  <text x="710" y="505" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    veRL Framework Integration
  </text>
  <text x="530" y="525" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Extended veRL for VLA-specific trajectory sampling
  </text>
  <text x="530" y="540" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Scalable parallelization and distributed training
  </text>
  <text x="530" y="555" font-family="Arial, sans-serif" font-size="10" fill="white">
    ‚Ä¢ Integrated training-inference-rendering pipeline
  </text>
  
  <!-- Benchmarks -->
  <rect x="200" y="620" width="600" height="60" rx="10" fill="#c0392b" filter="url(#shadow)"/>
  <text x="500" y="645" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
    Evaluation: LIBERO, RoboTwin 1.0 &amp; 2.0, Real-world Tasks
  </text>
  <text x="500" y="665" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
    State-of-the-art performance across simulation and real-world benchmarks
  </text>
  
  <!-- Flow connections with curved lines -->
  <path d="M 140 140 Q 140 160 140 180" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
  <path d="M 230 230 Q 280 230 320 230" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 440 230 Q 480 230 520 230" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 680 230 Q 720 230 750 230" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 200 280 Q 200 305 200 330" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 520 280 Q 520 305 520 330" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 350 380 Q 385 380 420 380" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 620 380 Q 650 380 680 380" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 250 430 Q 250 455 250 480" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 710 430 Q 710 455 710 480" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 250 560 Q 350 590 500 620" stroke="#2c3e50" stroke-width="2" fill="none"/>
  <path d="M 710 560 Q 610 590 500 620" stroke="#2c3e50" stroke-width="2" fill="none"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#2c3e50"/>
    </marker>
  </defs>
  
  <!-- Method labels -->
  <text x="20" y="750" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="#2c3e50">
    Key Innovation: Extending LLM RL techniques to VLA domain with outcome-based rewards
  </text>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="The 'pushcut' phenomenon where models learned to push objects instead of grasp-move-place">
                        <div class="quiz-question">1. What novel phenomenon did researchers observe during RL training that was not present in supervised data?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="The 'pushcut' phenomenon where models learned to push objects instead of grasp-move-place">The 'pushcut' phenomenon where models learned to push objects instead of grasp-move-place</div><div class="quiz-choice" data-value="The 'speedup' phenomenon where models executed tasks faster than demonstrations">The 'speedup' phenomenon where models executed tasks faster than demonstrations</div><div class="quiz-choice" data-value="The 'multipath' phenomenon where models found multiple solutions for the same task">The 'multipath' phenomenon where models found multiple solutions for the same task</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Improved from 17.3% to 91.7% success rate">
                        <div class="quiz-question">2. In the data scarcity experiment with One-Trajectory SFT, what remarkable improvement did SimpleVLA-RL achieve on LIBERO-Long tasks?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Improved from 17.3% to 48.9% success rate">Improved from 17.3% to 48.9% success rate</div><div class="quiz-choice" data-value="Improved from 17.3% to 91.7% success rate">Improved from 17.3% to 91.7% success rate</div><div class="quiz-choice" data-value="Improved from 48.9% to 91.7% success rate">Improved from 48.9% to 91.7% success rate</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="It uses simple binary rewards (1 for success, 0 for failure) based only on task completion">
                        <div class="quiz-question">3. What unique approach does SimpleVLA-RL take regarding reward design compared to traditional robotic RL?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It uses dense rewards based on distance to goal">It uses dense rewards based on distance to goal</div><div class="quiz-choice" data-value="It combines multiple weighted reward components">It combines multiple weighted reward components</div><div class="quiz-choice" data-value="It uses simple binary rewards (1 for success, 0 for failure) based only on task completion">It uses simple binary rewards (1 for success, 0 for failure) based only on task completion</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/black-linen-2.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>Kling-Avatar: Grounding Multimodal Instructions for Cascaded
  Long-Duration Avatar Animation Synthesis</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-09-11</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2509.09595" target="_blank">http://arxiv.org/pdf/2509.09595</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on AI-driven avatar animation synthesis, specifically generating high-fidelity portrait animations from audio, image, and text inputs.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on previous video diffusion models and audio-driven avatar generation, it introduces a novel approach using multimodal language models for semantic understanding of instructions rather than just low-level signal tracking.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper addresses the challenge of generating coherent, long-duration avatar animations that maintain semantic consistency with multimodal inputs while preserving high visual quality and lip synchronization.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> Uses a two-stage cascaded framework with an MLLM Director for instruction understanding and planning, followed by parallel generation of video sub-clips with blueprint keyframes for long-duration synthesis.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Achieves superior performance in generating 1080p 48fps videos with precise lip sync, emotional expressiveness, and identity consistency, outperforming baselines OmniHuman-1 and HeyGen across multiple evaluation metrics on a 375-sample benchmark.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>Kling-Avatar: Grounding Multimodal Instructions for Cascaded
  Long-Duration Avatar Animation Synthesis</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="18" font-weight="bold" fill="#2c3e50">Kling-Avatar: Cascaded Long-Duration Avatar Animation Synthesis</text>
  
  <!-- Input Section -->
  <rect x="50" y="60" width="200" height="80" rx="10" fill="#e8f4fd" stroke="#3498db" stroke-width="2"/>
  <text x="150" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Multimodal Inputs</text>
  <text x="150" y="105" text-anchor="middle" font-size="10" fill="#34495e">Image + Audio + Text</text>
  <text x="150" y="120" text-anchor="middle" font-size="10" fill="#34495e">Prompt</text>
  
  <!-- MLLM Director -->
  <rect x="300" y="50" width="180" height="100" rx="10" fill="#ffeaa7" stroke="#fdcb6e" stroke-width="2"/>
  <text x="390" y="75" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">MLLM Director</text>
  <text x="390" y="95" text-anchor="middle" font-size="10" fill="#34495e">Instruction Grounding</text>
  <text x="390" y="110" text-anchor="middle" font-size="10" fill="#34495e">Qwen2.5-Omni/VL</text>
  <text x="390" y="125" text-anchor="middle" font-size="10" fill="#34495e">‚Üí Storyline</text>
  
  <!-- Stage 1: Blueprint Generation -->
  <rect x="520" y="50" width="200" height="100" rx="10" fill="#dda0dd" stroke="#9b59b6" stroke-width="2"/>
  <text x="620" y="75" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Stage 1: Blueprint</text>
  <text x="620" y="95" text-anchor="middle" font-size="10" fill="#34495e">Human Video DiT</text>
  <text x="620" y="110" text-anchor="middle" font-size="10" fill="#34495e">Global Semantic</text>
  <text x="620" y="125" text-anchor="middle" font-size="10" fill="#34495e">Planning</text>
  
  <!-- Blueprint Video -->
  <rect x="760" y="60" width="150" height="80" rx="10" fill="#a8e6cf" stroke="#27ae60" stroke-width="2"/>
  <text x="835" y="85" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Blueprint Video</text>
  <text x="835" y="105" text-anchor="middle" font-size="10" fill="#34495e">High-level</text>
  <text x="835" y="120" text-anchor="middle" font-size="10" fill="#34495e">Semantics</text>
  
  <!-- Keyframe Extraction -->
  <rect x="650" y="200" width="180" height="60" rx="10" fill="#fab1a0" stroke="#e17055" stroke-width="2"/>
  <text x="740" y="220" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Keyframe Extraction</text>
  <text x="740" y="240" text-anchor="middle" font-size="10" fill="#34495e">Anchor Frame Selection</text>
  <text x="740" y="255" text-anchor="middle" font-size="10" fill="#34495e">First-Last Frame Strategy</text>
  
  <!-- Stage 2: Parallel Sub-clip Generation -->
  <rect x="100" y="320" width="800" height="100" rx="10" fill="#fd79a8" stroke="#e84393" stroke-width="2"/>
  <text x="500" y="345" text-anchor="middle" font-size="14" font-weight="bold" fill="#2c3e50">Stage 2: Parallel Sub-clip Generation</text>
  
  <!-- Sub-clips -->
  <rect x="150" y="360" width="120" height="50" rx="5" fill="#74b9ff" stroke="#0984e3" stroke-width="1"/>
  <text x="210" y="380" text-anchor="middle" font-size="10" fill="#2c3e50">Sub-clip 1</text>
  <text x="210" y="395" text-anchor="middle" font-size="9" fill="#34495e">Local Details</text>
  
  <rect x="300" y="360" width="120" height="50" rx="5" fill="#74b9ff" stroke="#0984e3" stroke-width="1"/>
  <text x="360" y="380" text-anchor="middle" font-size="10" fill="#2c3e50">Sub-clip 2</text>
  <text x="360" y="395" text-anchor="middle" font-size="9" fill="#34495e">Local Details</text>
  
  <rect x="450" y="360" width="120" height="50" rx="5" fill="#74b9ff" stroke="#0984e3" stroke-width="1"/>
  <text x="510" y="380" text-anchor="middle" font-size="10" fill="#2c3e50">Sub-clip 3</text>
  <text x="510" y="395" text-anchor="middle" font-size="9" fill="#34495e">Local Details</text>
  
  <rect x="600" y="360" width="120" height="50" rx="5" fill="#74b9ff" stroke="#0984e3" stroke-width="1"/>
  <text x="660" y="380" text-anchor="middle" font-size="10" fill="#2c3e50">Sub-clip N</text>
  <text x="660" y="395" text-anchor="middle" font-size="9" fill="#34495e">Local Details</text>
  
  <text x="560" y="380" text-anchor="middle" font-size="12" fill="#2c3e50">...</text>
  
  <!-- Data Preparation Section -->
  <rect x="50" y="480" width="400" height="120" rx="10" fill="#81ecec" stroke="#00b894" stroke-width="2"/>
  <text x="250" y="505" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Data Preparation Pipeline</text>
  <text x="80" y="525" font-size="10" fill="#34495e">‚Ä¢ Lip-clarity Filtering</text>
  <text x="80" y="540" font-size="10" fill="#34495e">‚Ä¢ Temporal-continuity Detection</text>
  <text x="80" y="555" font-size="10" fill="#34495e">‚Ä¢ Audio-visual Synchronization</text>
  <text x="80" y="570" font-size="10" fill="#34495e">‚Ä¢ Aesthetic Quality Assessment</text>
  <text x="80" y="585" font-size="10" fill="#34495e">‚Ä¢ Expert Model Filtering</text>
  
  <!-- Training Strategies -->
  <rect x="500" y="480" width="400" height="120" rx="10" fill="#fdcb6e" stroke="#f39c12" stroke-width="2"/>
  <text x="700" y="505" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Training & Inference Strategies</text>
  <text x="530" y="525" font-size="10" fill="#34495e">‚Ä¢ Sliding Window Audio Injection</text>
  <text x="530" y="540" font-size="10" fill="#34495e">‚Ä¢ Mouth Region Loss Weighting</text>
  <text x="530" y="555" font-size="10" fill="#34495e">‚Ä¢ Random Padding for Robustness</text>
  <text x="530" y="570" font-size="10" fill="#34495e">‚Ä¢ Negative Frame CFG</text>
  <text x="530" y="585" font-size="10" fill="#34495e">‚Ä¢ Text Cross-attention Freezing</text>
  
  <!-- Final Output -->
  <rect x="350" y="650" width="300" height="80" rx="10" fill="#55a3ff" stroke="#0984e3" stroke-width="2"/>
  <text x="500" y="675" text-anchor="middle" font-size="12" font-weight="bold" fill="white">Long-Duration Avatar Video</text>
  <text x="500" y="695" text-anchor="middle" font-size="10" fill="white">High-fidelity ‚Ä¢ Precise Lip-sync</text>
  <text x="500" y="710" text-anchor="middle" font-size="10" fill="white">Vivid Emotions ‚Ä¢ 1080p@48fps</text>
  
  <!-- Flow connections (simplified lines instead of arrows) -->
  <line x1="250" y1="100" x2="300" y2="100" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="480" y1="100" x2="520" y2="100" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="720" y1="100" x2="760" y2="100" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="835" y1="140" x2="835" y2="180" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="835" y1="180" x2="740" y2="200" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="740" y1="260" x2="500" y2="320" stroke="#7f8c8d" stroke-width="2"/>
  <line x1="500" y1="420" x2="500" y2="650" stroke="#7f8c8d" stroke-width="2"/>
  
  <!-- Architecture Components -->
  <rect x="50" y="750" width="150" height="40" rx="5" fill="#e17055" stroke="#d63031" stroke-width="1"/>
  <text x="125" y="770" text-anchor="middle" font-size="10" font-weight="bold" fill="white">Video DiT Architecture</text>
  
  <rect x="220" y="750" width="150" height="40" rx="5" fill="#6c5ce7" stroke="#5f3dc4" stroke-width="1"/>
  <text x="295" y="770" text-anchor="middle" font-size="10" font-weight="bold" fill="white">Audio Cross-attention</text>
  
  <rect x="390" y="750" width="150" height="40" rx="5" fill="#00b894" stroke="#00a085" stroke-width="1"/>
  <text x="465" y="770" text-anchor="middle" font-size="10" font-weight="bold" fill="white">T5 Text Encoder</text>
  
  <rect x="560" y="750" width="150" height="40" rx="5" fill="#fd79a8" stroke="#e84393" stroke-width="1"/>
  <text x="635" y="770" text-anchor="middle" font-size="10" font-weight="bold" fill="white">Whisper Audio</text>
  
  <rect x="730" y="750" width="150" height="40" rx="5" fill="#fdcb6e" stroke="#f39c12" stroke-width="1"/>
  <text x="805" y="770" text-anchor="middle" font-size="10" font-weight="bold" fill="white">Parallel Processing</text>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Using multimodal language models for semantic understanding of instructions">
                        <div class="quiz-question">1. What is the main innovation in Kling-Avatar's approach compared to previous avatar animation methods?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Using higher resolution video output">Using higher resolution video output</div><div class="quiz-choice" data-value="Using multimodal language models for semantic understanding of instructions">Using multimodal language models for semantic understanding of instructions</div><div class="quiz-choice" data-value="Using faster parallel processing techniques">Using faster parallel processing techniques</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="By generating parallel sub-clips guided by blueprint keyframes">
                        <div class="quiz-question">2. How does Kling-Avatar handle long-duration video generation?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By generating the entire video in one pass">By generating the entire video in one pass</div><div class="quiz-choice" data-value="By using recursive generation frame by frame">By using recursive generation frame by frame</div><div class="quiz-choice" data-value="By generating parallel sub-clips guided by blueprint keyframes">By generating parallel sub-clips guided by blueprint keyframes</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="Using expert models to filter data quality across multiple dimensions">
                        <div class="quiz-question">3. What unique approach does Kling-Avatar use for data preparation?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Simply collecting as much data as possible">Simply collecting as much data as possible</div><div class="quiz-choice" data-value="Using expert models to filter data quality across multiple dimensions">Using expert models to filter data quality across multiple dimensions</div><div class="quiz-choice" data-value="Only using manually curated data">Only using manually curated data</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
    
    
    <!-- Personal Takeaways Section -->
    <div id="takeaways-container"></div>

        <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // ÂàõÂª∫ÈÅÆÁΩ©Â±Ç
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // Ëé∑ÂèñÊâÄÊúâÈóÆÈ¢òÊ†áÁ≠æ
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // ËÆæÁΩÆÁÇπÂáª‰∫ã‰ª∂Â§ÑÁêÜ
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // ÁÇπÂáªÊ†áÁ≠æÂàáÊç¢ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫Áä∂ÊÄÅ
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // ÈòªÊ≠¢‰∫ã‰ª∂ÂÜíÊ≥°
                    
                    // Â¶ÇÊûúÂΩìÂâçÈóÆÈ¢òÂç°Â∑≤ÁªèÊòæÁ§∫ÔºåÂàôÈöêËóèÂÆÉ
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // ÂÖàÈöêËóèÊâÄÊúâÂÖ∂‰ªñÈóÆÈ¢òÂç°
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // Â∞ÜÂºπÁ™óÂÜÖÂÆπÂ§çÂà∂Âà∞È°µÈù¢ÊúÄÂ§ñÂ±ÇÁöÑÂºπÁ™ó‰∏≠
                        document.body.appendChild(popup);
                        
                        // ÊòæÁ§∫ÂΩìÂâçÈóÆÈ¢òÂç°ÂíåËÉåÊôØÈÅÆÁΩ©
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // Á°Æ‰øùÁÇπÂáªÈóÆÈ¢òÂç°ÂÜÖÈÉ®Êó∂‰∏ç‰ºöÂÖ≥Èó≠ÈóÆÈ¢òÂç°
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // ÁÇπÂáªÈÅÆÁΩ©Â±ÇÊàñÈ°µÈù¢‰ªª‰ΩïÂÖ∂‰ªñ‰ΩçÁΩÆÊó∂ÈöêËóèÊâÄÊúâÈóÆÈ¢òÂç°
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // ‰∏∫ÊØè‰∏™ÈÄâÈ°πÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // ÈáçÁΩÆÊâÄÊúâÈÄâÈ°π
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // Ê†áËÆ∞ÂΩìÂâçÈÄâÈ°π‰∏∫Â∑≤ÈÄâ
                    this.classList.add('selected');
                    
                    // Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '‚úîÔ∏è CorrectÔºÅ';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '‚ùå WrongÔºÅ';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // Âç°ÁâáËΩÆÊí≠ÂäüËÉΩ - Êñ∞Â¢û
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // Êõ¥Êñ∞ËÆ°Êï∞Âô®ÊòæÁ§∫
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // ÊòæÁ§∫ÊåáÂÆöÁ¥¢ÂºïÁöÑÂç°Áâá
                function showCard(index) {
                    // Â§ÑÁêÜÂæ™ÁéØ
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // Êõ¥Êñ∞ÂΩìÂâçÁ¥¢Âºï
                    currentIndex = index;
                    
                    // Êõ¥Êñ∞Âç°ÁâáÊòæÁ§∫
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // Êõ¥Êñ∞ËÆ°Êï∞Âô®
                    updateCounter();
                }
                
                // ‰∏ã‰∏ÄÂº†Âç°Áâá
                function nextCard(e) {
                    e.stopPropagation(); // Èò≤Ê≠¢‰∫ã‰ª∂ÂÜíÊ≥°ÂØºËá¥ÈóÆÈ¢òÂç°ÂÖ≥Èó≠
                    showCard(currentIndex + 1);
                }
                
                // ‰∏∫Âç°ÁâáÂÆπÂô®Ê∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
                cardDeck.addEventListener('click', function(e) {
                    // Ê£ÄÊü•ÁÇπÂáªÊòØÂê¶ÂèëÁîüÂú®ÊµÅÁ®ãÂõæÂç°ÁâáÂÜÖÈÉ®ÁöÑÊªöÂä®Âå∫Âüü
                    // Â¶ÇÊûúÊòØÂú®ÊªöÂä®Êù°‰∏äÁÇπÂáªÔºå‰∏çÂàáÊç¢Âç°Áâá
                    const targetCard = e.target.closest('.paper-card');
                    if (targetCard && targetCard.classList.contains('flowchart-card')) {
                        // ËÆ°ÁÆóÁÇπÂáª‰ΩçÁΩÆÊòØÂê¶Âú®ÊªöÂä®Êù°Âå∫Âüü
                        const rect = targetCard.getBoundingClientRect();
                        const isScrollbarClick = 
                            (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                            (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);
                        
                        if (!isScrollbarClick) {
                            nextCard(e);
                        }
                    } else {
                        nextCard(e);
                    }
                });
                
                // ÈîÆÁõòÂØºËà™
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Load and render markdown takeaways
            const dateMatch = document.querySelector('h1').textContent.match(/(\d{4}-\d{2}-\d{2})/);
            if (dateMatch) {
                const date = dateMatch[1];
                const markdownPath = `../notes/${date}.md`;

                // Fetch the markdown file
                const xhr = new XMLHttpRequest();
                xhr.open('GET', markdownPath, true);
                xhr.onload = function() {
                    if (xhr.status === 200) {
                        const markdownContent = xhr.responseText;
                        if (!markdownContent.trim()) {
                            console.log('Markdown file is empty');
                            return;
                        }

                        // Convert markdown to HTML
                        let htmlContent = marked.parse(markdownContent);

                        // Fix image paths
                        const fixedContent = htmlContent.replace(
                            /src="(?!http:\/\/|https:\/\/|\/|\.\.\/)(.*?)"/g,
                            `src="../images/${date}/$1"`
                        );

                        // Wrap in styled divs
                        const wrappedHtml = `
                            <div class="takeaways-section">
                                <h2>üìù My Takeaways</h2>
                                <div class="takeaways-content">
                                    ${fixedContent}
                                </div>
                            </div>
                        `;

                        document.getElementById('takeaways-container').innerHTML = wrappedHtml;
                        console.log('Takeaways section rendered');
                    } else {
                        console.log('XHR failed - Status:', xhr.status);
                    }
                };
                xhr.onerror = function() {
                    console.log('No takeaway file found for this date');
                };
                xhr.send();
            }
        });
    </script>

</body>
</html>
