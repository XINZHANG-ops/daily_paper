
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-03-20 Papers</title>
    <style>
        * {
            box-sizing: border-box; /* Á°Æ‰øùÊâÄÊúâÂÖÉÁ¥†‰ΩøÁî®border-boxÊ®°Âûã */
        }

        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        h1 {
            color: #333;
        }
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            margin-bottom: 20px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .paper-card:hover {
            transform: translateY(-5px); /* Lift effect on hover */
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */
        }
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        word-wrap: break-word;
            overflow-wrap: break-word;
            hyphens: auto;
        }
        .paper-card p {
            margin: 5px 0;
        word-wrap: break-word;
            overflow-wrap: break-word;
        }
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        .paper-card a:hover {
            text-decoration: underline;
        }
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */
        }
        .category-chunk:hover {
            transform: translateY(-3px); /* Slightly smaller lift for categories */
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */
        }
        .category-chunk:nth-child(1) { /* 1. Topic and Domain */
            background-color: #d3e3fd; /* Blue */
        }
        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */
            background-color: #e6d6fa; /* Purple */
        }
        .category-chunk:nth-child(3) { /* 3. Problem */
            background-color: #d4f8d9; /* Green */
        }
        .category-chunk:nth-child(4) { /* 4. Methods */
            background-color: #ffd7d5; /* Pink */
        }
        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */
            background-color: #d3e3fd; /* Reuse Blue */
        }
    
        
        /* ÊûÅÂ∞èÂ±èÂπïÔºàÂ¶ÇÂ∞èÊâãÊú∫Ôºâ */
        @media (max-width: 480px) {
            body {
                padding: 5px;
            }

            .card-deck {
                min-height: 300px;
            }

            .card-deck .paper-card {
                max-height: 80vh;
                font-size: 0.9em;
                padding: 10px;
            }

            .paper-card h2 {
                font-size: 1em;
            }

            .category-chunk {
                padding: 8px;
                font-size: 0.9em;
            }

            .quiz-tab {
                width: 40px;
                height: 40px;
                font-size: 12px;
            }
        }

        /* Personal Takeaways Section Styles */
        .takeaways-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .takeaways-section h2 {
            color: #ffffff;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid rgba(255, 255, 255, 0.3);
            padding-bottom: 15px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);
        }

        .takeaways-content {
            background-color: rgba(255, 255, 255, 0.95);
            border-radius: 8px;
            padding: 25px;
            line-height: 1.8;
            color: #333;
        }

        .takeaways-content h3 {
            color: #667eea;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .takeaways-content img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 15px 0;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .takeaways-content p {
            margin: 15px 0;
        }

        .takeaways-content ul, .takeaways-content ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        .takeaways-content li {
            margin: 8px 0;
        }

        .takeaways-content blockquote {
            border-left: 4px solid #667eea;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }

        .takeaways-content pre {
            background-color: #f6f8fa;
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            border: 1px solid #e1e4e8;
        }

        .takeaways-content code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d73a49;
        }

        .takeaways-content pre code {
            background-color: transparent;
            padding: 0;
            color: #333;
        }
    
    </style>
</head>
<body>
    <h1>2025-03-20 Papers</h1>
    
    <div class="paper-card">
        <h2>Paper: 1</h2>
        <p><strong>DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement
Learning</strong></p>
        <p><strong>Published: </strong>2025-03-19</p>
        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15265" target="_blank">http://arxiv.org/pdf/2503.15265</a></p>
        <div>
            <div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong>The paper focuses on 3D mesh generation, specifically creating artist-like triangle meshes within the domain of computer graphics and computer vision.</div>
            <div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong>The paper builds upon auto-regressive mesh generation methods like MeshGPT and BPT, proposing a new tokenization algorithm, data curation strategies, and the novel application of Direct Preference Optimization (DPO) for aligning mesh generation with human preferences.</div>
            <div class="category-chunk">3.  <strong>‚ùì Problem:</strong>The paper aims to solve the limitations of existing auto-regressive mesh generation methods, such as limited face counts, mesh incompleteness, high computational costs, and the lack of alignment with human aesthetic preferences.</div>
            <div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong>The authors use an improved mesh tokenization algorithm, data curation and packaging strategies, a decoder-only transformer architecture with cross-attention, and Direct Preference Optimization (DPO) with a novel scoring standard combining 3D metrics and human evaluation.</div>
            <div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong>The results demonstrate that DeepMesh generates higher-quality, more detailed, and aesthetically pleasing meshes compared to state-of-the-art methods, evaluated through quantitative metrics (Chamfer Distance, Hausdorff Distance), a user study, and comparisons of tokenization efficiency.</div>
        </div>
    </div>
    
    <div class="paper-card">
        <h2>Paper: 2</h2>
        <p><strong>TULIP: Towards Unified Language-Image Pretraining</strong></p>
        <p><strong>Published: </strong>2025-03-19</p>
        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15485" target="_blank">http://arxiv.org/pdf/2503.15485</a></p>
        <div>
            <div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong>The paper introduces TULIP, a unified language-image pretraining model designed to improve both high-level semantic understanding and fine-grained visual detail representation in image-text tasks.</div>
            <div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong>The paper builds on contrastive image-text models like CLIP and SigLIP, but proposes generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization.</div>
            <div class="category-chunk">3.  <strong>‚ùì Problem:</strong>Existing contrastive image-text models often struggle with vision-centric tasks requiring high-fidelity image understanding, such as spatial reasoning and fine-grained object recognition.</div>
            <div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong>The authors used generative data augmentation (GeCo), multi-view contrastive learning (image-text, image-image, text-text), and a reconstruction loss to train the model.</div>
            <div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong>TULIP outperforms state-of-the-art models on zero-shot classification, fine-grained recognition, object detection, and multi-modal reasoning tasks, demonstrating improved visual and language understanding.</div>
        </div>
    </div>
    
    <div class="paper-card">
        <h2>Paper: 3</h2>
        <p><strong>Cube: A Roblox View of 3D Intelligence</strong></p>
        <p><strong>Published: </strong>2025-03-19</p>
        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2503.15475" target="_blank">http://arxiv.org/pdf/2503.15475</a></p>
        <div>
            <div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong>The paper focuses on 3D generative AI and its application within the Roblox platform, specifically addressing 3D shape tokenization.</div>
            <div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong>The paper builds on foundation models, vector quantization, and transformer architectures, proposing Phase-Modulated Positional Encoding, stochastic linear shortcut, and self-supervised loss for 3D shape tokenization.</div>
            <div class="category-chunk">3.  <strong>‚ùì Problem:</strong>The paper aims to solve the challenge of representing and generating 3D shapes in a way that is compatible with large language models and suitable for various generative tasks.</div>
            <div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong>The authors used an encoder-decoder architecture with a Perceiver-based transformer, vector quantization, Phase-Modulated Positional Encoding, stochastic gradient shortcut, and self-supervised loss.</div>
            <div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong>The proposed shape tokenizer outperformed existing methods in shape reconstruction quality (measured by S-IoU and V-IoU), and enabled applications like text-to-shape, shape-to-text, and text-to-scene generation.</div>
        </div>
    </div>
        

    <!-- Personal Takeaways Section -->
        <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <div id="takeaways-container"></div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Load and render markdown takeaways
            const dateMatch = document.querySelector('h1').textContent.match(/(\d{4}-\d{2}-\d{2})/);
            if (dateMatch) {
                const date = dateMatch[1];
                const markdownPath = `../notes/${date}.md`;

                // Fetch the markdown file
                const xhr = new XMLHttpRequest();
                xhr.open('GET', markdownPath, true);
                xhr.onload = function() {
                    if (xhr.status === 200) {
                        const markdownContent = xhr.responseText;
                        if (!markdownContent.trim()) {
                            console.log('Markdown file is empty');
                            return;
                        }

                        // Convert markdown to HTML
                        let htmlContent = marked.parse(markdownContent);

                        // Fix image paths
                        const fixedContent = htmlContent.replace(
                            /src="(?!http:\/\/|https:\/\/|\/|\.\.\/)(.*?)"/g,
                            `src="../images/${date}/$1"`
                        );

                        // Wrap in styled divs
                        const wrappedHtml = `
                            <div class="takeaways-section">
                                <h2>üìù My Takeaways</h2>
                                <div class="takeaways-content">
                                    ${fixedContent}
                                </div>
                            </div>
                        `;

                        document.getElementById('takeaways-container').innerHTML = wrappedHtml;
                        console.log('Takeaways section rendered');
                            // Trigger MathJax to render LaTeX equations
                            if (typeof MathJax !== 'undefined') {
                                MathJax.typesetPromise([document.getElementById('takeaways-container')])
                                    .then(() => {
                                        console.log('MathJax rendering complete');
                                    })
                                    .catch((err) => console.error('MathJax rendering error:', err));
                            }
                    } else {
                        console.log('XHR failed - Status:', xhr.status);
                    }
                };
                xhr.onerror = function() {
                    console.log('No takeaway file found for this date');
                };
                xhr.send();
            }
        });
    </script>

</body>
</html>
