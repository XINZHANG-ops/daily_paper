
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2025-12-03 Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            background-color: #4d4042;
            background-image: url('bg.png');
            background-size: auto;
            background-repeat: repeat;
            overflow-x: hidden;
        }
        h1 {
            color: #333;
        }
        .paper-container {
            position: relative;
            display: flex;
            margin-bottom: 30px;
            justify-content: space-between;
            max-width: 100%;
            transition: all 0.3s ease;
        }
        
        /* Âç°ÁâáÂÆπÂô®Ê†∑Âºè - Êñ∞Â¢û */
        .card-deck {
            width: 100%;
            position: relative;
            margin-right: 20px;
            height: 600px; /* Âõ∫ÂÆöÈ´òÂ∫¶ */
            cursor: pointer; /* Â¢ûÂä†ÊåáÈíàÊ†∑ÂºèÊèêÁ§∫ÂèØÁÇπÂáª */
        }
        
        /* Âç°ÁâáÈÄöÁî®Ê†∑Âºè */
        .paper-card {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 15px;
            transition: all 0.3s ease;
            background-size: auto;
            background-repeat: repeat;
            background-position: center;
            background: linear-gradient(rgba(255, 255, 255, 0.5), rgba(255, 255, 255, 0.5)), url('');
            background-blend-mode: overlay;
            overflow-wrap: break-word;
        }
        
        /* ËΩÆÊí≠Âç°ÁâáÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            box-sizing: border-box;
            height: 100%;
            transition: transform 0.5s ease, opacity 0.5s ease;
        }
        
        /* ÈùûÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card:not(.active) {
            opacity: 0;
            pointer-events: none;
            transform: translateY(-10px);
        }
        
        /* ÊøÄÊ¥ªÂç°ÁâáÁöÑÊ†∑Âºè - Êñ∞Â¢û */
        .card-deck .paper-card.active {
            opacity: 1;
            pointer-events: auto;
            transform: translateY(0);
            z-index: 1;
        }
        
        /* Á¨¨‰∏ÄÂº†Âç°ÁâáÔºàÊñáÊú¨ÂÜÖÂÆπÔºâ‰∏çÈúÄË¶ÅÊªöÂä® */
        .card-deck .paper-card:first-child {
            overflow-y: auto;
        }

        .card-deck .paper-card:first-child:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        /* Á¨¨‰∫åÂº†Âç°ÁâáÔºàÊµÅÁ®ãÂõæÔºâÊîØÊåÅÊªöÂä® */
        .flowchart-card {
            text-align: center;
            background-color: #fff !important;
            overflow: auto !important;
            padding-bottom: 50px; /* Ê∑ªÂä†Â∫ïÈÉ®Â°´ÂÖÖ */
        }

        .flowchart-card svg {
            width: 100%;
            height: auto;
            max-height: none; /* ÁßªÈô§‰ªª‰ΩïÈ´òÂ∫¶ÈôêÂà∂ */
        }
        
        /* ‰º†ÁªüÂç°ÁâáÊ†∑Âºè */
        .paper-container > .paper-card {
            width: 100%;
            margin-right: 20px;
        }
        
        .paper-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .paper-card h2 {
            margin: 0 0 10px;
            font-size: 1.2em;
        }
        
        .paper-card p {
            margin: 5px 0;
        }
        
        .paper-card a {
            color: #1a73e8;
            text-decoration: none;
        }
        
        .paper-card a:hover {
            text-decoration: underline;
        }
        
        .category-chunk {
            padding: 10px;
            margin: 5px 0;
            border-radius: 5px;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        
        .category-chunk:hover {
            transform: translateY(-3px);
            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15);
        }
        
        .category-chunk:nth-child(1) {
            background-color: #d3e3fd;
        }
        
        .category-chunk:nth-child(2) {
            background-color: #e6d6fa;
        }
        
        .category-chunk:nth-child(3) {
            background-color: #d4f8d9;
        }
        
        .category-chunk:nth-child(4) {
            background-color: #ffd7d5;
        }
        
        .category-chunk:nth-child(5) {
            background-color: #d3e3fd;
        }
        
        /* Âç°ÁâáËÆ°Êï∞Âô® - Êñ∞Â¢û */
        .card-counter {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 12px;
            z-index: 2;
        }

        /* Quiz tabs and popup styles */
        .quiz-tabs {
            display: flex;
            flex-direction: column;
            position: sticky;
            top: 20px;
            align-self: flex-start;
            width: fit-content;
            min-width: 50px;
            margin-left: auto;
        }
        .quiz-tab {
            width: 50px;
            height: 50px;
            background-color: #1a73e8;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 15px;
            cursor: pointer;
            position: relative;
            font-weight: bold;
            font-size: 16px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            z-index: 10;
        }
        .quiz-tab:hover {
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        }
        .quiz-popup {
            position: fixed; /* Êîπ‰∏∫Âõ∫ÂÆöÂÆö‰ΩçÔºå‰∏çÈöèÊªöÂä®ËÄåÁßªÂä® */
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%); /* Â±Ö‰∏≠ÊòæÁ§∫ */
            width: 90%;
            max-width: 500px; /* Â¢ûÂä†ÊúÄÂ§ßÂÆΩÂ∫¶ÔºåÈÄÇÂ∫îÈïøÂÜÖÂÆπ */
            max-height: 80vh; /* ÈôêÂà∂ÊúÄÂ§ßÈ´òÂ∫¶ */
            overflow-y: auto; /* ÂÜÖÂÆπËøáÂ§öÊó∂ÂèØÊªöÂä® */
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
            display: none;
            z-index: 9999; /* Á°Æ‰øùÊòæÁ§∫Âú®ÊúÄ‰∏äÂ±Ç */
        }
        
        /* Ê∑ªÂä†ÈÅÆÁΩ©Â±ÇÔºåÈò≤Ê≠¢ÈóÆÈ¢òÂç°Ë¢´ÂÖ∂‰ªñÂÜÖÂÆπÈÅÆÊå° */
        .popup-backdrop {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: rgba(0, 0, 0, 0.5);
            z-index: 9998;
            display: none;
        }
        
        .popup-backdrop.active {
            display: block;
        }
        /* ‰ΩøÁî®JavaScriptÊéßÂà∂ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫ÂíåÈöêËóèÔºå‰∏çÂÜç‰ΩøÁî®hover */
        .quiz-popup.active {
            display: block;
        }
        .quiz-question {
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            font-size: 18px;
            line-height: 1.5;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            hyphens: auto; /* Âú®ÂøÖË¶ÅÊó∂‰ΩøÁî®ËøûÂ≠óÁ¨¶ */
        }
        .quiz-choices {
            display: flex;
            flex-direction: column;
        }
        .quiz-choice {
            padding: 12px 15px;
            margin: 8px 0;
            border: 1px solid #ddd;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.2s;
            color: #333;
            font-size: 15px;
            background-color: #f9f9f9;
            word-wrap: break-word; /* Á°Æ‰øùÈïøÂçïËØçËá™Âä®Êç¢Ë°å */
            overflow-wrap: break-word;
            line-height: 1.4;
            text-align: left; /* ÈïøÊñáÊú¨Â∑¶ÂØπÈΩê */
            display: block; /* Á°Æ‰øùÊòØÂùóÁ∫ßÂÖÉÁ¥† */
            white-space: normal; /* ÂÖÅËÆ∏Ëá™Âä®Êç¢Ë°å */
        }
        .quiz-choice:hover {
            background-color: #f0f0f0;
        }
        .quiz-choice.selected {
            background-color: #d3e3fd;
            border-color: #1a73e8;
        }
        .quiz-choice.correct {
            background-color: #d4f8d9;
            border-color: #0f9d58;
        }
        .quiz-choice.incorrect {
            background-color: #ffd7d5;
            border-color: #d93025;
        }
        .quiz-feedback {
            margin-top: 15px;
            padding: 10px;
            border-radius: 4px;
            display: none;
            font-weight: bold;
            text-align: center;
        }
        .quiz-feedback.correct {
            background-color: #d4f8d9;
            color: #0f9d58;
            display: block;
            border: 1px solid #0f9d58;
        }
        .quiz-feedback.incorrect {
            background-color: #ffd7d5;
            color: #d93025;
            display: block;
            border: 1px solid #d93025;
        }
        
        /* ÈïøÊñáÊú¨ÈÄâÈ°πÁöÑÁâπÊÆäÊ†∑Âºè */
        .quiz-choice.long-text {
            font-size: 13px;
            line-height: 1.3;
            padding: 10px 12px;
        }
        
        /* Á°Æ‰øùÂºπÁ™ó‰∏≠ÁöÑÊåâÈíÆÊñáÊú¨‰∏ç‰ºöÊ∫¢Âá∫ */
        .quiz-choice button,
        .quiz-choice a {
            word-break: break-word;
            white-space: normal;
            text-align: left;
            width: 100%;
        }
        
        /* ÈÄÇÂ∫îË∂ÖÈïøÈÄâÈ°πÊñáÊú¨ */
        @media (max-width: 500px) {
            .quiz-popup {
                width: 95%;
                padding: 12px;
            }
            .quiz-question {
                font-size: 15px;
                margin-bottom: 12px;
            }
            .quiz-choice {
                padding: 8px 10px;
                font-size: 13px;
                line-height: 1.3;
            }
            .quiz-feedback {
                font-size: 13px;
                padding: 8px;
            }
        }
        
        @media (max-width: 768px) {
            .paper-container {
                flex-direction: column;
            }
            
            .card-deck {
                margin-right: 0;
                margin-bottom: 40px;
                height: 650px; /* ÁßªÂä®ËÆæÂ§á‰∏äÈ´òÂ∫¶Ë∞ÉÊï¥ */
            }
            
            .paper-container > .paper-card {
                width: 100% !important;
                margin-bottom: 20px;
                margin-right: 0;
            }
            
            .quiz-tabs {
                width: 100%;
                flex-direction: row;
                flex-wrap: wrap;
                justify-content: flex-start;
                position: relative;
                margin-left: 0;
            }
            .quiz-tab {
                margin-right: 10px;
                margin-bottom: 10px;
            }
        }
    </style>
</head>
<body>
    <h1>2025-12-03 Papers</h1>
    
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/broken-noise.png');">
                        <h2 style="color: #ffffff;">Paper 1</h2>
                        <p style="color: #badb12;"><strong>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-12-02</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2512.02556" target="_blank">http://arxiv.org/pdf/2512.02556</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> Development of DeepSeek-V3.2, an open-source large language model focusing on computational efficiency, reasoning capabilities, and agent performance in the domain of artificial intelligence and natural language processing.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on previous work in large language models like DeepSeek-V3.1, it introduces DeepSeek Sparse Attention (DSA) for efficient computation, a scalable reinforcement learning framework, and a novel agentic task synthesis pipeline.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper addresses three critical limitations in open-source models: inefficient attention mechanisms for long sequences, insufficient computational investment during post-training, and poor generalization in AI agent applications.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The paper implements DSA to reduce computational complexity, uses a scalable reinforcement learning protocol with increased post-training compute, and develops a large-scale agentic task synthesis pipeline generating over 1,800 environments and 85,000 complex prompts.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> DeepSeek-V3.2 achieved comparable performance to GPT-5 across multiple reasoning benchmarks, while its specialized variant DeepSeek-V3.2-Speciale surpassed GPT-5 and matched Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad and International Olympiad in Informatics.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <rect width="1000" height="800" fill="#f8f9fa"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-size="24" font-weight="bold" fill="#2c3e50">
    DeepSeek-V3.2 Workflow
  </text>
  
  <!-- Phase 1: Architecture Design -->
  <rect x="50" y="70" width="280" height="120" fill="#e8f4fd" stroke="#3498db" stroke-width="2" rx="10"/>
  <text x="190" y="95" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">Architecture Design</text>
  <text x="190" y="115" text-anchor="middle" font-size="12" fill="#34495e">DeepSeek Sparse Attention (DSA)</text>
  <text x="70" y="135" font-size="11" fill="#34495e">‚Ä¢ Lightning Indexer</text>
  <text x="70" y="150" font-size="11" fill="#34495e">‚Ä¢ Fine-grained Token Selection</text>
  <text x="70" y="165" font-size="11" fill="#34495e">‚Ä¢ O(L¬≤) ‚Üí O(Lk) complexity</text>
  
  <!-- Phase 2: Continued Pre-training -->
  <rect x="360" y="70" width="280" height="120" fill="#f0f8e8" stroke="#27ae60" stroke-width="2" rx="10"/>
  <text x="500" y="95" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">Continued Pre-training</text>
  <text x="380" y="115" font-size="11" fill="#34495e">Dense Warm-up Stage:</text>
  <text x="380" y="130" font-size="11" fill="#34495e">‚Ä¢ 1000 steps, 2.1B tokens</text>
  <text x="380" y="145" font-size="11" fill="#34495e">Sparse Training Stage:</text>
  <text x="380" y="160" font-size="11" fill="#34495e">‚Ä¢ 15000 steps, 943.7B tokens</text>
  
  <!-- Phase 3: Post-training -->
  <rect x="670" y="70" width="280" height="120" fill="#fff2e6" stroke="#e67e22" stroke-width="2" rx="10"/>
  <text x="810" y="95" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">Post-training</text>
  <text x="690" y="115" font-size="11" fill="#34495e">Specialist Distillation:</text>
  <text x="690" y="130" font-size="11" fill="#34495e">‚Ä¢ 6 specialized domains</text>
  <text x="690" y="145" font-size="11" fill="#34495e">Mixed RL Training:</text>
  <text x="690" y="160" font-size="11" fill="#34495e">‚Ä¢ GRPO algorithm</text>
  
  <!-- Phase 4: Scaling GRPO -->
  <rect x="50" y="250" width="400" height="140" fill="#fdf2e9" stroke="#d35400" stroke-width="2" rx="10"/>
  <text x="250" y="275" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">Scaling GRPO</text>
  <text x="70" y="295" font-size="12" fill="#34495e">Stability Improvements:</text>
  <text x="70" y="315" font-size="11" fill="#34495e">‚Ä¢ Unbiased KL Estimate</text>
  <text x="70" y="330" font-size="11" fill="#34495e">‚Ä¢ Off-Policy Sequence Masking</text>
  <text x="70" y="345" font-size="11" fill="#34495e">‚Ä¢ Keep Routing (MoE)</text>
  <text x="70" y="360" font-size="11" fill="#34495e">‚Ä¢ Keep Sampling Mask</text>
  
  <!-- Phase 5: Thinking in Tool-Use -->
  <rect x="500" y="250" width="450" height="140" fill="#f4f0ff" stroke="#8e44ad" stroke-width="2" rx="10"/>
  <text x="725" y="275" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">Thinking in Tool-Use</text>
  <text x="520" y="295" font-size="12" fill="#34495e">Context Management:</text>
  <text x="520" y="310" font-size="11" fill="#34495e">‚Ä¢ Retain reasoning across tool calls</text>
  <text x="520" y="325" font-size="11" fill="#34495e">Cold-Start Integration</text>
  <text x="520" y="345" font-size="12" fill="#34495e">Large-Scale Agentic Tasks:</text>
  <text x="520" y="360" font-size="11" fill="#34495e">‚Ä¢ 1,827 environments, 85K prompts</text>
  
  <!-- Phase 6: Agentic Task Synthesis -->
  <rect x="50" y="430" width="900" height="180" fill="#e8f8f5" stroke="#16a085" stroke-width="2" rx="10"/>
  <text x="500" y="455" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">Large-Scale Agentic Task Synthesis</text>
  
  <!-- Four agent types in a grid -->
  <rect x="80" y="475" width="180" height="60" fill="#d5f4e6" stroke="#27ae60" stroke-width="1" rx="5"/>
  <text x="170" y="495" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Search Agent</text>
  <text x="170" y="510" text-anchor="middle" font-size="10" fill="#34495e">50,275 tasks</text>
  <text x="170" y="525" text-anchor="middle" font-size="10" fill="#34495e">Real env + Synthetic prompts</text>
  
  <rect x="290" y="475" width="180" height="60" fill="#fdeaa7" stroke="#f39c12" stroke-width="1" rx="5"/>
  <text x="380" y="495" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Code Agent</text>
  <text x="380" y="510" text-anchor="middle" font-size="10" fill="#34495e">24,667 tasks</text>
  <text x="380" y="525" text-anchor="middle" font-size="10" fill="#34495e">Real env + Extracted prompts</text>
  
  <rect x="500" y="475" width="180" height="60" fill="#fadbd8" stroke="#e74c3c" stroke-width="1" rx="5"/>
  <text x="590" y="495" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Code Interpreter</text>
  <text x="590" y="510" text-anchor="middle" font-size="10" fill="#34495e">5,908 tasks</text>
  <text x="590" y="525" text-anchor="middle" font-size="10" fill="#34495e">Real env + Extracted prompts</text>
  
  <rect x="710" y="475" width="180" height="60" fill="#e8daef" stroke="#9b59b6" stroke-width="1" rx="5"/>
  <text x="800" y="495" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">General Agent</text>
  <text x="800" y="510" text-anchor="middle" font-size="10" fill="#34495e">4,417 tasks</text>
  <text x="800" y="525" text-anchor="middle" font-size="10" fill="#34495e">Synthetic env + prompts</text>
  
  <!-- Synthesis Process -->
  <text x="500" y="565" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">Environment Synthesis Process</text>
  <text x="100" y="585" font-size="11" fill="#34495e">1. Environment & Toolset Construction ‚Üí 2. Task Synthesis ‚Üí 3. Solution Generation & Verification</text>
  
  <!-- Final Models -->
  <rect x="200" y="650" width="250" height="80" fill="#e3f2fd" stroke="#2196f3" stroke-width="2" rx="10"/>
  <text x="325" y="675" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">DeepSeek-V3.2</text>
  <text x="325" y="695" text-anchor="middle" font-size="11" fill="#34495e">Balanced reasoning & efficiency</text>
  <text x="325" y="710" text-anchor="middle" font-size="11" fill="#34495e">128K context, length constraints</text>
  
  <rect x="550" y="650" width="250" height="80" fill="#fff3e0" stroke="#ff9800" stroke-width="2" rx="10"/>
  <text x="675" y="675" text-anchor="middle" font-size="16" font-weight="bold" fill="#2c3e50">DeepSeek-V3.2-Speciale</text>
  <text x="675" y="695" text-anchor="middle" font-size="11" fill="#34495e">Extended thinking capability</text>
  <text x="675" y="710" text-anchor="middle" font-size="11" fill="#34495e">Gold medal performance</text>
  
  <!-- Flow indicators with curved paths -->
  <path d="M 330 130 Q 380 150 360 180" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 640 130 Q 690 150 670 180" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 190 190 Q 220 220 250 250" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 810 190 Q 780 220 725 250" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 500 390 Q 500 410 500 430" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 400 610 Q 350 630 325 650" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  <path d="M 600 610 Q 650 630 675 650" fill="none" stroke="#7f8c8d" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Arrow marker definition -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#7f8c8d"/>
    </marker>
  </defs>
  
  <!-- Performance metrics box -->
  <rect x="20" y="750" width="960" height="40" fill="#ecf0f1" stroke="#bdc3c7" stroke-width="1" rx="5"/>
  <text x="500" y="770" text-anchor="middle" font-size="12" font-weight="bold" fill="#2c3e50">
    Key Results: GPT-5 level reasoning ‚Ä¢ Gold medal IOI/IMO ‚Ä¢ 10%+ post-training compute ‚Ä¢ Significant agentic improvements
  </text>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="DeepSeek Sparse Attention (DSA)">
                        <div class="quiz-question">1. What is the main innovation in DeepSeek-V3.2's attention mechanism that helps improve computational efficiency?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="DeepSeek Sparse Attention (DSA)">DeepSeek Sparse Attention (DSA)</div><div class="quiz-choice" data-value="Multi-Head Attention (MHA)">Multi-Head Attention (MHA)</div><div class="quiz-choice" data-value="Self-Attention Pooling (SAP)">Self-Attention Pooling (SAP)</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="1,800 environments and 85,000 prompts">
                        <div class="quiz-question">2. How many distinct environments and complex prompts were generated through the agentic task synthesis pipeline?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="850 environments and 18,000 prompts">850 environments and 18,000 prompts</div><div class="quiz-choice" data-value="1,800 environments and 85,000 prompts">1,800 environments and 85,000 prompts</div><div class="quiz-choice" data-value="8,500 environments and 180,000 prompts">8,500 environments and 180,000 prompts</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="It earned gold medals in both IMO and IOI 2025">
                        <div class="quiz-question">3. What unique achievement did DeepSeek-V3.2-Speciale accomplish in competitive evaluations?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It outperformed all existing language models in general tasks">It outperformed all existing language models in general tasks</div><div class="quiz-choice" data-value="It achieved bronze medals in international competitions">It achieved bronze medals in international competitions</div><div class="quiz-choice" data-value="It earned gold medals in both IMO and IOI 2025">It earned gold medals in both IMO and IOI 2025</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/woven.png');">
                        <h2 style="color: #ffffff;">Paper 2</h2>
                        <p style="color: #badb12;"><strong>MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-12-02</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2512.03041" target="_blank">http://arxiv.org/pdf/2512.03041</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> The paper presents MultiShotMaster, a controllable multi-shot video generation framework in the domain of AI video generation and computer vision.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> The work builds upon pretrained single-shot text-to-video models but introduces novel RoPE variants to enable flexible shot arrangements and reference injection, which existing multi-shot methods lack.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the limitations of current video generation methods that can only produce single-shot clips or multi-shot videos with fixed durations and limited controllability.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> The authors extend a pretrained model with Multi-Shot Narrative RoPE for shot transitions, Spatiotemporal Position-Aware RoPE for reference injection, and design a multi-shot & multi-reference attention mask along with an automated data curation pipeline.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> The framework achieves superior performance across metrics like text alignment, inter-shot consistency, transition deviation, and narrative coherence, while providing unprecedented control over shot arrangements, subject motion, and scene customization.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <!-- Background -->
  <defs>
    <linearGradient id="bgGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0f8ff;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#e6f3ff;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="blueGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#4a90e2;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#357abd;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="greenGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#5cb85c;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#449d44;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="orangeGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#f0ad4e;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#ec971f;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="purpleGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#9b59b6;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#8e44ad;stop-opacity:1" />
    </linearGradient>
  </defs>
  
  <rect width="1000" height="800" fill="url(#bgGrad)"/>
  
  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-family="Arial, sans-serif" font-size="20" font-weight="bold" fill="#2c3e50">MultiShotMaster: Controllable Multi-Shot Video Generation Framework</text>
  
  <!-- Data Curation Pipeline (Top Section) -->
  <rect x="50" y="60" width="900" height="120" rx="10" fill="url(#blueGrad)" opacity="0.1" stroke="#4a90e2" stroke-width="2"/>
  <text x="500" y="80" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="#2c3e50">Data Curation Pipeline</text>
  
  <rect x="70" y="90" width="140" height="40" rx="5" fill="url(#blueGrad)" opacity="0.8"/>
  <text x="140" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Long Video Collection</text>
  <text x="140" y="118" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">& Shot Detection</text>
  
  <rect x="230" y="90" width="140" height="40" rx="5" fill="url(#blueGrad)" opacity="0.8"/>
  <text x="300" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Scene Segmentation</text>
  <text x="300" y="118" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">& Multi-Shot Sampling</text>
  
  <rect x="390" y="90" width="140" height="40" rx="5" fill="url(#blueGrad)" opacity="0.8"/>
  <text x="460" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Hierarchical Caption</text>
  <text x="460" y="118" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Generation</text>
  
  <rect x="550" y="90" width="140" height="40" rx="5" fill="url(#blueGrad)" opacity="0.8"/>
  <text x="620" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Subject Detection</text>
  <text x="620" y="118" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">& Tracking</text>
  
  <rect x="710" y="90" width="140" height="40" rx="5" fill="url(#blueGrad)" opacity="0.8"/>
  <text x="780" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Background</text>
  <text x="780" y="118" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">Extraction</text>
  
  <text x="70" y="155" font-family="Arial, sans-serif" font-size="9" fill="#7f8c8d">TransNet V2</text>
  <text x="230" y="155" font-family="Arial, sans-serif" font-size="9" fill="#7f8c8d">Scene Clustering</text>
  <text x="390" y="155" font-family="Arial, sans-serif" font-size="9" fill="#7f8c8d">Gemini-2.5</text>
  <text x="550" y="155" font-family="Arial, sans-serif" font-size="9" fill="#7f8c8d">YOLOv11+ByteTrack</text>
  <text x="710" y="155" font-family="Arial, sans-serif" font-size="9" fill="#7f8c8d">OmniEraser</text>
  
  <!-- Core Architecture (Middle Section) -->
  <rect x="50" y="200" width="900" height="300" rx="10" fill="url(#greenGrad)" opacity="0.1" stroke="#5cb85c" stroke-width="2"/>
  <text x="500" y="220" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="#2c3e50">Core Architecture & Methods</text>
  
  <!-- Input Processing -->
  <rect x="70" y="240" width="120" height="60" rx="8" fill="url(#greenGrad)" opacity="0.9"/>
  <text x="130" y="260" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Input Processing</text>
  <text x="130" y="275" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Multi-Shot Videos</text>
  <text x="130" y="285" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Reference Images</text>
  <text x="130" y="295" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Text Captions</text>
  
  <!-- Multi-Shot Narrative RoPE -->
  <rect x="220" y="240" width="150" height="60" rx="8" fill="url(#orangeGrad)" opacity="0.9"/>
  <text x="295" y="258" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Multi-Shot</text>
  <text x="295" y="270" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Narrative RoPE</text>
  <text x="295" y="285" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Phase Shift at Transitions</text>
  <text x="295" y="295" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Shot Boundary Detection</text>
  
  <!-- Spatiotemporal Position-Aware RoPE -->
  <rect x="400" y="240" width="150" height="60" rx="8" fill="url(#purpleGrad)" opacity="0.9"/>
  <text x="475" y="253" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" font-weight="bold" fill="white">Spatiotemporal</text>
  <text x="475" y="265" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" font-weight="bold" fill="white">Position-Aware RoPE</text>
  <text x="475" y="280" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Grounded Reference</text>
  <text x="475" y="290" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Injection</text>
  
  <!-- Attention Mechanism -->
  <rect x="580" y="240" width="150" height="60" rx="8" fill="#e74c3c" opacity="0.9"/>
  <text x="655" y="258" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Multi-Shot &</text>
  <text x="655" y="270" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Multi-Reference</text>
  <text x="655" y="285" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Attention Mask</text>
  
  <!-- DiT Architecture -->
  <rect x="760" y="240" width="120" height="60" rx="8" fill="#34495e" opacity="0.9"/>
  <text x="820" y="258" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">DiT Architecture</text>
  <text x="820" y="275" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Temporal Attention</text>
  <text x="820" y="285" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Cross Attention</text>
  <text x="820" y="295" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">FFN</text>
  
  <!-- Training Strategy -->
  <rect x="220" y="320" width="560" height="80" rx="8" fill="#95a5a6" opacity="0.2" stroke="#7f8c8d" stroke-width="2"/>
  <text x="500" y="340" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="#2c3e50">Three-Stage Training Strategy</text>
  
  <rect x="240" y="350" width="160" height="40" rx="5" fill="#3498db" opacity="0.8"/>
  <text x="320" y="367" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Stage 1: Single-Shot</text>
  <text x="320" y="377" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Reference Injection</text>
  
  <rect x="420" y="350" width="160" height="40" rx="5" fill="#2ecc71" opacity="0.8"/>
  <text x="500" y="367" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Stage 2: Multi-Shot</text>
  <text x="500" y="377" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Joint Training</text>
  
  <rect x="600" y="350" width="160" height="40" rx="5" fill="#e67e22" opacity="0.8"/>
  <text x="680" y="367" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Stage 3: Subject-Focused</text>
  <text x="680" y="377" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Post-Training</text>
  
  <!-- Key Features -->
  <rect x="70" y="430" width="810" height="60" rx="8" fill="#f39c12" opacity="0.2" stroke="#e67e22" stroke-width="2"/>
  <text x="475" y="450" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="#2c3e50">Key Capabilities</text>
  
  <circle cx="150" cy="470" r="15" fill="#e74c3c"/>
  <text x="150" y="475" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">1</text>
  <text x="180" y="475" font-family="Arial, sans-serif" font-size="10" fill="#2c3e50">Text-driven Inter-shot Consistency</text>
  
  <circle cx="400" cy="470" r="15" fill="#27ae60"/>
  <text x="400" y="475" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">2</text>
  <text x="430" y="475" font-family="Arial, sans-serif" font-size="10" fill="#2c3e50">Customized Subject Motion Control</text>
  
  <circle cx="650" cy="470" r="15" fill="#8e44ad"/>
  <text x="650" y="475" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">3</text>
  <text x="680" y="475" font-family="Arial, sans-serif" font-size="10" fill="#2c3e50">Background-driven Scene Consistency</text>
  
  <!-- Output & Applications -->
  <rect x="50" y="520" width="900" height="120" rx="10" fill="url(#purpleGrad)" opacity="0.1" stroke="#9b59b6" stroke-width="2"/>
  <text x="500" y="540" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="#2c3e50">Output & Applications</text>
  
  <rect x="100" y="560" width="180" height="50" rx="8" fill="url(#purpleGrad)" opacity="0.8"/>
  <text x="190" y="580" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Variable Shot Count</text>
  <text x="190" y="595" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">& Flexible Duration</text>
  
  <rect x="310" y="560" width="180" height="50" rx="8" fill="url(#purpleGrad)" opacity="0.8"/>
  <text x="400" y="580" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Controllable Multi-Shot</text>
  <text x="400" y="595" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Video Generation</text>
  
  <rect x="520" y="560" width="180" height="50" rx="8" fill="url(#purpleGrad)" opacity="0.8"/>
  <text x="610" y="580" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Narrative Coherence</text>
  <text x="610" y="595" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">& Visual Consistency</text>
  
  <rect x="730" y="560" width="180" height="50" rx="8" fill="url(#purpleGrad)" opacity="0.8"/>
  <text x="820" y="580" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">Director-Level</text>
  <text x="820" y="595" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">Control</text>
  
  <!-- Evaluation Metrics -->
  <rect x="50" y="660" width="900" height="80" rx="10" fill="#ecf0f1" stroke="#bdc3c7" stroke-width="2"/>
  <text x="500" y="680" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="#2c3e50">Evaluation Metrics</text>
  
  <text x="80" y="700" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Text Alignment</text>
  <text x="80" y="715" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Inter-Shot Consistency</text>
  <text x="80" y="730" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Transition Deviation</text>
  
  <text x="250" y="700" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Narrative Coherence</text>
  <text x="250" y="715" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Reference Consistency</text>
  <text x="250" y="730" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Grounding Accuracy</text>
  
  <text x="450" y="700" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Subject Consistency</text>
  <text x="450" y="715" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Scene Consistency</text>
  <text x="450" y="730" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Motion Control</text>
  
  <text x="650" y="700" font-family="Arial, sans-serif" font-size="10" fill="#34495e">Baselines:</text>
  <text x="650" y="715" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ CineTrans, EchoShot</text>
  <text x="650" y="730" font-family="Arial, sans-serif" font-size="10" fill="#34495e">‚Ä¢ Phantom, VACE</text>
  
  <!-- Flow connections (subtle) -->
  <line x1="190" y1="300" x2="220" y2="320" stroke="#7f8c8d" stroke-width="1" opacity="0.5"/>
  <line x1="370" y1="270" x2="400" y2="270" stroke="#7f8c8d" stroke-width="1" opacity="0.5"/>
  <line x1="550" y1="270" x2="580" y2="270" stroke="#7f8c8d" stroke-width="1" opacity="0.5"/>
  <line x1="730" y1="270" x2="760" y2="270" stroke="#7f8c8d" stroke-width="1" opacity="0.5"/>
  
  <line x1="500" y1="400" x2="500" y2="430" stroke="#7f8c8d" stroke-width="2" opacity="0.5"/>
  <line x1="500" y1="490" x2="500" y2="520" stroke="#7f8c8d" stroke-width="2" opacity="0.5"/>
  <line x1="500" y1="640" x2="500" y2="660" stroke="#7f8c8d" stroke-width="2" opacity="0.5"/>
  
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Multi-Shot Narrative RoPE with phase shifts">
                        <div class="quiz-question">1. What is the key innovation in MultiShotMaster's architecture that enables flexible shot transitions?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Multi-Shot Narrative RoPE with phase shifts">Multi-Shot Narrative RoPE with phase shifts</div><div class="quiz-choice" data-value="Traditional attention masks">Traditional attention masks</div><div class="quiz-choice" data-value="Temporal convolution layers">Temporal convolution layers</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="By establishing an automated data annotation pipeline">
                        <div class="quiz-question">2. How does the framework handle data scarcity for training multi-shot video generation?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="By using only synthetic data">By using only synthetic data</div><div class="quiz-choice" data-value="By establishing an automated data annotation pipeline">By establishing an automated data annotation pipeline</div><div class="quiz-choice" data-value="By limiting training to single-shot videos">By limiting training to single-shot videos</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="It allows both variable shot counts and flexible shot durations">
                        <div class="quiz-question">3. What is a unique capability of MultiShotMaster compared to existing methods?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It can only generate fixed-length videos">It can only generate fixed-length videos</div><div class="quiz-choice" data-value="It requires manual annotation of shot transitions">It requires manual annotation of shot transitions</div><div class="quiz-choice" data-value="It allows both variable shot counts and flexible shot durations">It allows both variable shot counts and flexible shot durations</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
            <div class="paper-container">
                <div class="card-deck">
                    <!-- Âç°ÁâáËÆ°Êï∞Âô® -->
                    <div class="card-counter">1/2</div>
                    
                    <!-- Á¨¨‰∏ÄÂº†Âç°ÁâáÔºöËÆ∫ÊñáÊ¶ÇËø∞ -->
                    <div class="paper-card active" style="background-image: url('bg/stressed-linen.png');">
                        <h2 style="color: #ffffff;">Paper 3</h2>
                        <p style="color: #badb12;"><strong>ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation</strong></p>
                        <p style="color: #ffffff;"><strong>Published: </strong>2025-12-02</p>
                        <p><strong>Link: </strong><a href="http://arxiv.org/pdf/2512.03036" target="_blank">http://arxiv.org/pdf/2512.03036</a></p>
                        <div><div class="category-chunk">1.  <strong>üìò Topic and Domain:</strong> End-to-end video-driven binaural spatial audio generation in computer vision and audio processing.</div><div class="category-chunk">2.  <strong>üí° Previous Research and New Ideas:</strong> Based on previous video-to-mono audio generation and two-stage binaural audio synthesis; proposes novel end-to-end framework for direct binaural audio generation from video.</div><div class="category-chunk">3.  <strong>‚ùì Problem:</strong> Current methods generate spatial audio in two separate stages (mono generation then spatialization), leading to error accumulation and inconsistencies; limited datasets also constrain progress.</div><div class="category-chunk">4.  <strong>üõ†Ô∏è Methods:</strong> Introduces ViSAudio framework with dual-branch audio generation and conditional spacetime module, along with BiAudio dataset containing 97K video-binaural pairs with diverse camera motions.</div><div class="category-chunk">5.  <strong>üìä Results and Evaluation:</strong> Outperformed existing methods on both objective metrics and subjective evaluations, demonstrating better spatial impression, audio-visual consistency, and adaptation to viewpoint changes.</div></div>
                    </div>
                    
                    <!-- Á¨¨‰∫åÂº†Âç°ÁâáÔºöÊµÅÁ®ãÂõæ -->
                    <div class="paper-card flowchart-card" style="background-color: white;">
                        <h2>ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation</h2>
                        <svg width="100%" viewBox="0 0 1000 800">
  <defs>
    <linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#4A90E2;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#7B68EE;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="grad2" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#50C878;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#32CD32;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="grad3" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#FF6B6B;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#FF8E53;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="grad4" x1="0%" y1="0%" x2="100%" y2="0%">
      <stop offset="0%" style="stop-color:#FFD93D;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#6BCF7F;stop-opacity:1" />
    </linearGradient>
    <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
      <feDropShadow dx="3" dy="3" stdDeviation="3" flood-color="#000000" flood-opacity="0.3"/>
    </filter>
  </defs>

  <!-- Title -->
  <text x="500" y="30" text-anchor="middle" font-family="Arial, sans-serif" font-size="24" font-weight="bold" fill="#2C3E50">
    ViSAudio: End-to-End Binaural Spatial Audio Generation
  </text>

  <!-- Input Section -->
  <g filter="url(#shadow)">
    <rect x="50" y="70" width="180" height="120" rx="15" fill="url(#grad1)" opacity="0.9"/>
    <text x="140" y="95" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Input Data
    </text>
    <text x="140" y="115" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Silent Video
    </text>
    <text x="140" y="130" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Optional Text
    </text>
    <text x="140" y="145" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      (BiAudio Dataset)
    </text>
    <text x="140" y="165" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="#E8F4FD">
      97K video-binaural pairs
    </text>
  </g>

  <!-- Feature Extraction -->
  <g filter="url(#shadow)">
    <rect x="280" y="70" width="160" height="120" rx="15" fill="url(#grad2)" opacity="0.9"/>
    <text x="360" y="95" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Feature Extraction
    </text>
    <text x="360" y="115" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      CLIP Features (F_vis, F_text)
    </text>
    <text x="360" y="130" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      Sync Features (F_sync)
    </text>
    <text x="360" y="145" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      Spatial Features (F_pe)
    </text>
    <text x="360" y="165" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      PE Spatial Encoder
    </text>
  </g>

  <!-- Conditional Spacetime Module -->
  <g filter="url(#shadow)">
    <rect x="480" y="70" width="180" height="120" rx="15" fill="url(#grad3)" opacity="0.9"/>
    <text x="570" y="95" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Conditional Spacetime
    </text>
    <text x="570" y="110" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Module
    </text>
    <text x="570" y="130" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      Spatial PE + Sync Features
    </text>
    <text x="570" y="145" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      Global Spacetime Features
    </text>
    <text x="570" y="165" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      Learnable Position Embeddings
    </text>
  </g>

  <!-- Dual-Branch Audio Generation -->
  <g filter="url(#shadow)">
    <rect x="100" y="250" width="300" height="150" rx="15" fill="url(#grad4)" opacity="0.9"/>
    <text x="250" y="275" text-anchor="middle" font-family="Arial, sans-serif" font-size="16" font-weight="bold" fill="#2C3E50">
      Dual-Branch Audio Generation
    </text>
    
    <!-- Left Branch -->
    <rect x="120" y="290" width="120" height="80" rx="10" fill="#FF9999" opacity="0.8"/>
    <text x="180" y="310" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
      Left Channel
    </text>
    <text x="180" y="325" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Flow-L (v_Œ∏^l)
    </text>
    <text x="180" y="340" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Latent x_t^l
    </text>
    <text x="180" y="355" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Spatial PE-L
    </text>

    <!-- Right Branch -->
    <rect x="260" y="290" width="120" height="80" rx="10" fill="#9999FF" opacity="0.8"/>
    <text x="320" y="310" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
      Right Channel
    </text>
    <text x="320" y="325" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Flow-R (v_Œ∏^r)
    </text>
    <text x="320" y="340" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Latent x_t^r
    </text>
    <text x="320" y="355" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Spatial PE-R
    </text>
  </g>

  <!-- Transformer Architecture -->
  <g filter="url(#shadow)">
    <rect x="500" y="250" width="400" height="150" rx="15" fill="#8E44AD" opacity="0.9"/>
    <text x="700" y="275" text-anchor="middle" font-family="Arial, sans-serif" font-size="16" font-weight="bold" fill="white">
      Transformer Architecture
    </text>
    
    <!-- Multimodal Joint Blocks -->
    <rect x="520" y="290" width="170" height="50" rx="8" fill="#E74C3C" opacity="0.8"/>
    <text x="605" y="310" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">
      Multimodal Joint Blocks
    </text>
    <text x="605" y="325" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Cross-modal Alignment
    </text>

    <!-- Single-modal Blocks -->
    <rect x="710" y="290" width="170" height="50" rx="8" fill="#3498DB" opacity="0.8"/>
    <text x="795" y="310" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" font-weight="bold" fill="white">
      Single-modal Blocks
    </text>
    <text x="795" y="325" text-anchor="middle" font-family="Arial, sans-serif" font-size="9" fill="white">
      Channel-specific Processing
    </text>

    <!-- Flow Matching -->
    <rect x="520" y="350" width="360" height="30" rx="8" fill="#F39C12" opacity="0.8"/>
    <text x="700" y="370" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" font-weight="bold" fill="white">
      Conditional Flow Matching (CFM)
    </text>
  </g>

  <!-- Audio Decoding -->
  <g filter="url(#shadow)">
    <rect x="150" y="450" width="200" height="100" rx="15" fill="url(#grad1)" opacity="0.9"/>
    <text x="250" y="475" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Audio Decoding
    </text>
    <text x="250" y="495" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      VAE Decoder
    </text>
    <text x="250" y="510" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Mel Spectrograms
    </text>
    <text x="250" y="525" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Vocoder
    </text>
  </g>

  <!-- Output -->
  <g filter="url(#shadow)">
    <rect x="400" y="450" width="200" height="100" rx="15" fill="url(#grad3)" opacity="0.9"/>
    <text x="500" y="475" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Binaural Audio Output
    </text>
    <text x="500" y="495" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Left Channel
    </text>
    <text x="500" y="510" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Right Channel
    </text>
    <text x="500" y="525" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Spatial Audio
    </text>
  </g>

  <!-- Training Objective -->
  <g filter="url(#shadow)">
    <rect x="650" y="450" width="250" height="100" rx="15" fill="url(#grad2)" opacity="0.9"/>
    <text x="775" y="475" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">
      Training Objective
    </text>
    <text x="775" y="495" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="white">
      Œ£_{a‚àà{l,r}} E_t ||v_Œ∏^a(t,C,x_t^a) - (x_1^a - x_0^a)||¬≤
    </text>
    <text x="775" y="515" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Dual-channel Flow Matching
    </text>
    <text x="775" y="530" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">
      Spatial Consistency
    </text>
  </g>

  <!-- Key Features -->
  <g filter="url(#shadow)">
    <rect x="50" y="600" width="900" height="120" rx="15" fill="#34495E" opacity="0.9"/>
    <text x="500" y="625" text-anchor="middle" font-family="Arial, sans-serif" font-size="16" font-weight="bold" fill="white">
      Key Innovations
    </text>
    
    <circle cx="150" cy="660" r="8" fill="#E74C3C"/>
    <text x="170" y="665" font-family="Arial, sans-serif" font-size="12" fill="white">
      End-to-end binaural generation (no two-stage pipeline)
    </text>
    
    <circle cx="150" cy="680" r="8" fill="#3498DB"/>
    <text x="170" y="685" font-family="Arial, sans-serif" font-size="12" fill="white">
      BiAudio dataset: 97K video-binaural pairs with camera motion
    </text>
    
    <circle cx="500" cy="660" r="8" fill="#2ECC71"/>
    <text x="520" y="665" font-family="Arial, sans-serif" font-size="12" fill="white">
      Dual-branch architecture for channel consistency
    </text>
    
    <circle cx="500" cy="680" r="8" fill="#F39C12"/>
    <text x="520" y="685" font-family="Arial, sans-serif" font-size="12" fill="white">
      Conditional spacetime module for spatial-temporal alignment
    </text>
  </g>

  <!-- Connection Lines -->
  <line x1="230" y1="130" x2="280" y2="130" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
  <line x1="440" y1="130" x2="480" y2="130" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
  <line x1="250" y1="190" x2="250" y2="250" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
  <line x1="570" y1="190" x2="700" y2="250" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
  <line x1="250" y1="400" x2="250" y2="450" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
  <line x1="350" y1="500" x2="400" y2="500" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
  <line x1="700" y1="400" x2="775" y2="450" stroke="#2C3E50" stroke-width="2" opacity="0.7"/>
</svg>
                    </div>
                </div>
                <div class="quiz-tabs">
                <div class="quiz-tab" title="Click To Open Question #1">Q1
                    <div class="quiz-popup" data-answer="Error accumulation and spatio-temporal inconsistencies">
                        <div class="quiz-question">1. What is the main limitation of traditional two-stage binaural audio generation approaches that ViSAudio aims to overcome?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="High computational cost and slow processing speed">High computational cost and slow processing speed</div><div class="quiz-choice" data-value="Error accumulation and spatio-temporal inconsistencies">Error accumulation and spatio-temporal inconsistencies</div><div class="quiz-choice" data-value="Limited ability to handle multiple audio channels">Limited ability to handle multiple audio channels</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #2">Q2
                    <div class="quiz-popup" data-answer="Diverse camera rotation trajectories">
                        <div class="quiz-question">2. What unique feature of the BiAudio dataset helps improve spatial audio generation compared to existing datasets?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="Higher audio quality recordings">Higher audio quality recordings</div><div class="quiz-choice" data-value="Larger number of indoor scenes">Larger number of indoor scenes</div><div class="quiz-choice" data-value="Diverse camera rotation trajectories">Diverse camera rotation trajectories</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                
                <div class="quiz-tab" title="Click To Open Question #3">Q3
                    <div class="quiz-popup" data-answer="It processes left and right channels independently while maintaining consistency">
                        <div class="quiz-question">3. How does ViSAudio's dual-branch architecture contribute to better binaural audio generation?</div>
                        <div class="quiz-choices"><div class="quiz-choice" data-value="It processes left and right channels independently while maintaining consistency">It processes left and right channels independently while maintaining consistency</div><div class="quiz-choice" data-value="It reduces the total number of model parameters">It reduces the total number of model parameters</div><div class="quiz-choice" data-value="It enables faster parallel processing">It enables faster parallel processing</div></div>
                        <div class="quiz-feedback"></div>
                    </div>
                </div>
                </div>
            </div>
            
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // ÂàõÂª∫ÈÅÆÁΩ©Â±Ç
            const backdrop = document.createElement('div');
            backdrop.className = 'popup-backdrop';
            document.body.appendChild(backdrop);
            
            // Ëé∑ÂèñÊâÄÊúâÈóÆÈ¢òÊ†áÁ≠æ
            const quizTabs = document.querySelectorAll('.quiz-tab');
            
            // ËÆæÁΩÆÁÇπÂáª‰∫ã‰ª∂Â§ÑÁêÜ
            quizTabs.forEach(tab => {
                const popup = tab.querySelector('.quiz-popup');
                
                // ÁÇπÂáªÊ†áÁ≠æÂàáÊç¢ÈóÆÈ¢òÂç°ÁöÑÊòæÁ§∫Áä∂ÊÄÅ
                tab.addEventListener('click', function(e) {
                    e.stopPropagation(); // ÈòªÊ≠¢‰∫ã‰ª∂ÂÜíÊ≥°
                    
                    // Â¶ÇÊûúÂΩìÂâçÈóÆÈ¢òÂç°Â∑≤ÁªèÊòæÁ§∫ÔºåÂàôÈöêËóèÂÆÉ
                    if (popup.classList.contains('active')) {
                        popup.classList.remove('active');
                        backdrop.classList.remove('active');
                    } else {
                        // ÂÖàÈöêËóèÊâÄÊúâÂÖ∂‰ªñÈóÆÈ¢òÂç°
                        document.querySelectorAll('.quiz-popup').forEach(p => {
                            p.classList.remove('active');
                        });
                        
                        // Â∞ÜÂºπÁ™óÂÜÖÂÆπÂ§çÂà∂Âà∞È°µÈù¢ÊúÄÂ§ñÂ±ÇÁöÑÂºπÁ™ó‰∏≠
                        document.body.appendChild(popup);
                        
                        // ÊòæÁ§∫ÂΩìÂâçÈóÆÈ¢òÂç°ÂíåËÉåÊôØÈÅÆÁΩ©
                        popup.classList.add('active');
                        backdrop.classList.add('active');
                    }
                });
                
                // Á°Æ‰øùÁÇπÂáªÈóÆÈ¢òÂç°ÂÜÖÈÉ®Êó∂‰∏ç‰ºöÂÖ≥Èó≠ÈóÆÈ¢òÂç°
                popup.addEventListener('click', function(e) {
                    e.stopPropagation();
                });
            });
            
            // ÁÇπÂáªÈÅÆÁΩ©Â±ÇÊàñÈ°µÈù¢‰ªª‰ΩïÂÖ∂‰ªñ‰ΩçÁΩÆÊó∂ÈöêËóèÊâÄÊúâÈóÆÈ¢òÂç°
            backdrop.addEventListener('click', closeAllPopups);
            document.addEventListener('click', closeAllPopups);
            
            function closeAllPopups() {
                document.querySelectorAll('.quiz-popup').forEach(popup => {
                    popup.classList.remove('active');
                });
                backdrop.classList.remove('active');
            }
            
            // ‰∏∫ÊØè‰∏™ÈÄâÈ°πÊ∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
            document.querySelectorAll('.quiz-choice').forEach(choice => {
                choice.addEventListener('click', function() {
                    const choiceContainer = this.closest('.quiz-choices');
                    const popupContainer = this.closest('.quiz-popup');
                    const feedbackElement = popupContainer.querySelector('.quiz-feedback');
                    const correctAnswer = popupContainer.getAttribute('data-answer');
                    
                    // ÈáçÁΩÆÊâÄÊúâÈÄâÈ°π
                    choiceContainer.querySelectorAll('.quiz-choice').forEach(c => {
                        c.classList.remove('selected', 'correct', 'incorrect');
                    });
                    
                    // Ê†áËÆ∞ÂΩìÂâçÈÄâÈ°π‰∏∫Â∑≤ÈÄâ
                    this.classList.add('selected');
                    
                    // Ê£ÄÊü•ÊòØÂê¶Ê≠£Á°Æ
                    if (this.getAttribute('data-value') === correctAnswer) {
                        this.classList.add('correct');
                        feedbackElement.textContent = '‚úîÔ∏è CorrectÔºÅ';
                        feedbackElement.classList.add('correct');
                        feedbackElement.classList.remove('incorrect');
                    } else {
                        this.classList.add('incorrect');
                        feedbackElement.textContent = '‚ùå WrongÔºÅ';
                        feedbackElement.classList.add('incorrect');
                        feedbackElement.classList.remove('correct');
                    }
                    
                    feedbackElement.style.display = 'block';
                });
            });
            
            // Âç°ÁâáËΩÆÊí≠ÂäüËÉΩ - Êñ∞Â¢û
            const cardDecks = document.querySelectorAll('.card-deck');
            
            cardDecks.forEach(cardDeck => {
                const cards = cardDeck.querySelectorAll('.paper-card');
                const counter = cardDeck.querySelector('.card-counter');
                let currentIndex = 0;
                const totalCards = cards.length;
                
                // Êõ¥Êñ∞ËÆ°Êï∞Âô®ÊòæÁ§∫
                function updateCounter() {
                    if (counter) {
                        counter.textContent = `${currentIndex + 1}/${totalCards}`;
                    }
                }
                
                // ÊòæÁ§∫ÊåáÂÆöÁ¥¢ÂºïÁöÑÂç°Áâá
                function showCard(index) {
                    // Â§ÑÁêÜÂæ™ÁéØ
                    if (index >= totalCards) index = 0;
                    if (index < 0) index = totalCards - 1;
                    
                    // Êõ¥Êñ∞ÂΩìÂâçÁ¥¢Âºï
                    currentIndex = index;
                    
                    // Êõ¥Êñ∞Âç°ÁâáÊòæÁ§∫
                    cards.forEach((card, i) => {
                        if (i === currentIndex) {
                            card.classList.add('active');
                        } else {
                            card.classList.remove('active');
                        }
                    });
                    
                    // Êõ¥Êñ∞ËÆ°Êï∞Âô®
                    updateCounter();
                }
                
                // ‰∏ã‰∏ÄÂº†Âç°Áâá
                function nextCard(e) {
                    e.stopPropagation(); // Èò≤Ê≠¢‰∫ã‰ª∂ÂÜíÊ≥°ÂØºËá¥ÈóÆÈ¢òÂç°ÂÖ≥Èó≠
                    showCard(currentIndex + 1);
                }
                
                // ‰∏∫Âç°ÁâáÂÆπÂô®Ê∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
                cardDeck.addEventListener('click', function(e) {
                    // Ê£ÄÊü•ÁÇπÂáªÊòØÂê¶ÂèëÁîüÂú®ÊµÅÁ®ãÂõæÂç°ÁâáÂÜÖÈÉ®ÁöÑÊªöÂä®Âå∫Âüü
                    // Â¶ÇÊûúÊòØÂú®ÊªöÂä®Êù°‰∏äÁÇπÂáªÔºå‰∏çÂàáÊç¢Âç°Áâá
                    const targetCard = e.target.closest('.paper-card');
                    if (targetCard && targetCard.classList.contains('flowchart-card')) {
                        // ËÆ°ÁÆóÁÇπÂáª‰ΩçÁΩÆÊòØÂê¶Âú®ÊªöÂä®Êù°Âå∫Âüü
                        const rect = targetCard.getBoundingClientRect();
                        const isScrollbarClick = 
                            (e.clientY >= rect.top && e.clientY <= rect.bottom && e.clientX >= rect.right - 20 && e.clientX <= rect.right) ||
                            (e.clientX >= rect.left && e.clientX <= rect.right && e.clientY >= rect.bottom - 20 && e.clientY <= rect.bottom);
                        
                        if (!isScrollbarClick) {
                            nextCard(e);
                        }
                    } else {
                        nextCard(e);
                    }
                });
                
                // ÈîÆÁõòÂØºËà™
                document.addEventListener('keydown', (e) => {
                    if (e.key === 'ArrowRight') {
                        showCard(currentIndex + 1);
                    } else if (e.key === 'ArrowLeft') {
                        showCard(currentIndex - 1);
                    }
                });
            });
        });
    </script>
</body>
</html>
