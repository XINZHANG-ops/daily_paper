{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.transparenttextures.com/\n",
    "\n",
    "https://xinzhang-ops.github.io/daily_paper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "1. üìò Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. 2. üí° Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. 3. ‚ùì Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. 4. üõ†Ô∏è Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. 5. üìä Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. üìò Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. 2. üí° Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. 3. ‚ùì Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. 4. üõ†Ô∏è Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. 5. üìä Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_formatted = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', content)\n",
    "content_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '1. üìò Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. ',\n",
       " '2. üí° Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. ',\n",
       " '3. ‚ùì Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. ',\n",
       " '4. üõ†Ô∏è Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. ',\n",
       " '5. üìä Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'(?=\\d+\\.\\s*[üìòüí°‚ùìüõ†Ô∏èüìä])', content_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "from string import Template \n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# HTML Ê®°ÊùøÔºö‰∏ªÈ°µÈù¢Ôºå‰ΩøÁî® $ ‰Ωú‰∏∫Âç†‰ΩçÁ¨¶\n",
    "INDEX_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Daily Paper</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            color: #333;\n",
    "        }\n",
    "        ul {\n",
    "            list-style: none;\n",
    "            padding: 0;\n",
    "        }\n",
    "        li {\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        a {\n",
    "            text-decoration: none;\n",
    "            color: #1a73e8;\n",
    "        }\n",
    "        a:hover {\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Daily Paper</h1>\n",
    "    <ul>\n",
    "        $date_links\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML Ê®°ÊùøÔºöÂ≠êÈ°µÈù¢Ôºå‰ΩøÁî® $ ‰Ωú‰∏∫Âç†‰ΩçÁ¨¶\n",
    "SUBPAGE_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>$date ËÆ∫ÊñáÊé®ÈÄÅ</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        .paper-card {\n",
    "            background-color: #f9f9f9;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "            padding: 15px;\n",
    "            margin-bottom: 20px;\n",
    "            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */\n",
    "        }\n",
    "        .paper-card:hover {\n",
    "            transform: translateY(-5px); /* Lift effect on hover */\n",
    "            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */\n",
    "        }\n",
    "        .paper-card h2 {\n",
    "            margin: 0 0 10px;\n",
    "            font-size: 1.2em;\n",
    "        }\n",
    "        .paper-card p {\n",
    "            margin: 5px 0;\n",
    "        }\n",
    "        .paper-card a {\n",
    "            color: #1a73e8;\n",
    "            text-decoration: none;\n",
    "        }\n",
    "        .paper-card a:hover {\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "        .category-chunk {\n",
    "            padding: 10px;\n",
    "            margin: 5px 0;\n",
    "            border-radius: 5px;\n",
    "            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */\n",
    "        }\n",
    "        .category-chunk:hover {\n",
    "            transform: translateY(-3px); /* Slightly smaller lift for categories */\n",
    "            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */\n",
    "        }\n",
    "        .category-chunk:nth-child(1) { /* 1. Topic and Domain */\n",
    "            background-color: #d3e3fd; /* Blue */\n",
    "        }\n",
    "        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */\n",
    "            background-color: #e6d6fa; /* Purple */\n",
    "        }\n",
    "        .category-chunk:nth-child(3) { /* 3. Problem */\n",
    "            background-color: #d4f8d9; /* Green */\n",
    "        }\n",
    "        .category-chunk:nth-child(4) { /* 4. Methods */\n",
    "            background-color: #ffd7d5; /* Pink */\n",
    "        }\n",
    "        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */\n",
    "            background-color: #d3e3fd; /* Reuse Blue */\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>$date ËÆ∫ÊñáÊé®ÈÄÅ</h1>\n",
    "    $paper_content\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_categories(text):\n",
    "    \"\"\"\n",
    "    Extract the 5 categories and their content from a formatted text string.\n",
    "    \n",
    "    Each category is identified by its unique emoji (üìò, üí°, ‚ùì, üõ†Ô∏è, üìä) regardless of the\n",
    "    exact title text. The function maps these to standardized category names.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the 5 categories\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with standardized category titles as keys and their content as values\n",
    "    \"\"\"\n",
    "    # Define patterns based on emojis only, not the category titles\n",
    "    patterns = [\n",
    "        (r'\\d+\\.\\s+\\*\\*üìò.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"üìò Topic and Domain\",),\n",
    "        (r'\\d+\\.\\s+\\*\\*üí°.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"üí° Previous Research and New Ideas\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*‚ùì.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"‚ùì Problem\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*üõ†Ô∏è.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"üõ†Ô∏è Methods\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*üìä.*?\\*\\*\\s+(.*?)(?=\\n\\n|\\Z)', \"üìä Results and Evaluation\")\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store results\n",
    "    results = []\n",
    "    \n",
    "    # Apply each pattern and store results with standardized category names\n",
    "    for pattern, category_name in patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            results.append((category_name, match.group(1).strip()))    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_paper_html(articles):\n",
    "    \"\"\"ÁîüÊàêÂ≠êÈ°µÈù¢ÁöÑËÆ∫ÊñáÂÜÖÂÆπ HTMLÔºå‰∏é Google Chat Êé®ÈÄÅÂÜÖÂÆπ‰∏ÄËá¥\"\"\"\n",
    "    # logger.debug(articles)\n",
    "    paper_html = \"\"\n",
    "    for idx, article in enumerate(articles):\n",
    "        title = article.get('title', 'No Title')\n",
    "        published_at = article.get('published_at', 'No Date')\n",
    "        url = article.get('url', '#')\n",
    "        content = article.get('content', 'No Content')\n",
    "        categories = extract_categories(content)\n",
    "        # ‰∏∫ÊØè‰∏™Á±ªÂà´Ê∑ªÂä† div ÂíåÊ†∑Âºè\n",
    "        content_html = \"\"\n",
    "        for idx, (cat, cat_content) in enumerate(categories):\n",
    "            content_html += f\"\"\"<div class=\"category-chunk\">{idx+1}.  <strong>{cat}:</strong> {cat_content}</div>\"\"\"\n",
    "\n",
    "        paper_html += f\"\"\"\n",
    "        <div class=\"paper-card\">\n",
    "            <h2>Paper: {idx+1}</h2>\n",
    "            <p><strong>{title}</strong></p>\n",
    "            <p><strong>Published: </strong>{published_at}</p>\n",
    "            <p><strong>Link: </strong><a href=\"{url}\" target=\"_blank\">{url}</a></p>\n",
    "            <div>{content_html}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    return paper_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [{'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'published_at': '2025-03-19', 'url': 'http://arxiv.org/pdf/2503.15417', 'content': '1.  **üìò Topic and Domain:** The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.\\n\\n5.  **üìä Results and Evaluation:** FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.\\n'}, {'title': 'Optimizing Decomposition for Optimal Claim Verification', 'published_at': '2025-03-19', 'url': 'http://arxiv.org/pdf/2503.15354', 'content': 'Here\\'s a concise analysis of the paper based on your requested format:\\n\\n1.  **üìò Topic and Domain:** The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don\\'t generate subclaims with optimal atomicity for downstream verification.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.\\n\\n5.  **üìä Results and Evaluation:** The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.\\n'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.  **üìò Topic and Domain:** The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.\\n\\n5.  **üìä Results and Evaluation:** FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a concise analysis of the paper based on your requested format:\\n\\n1.  **üìò Topic and Domain:** The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don\\'t generate subclaims with optimal atomicity for downstream verification.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.\\n\\n5.  **üìä Results and Evaluation:** The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <div class=\"paper-card\">\n",
      "            <h2>Paper: 5</h2>\n",
      "            <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>\n",
      "            <p><strong>Published: </strong>2025-03-19</p>\n",
      "            <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15417\" target=\"_blank\">http://arxiv.org/pdf/2503.15417</a></p>\n",
      "            <div><div class=\"category-chunk\">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.</div><div class=\"category-chunk\">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.</div><div class=\"category-chunk\">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.</div><div class=\"category-chunk\">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.</div><div class=\"category-chunk\">5.  <strong>üìä Results and Evaluation:</strong> FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.</div></div>\n",
      "        </div>\n",
      "        \n",
      "        <div class=\"paper-card\">\n",
      "            <h2>Paper: 5</h2>\n",
      "            <p><strong>Optimizing Decomposition for Optimal Claim Verification</strong></p>\n",
      "            <p><strong>Published: </strong>2025-03-19</p>\n",
      "            <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15354\" target=\"_blank\">http://arxiv.org/pdf/2503.15354</a></p>\n",
      "            <div><div class=\"category-chunk\">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.</div><div class=\"category-chunk\">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.</div><div class=\"category-chunk\">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don't generate subclaims with optimal atomicity for downstream verification.</div><div class=\"category-chunk\">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.</div><div class=\"category-chunk\">5.  <strong>üìä Results and Evaluation:</strong> The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.</div></div>\n",
      "        </div>\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(generate_paper_html(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"\"\"\n",
    "<div class=\"paper-card\">\n",
    "    <h2>Paper: 1</h2>\n",
    "    <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>\n",
    "    <p><strong>Published: </strong>2025-03-19</p>\n",
    "    <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15417\" target=\"_blank\">http://arxiv.org/pdf/2503.15417</a></p>\n",
    "    <div>\n",
    "        <div class=\"category-chunk\">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on improving the temporal quality of video generation, specifically addressing temporal coherence and diversity, within the domain of computer vision and deep learning.</div>\n",
    "        <div class=\"category-chunk\">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization), and proposes FLUX FLOW, a novel data-level temporal augmentation strategy.</div>\n",
    "        <div class=\"category-chunk\">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion) and limited temporal diversity in videos generated by current video generation models.</div>\n",
    "        <div class=\"category-chunk\">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used FLUX FLOW, which involves frame-level and block-level temporal perturbations (random shuffling of frames or blocks of frames) during the training of video generation models.</div>\n",
    "        <div class=\"category-chunk\">5.  <strong>üìä Results and Evaluation:</strong> FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and various VBench temporal and frame-wise quality scores, and was further supported by a user study.</div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "summaries = []\n",
    "with open('summaries.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        summaries.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GENAI_GATEWAY_API_KEY = os.getenv(\"GENAI_GATEWAY_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = GENAI_GATEWAY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from geotab_genai.genai_gateway_client import GenaiGatewayClient\n",
    "\n",
    "\n",
    "claude35_sonnet_v2 = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='anthropics',\n",
    "    chat_model='claude-3-5-sonnet-v2',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "\n",
    "claude37_sonnet = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='anthropics',\n",
    "    chat_model='claude-3-7-sonnet',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "gemini_20_pro = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='vertex-ai',\n",
    "    chat_model='gemini-2.0-pro',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "gemini_25_pro = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='vertex-ai',\n",
    "    chat_model='gemini-2.5-pro',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "\n",
    "model_map = {\n",
    "    'claude35': claude35_sonnet_v2,\n",
    "    # 'claude35_haiku': claude35_haiku,\n",
    "    'claude37': claude37_sonnet,\n",
    "    'gemini_20_pro': gemini_20_pro,\n",
    "    'gemini_25_pro': gemini_25_pro,\n",
    "}\n",
    "\n",
    "\n",
    "def model_response(prompt, model_name, max_tokens=4096):\n",
    "    model = model_map[model_name]\n",
    "    version = None\n",
    "    if model_name == 'gemini_20_flash':\n",
    "        version = '001'\n",
    "    if model_name == 'gemini_25_pro':\n",
    "        version = 'exp-03-25'\n",
    "    response = model.create_message(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        provider=model._provider,\n",
    "        model=model._chat_model,\n",
    "        version=version,\n",
    "\n",
    "    )['message']['content']\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_article.txt', 'r') as f:\n",
    "    test_article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a research assistant. You job is to help me to create a flow chart of the paper content. \n",
    "Since it is about the workflow of the paper, your focus is the method applied in the paper.\n",
    "\n",
    "Your should contain your answer in a SVG format as following format:\n",
    "<format>\n",
    "you should have your output with this specific <svg> tag.\n",
    "\n",
    "<svg width=\"100%\" viewBox=\"0 0 1000 800\">\n",
    "Here are the content you can create freely, use all shapes, text format or styles as you like.\n",
    "Try to be creative, and make it look good and colorful.\n",
    "Try use less arrows, since the arrow you give tends to be messy.\n",
    "</svg>\n",
    "\n",
    "</format>\n",
    "\n",
    "Here is the content of the paper:\n",
    "<content>\n",
    "{article_content}\n",
    "</content>\n",
    "\n",
    "Now please give me the SVG format of the flow chart, you should only give me the SVG format directly, do not output backticks for formatting, no other text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = system_prompt.format(article_content=test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_response(prompt, 'gemini_25_pro', max_tokens=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```svg\n",
      "<svg width=\"100%\" viewBox=\"0 0 1000 800\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "\n",
      "  <!-- Define styles -->\n",
      "  <style>\n",
      "    .title { font-family: sans-serif; font-size: 28px; font-weight: bold; text-anchor: middle; fill: #333; }\n",
      "    .phase-title { font-family: sans-serif; font-size: 20px; font-weight: bold; text-anchor: middle; fill: #222; }\n",
      "    .step-text { font-family: sans-serif; font-size: 14px; text-anchor: middle; fill: #111; }\n",
      "    .output-text { font-family: sans-serif; font-size: 15px; font-weight: bold; text-anchor: middle; fill: #0056b3; }\n",
      "    .arrow-head { fill: #555; }\n",
      "    .arrow-line { stroke: #555; stroke-width: 2; }\n",
      "  </style>\n",
      "\n",
      "  <!-- Define arrow marker -->\n",
      "  <defs>\n",
      "    <marker id=\"arrow\" markerWidth=\"10\" markerHeight=\"7\" refX=\"0\" refY=\"3.5\" orient=\"auto\">\n",
      "      <polygon points=\"0 0, 10 3.5, 0 7\" class=\"arrow-head\" />\n",
      "    </marker>\n",
      "  </defs>\n",
      "\n",
      "  <!-- Title -->\n",
      "  <text x=\"500\" y=\"40\" class=\"title\">DeepSeek LLM Methodology Flowchart</text>\n",
      "\n",
      "  <!-- Phase 1: Scaling Law Investigation -->\n",
      "  <g id=\"phase1\">\n",
      "    <rect x=\"150\" y=\"80\" width=\"700\" height=\"150\" rx=\"15\" ry=\"15\" fill=\"#e0f7fa\" stroke=\"#b2ebf2\" stroke-width=\"1\"/>\n",
      "    <text x=\"500\" y=\"110\" class=\"phase-title\">Phase 1: Scaling Law Investigation</text>\n",
      "    <rect x=\"180\" y=\"130\" width=\"200\" height=\"70\" rx=\"5\" ry=\"5\" fill=\"#b2ebf2\" />\n",
      "    <text x=\"280\" y=\"155\" class=\"step-text\">Study Hyperparameter</text>\n",
      "    <text x=\"280\" y=\"175\" class=\"step-text\">(Batch Size, LR) Scaling</text>\n",
      "    <rect x=\"400\" y=\"130\" width=\"200\" height=\"70\" rx=\"5\" ry=\"5\" fill=\"#b2ebf2\" />\n",
      "    <text x=\"500\" y=\"155\" class=\"step-text\">Study Optimal Model/</text>\n",
      "    <text x=\"500\" y=\"175\" class=\"step-text\">Data Scaling Allocation</text>\n",
      "    <rect x=\"620\" y=\"130\" width=\"200\" height=\"70\" rx=\"5\" ry=\"5\" fill=\"#b2ebf2\" />\n",
      "    <text x=\"720\" y=\"155\" class=\"step-text\">Analyze Dataset Impact</text>\n",
      "    <text x=\"720\" y=\"175\" class=\"step-text\">on Scaling Laws</text>\n",
      "  </g>\n",
      "\n",
      "  <!-- Arrow 1 -->\n",
      "  <line x1=\"500\" y1=\"230\" x2=\"500\" y2=\"260\" class=\"arrow-line\" marker-end=\"url(#arrow)\" />\n",
      "  <text x=\"520\" y=\"250\" class=\"step-text\" style=\"font-style: italic; fill: #555;\">Guides</text>\n",
      "\n",
      "  <!-- Phase 2: Pre-training -->\n",
      "  <g id=\"phase2\">\n",
      "    <rect x=\"150\" y=\"270\" width=\"700\" height=\"180\" rx=\"15\" ry=\"15\" fill=\"#e8f5e9\" stroke=\"#c8e6c9\" stroke-width=\"1\"/>\n",
      "    <text x=\"500\" y=\"300\" class=\"phase-title\">Phase 2: Pre-training</text>\n",
      "    <rect x=\"180\" y=\"320\" width=\"200\" height=\"70\" rx=\"5\" ry=\"5\" fill=\"#c8e6c9\" />\n",
      "    <text x=\"280\" y=\"345\" class=\"step-text\">Data Collection</text>\n",
      "    <text x=\"280\" y=\"365\" class=\"step-text\">(2T+ Tokens, En/Zh)</text>\n",
      "    <rect x=\"400\" y=\"320\" width=\"200\" height=\"70\" rx=\"5\" ry=\"5\" fill=\"#c8e6c9\" />\n",
      "    <text x=\"500\" y=\"345\" class=\"step-text\">Model Architecture</text>\n",
      "    <text x=\"500\" y=\"365\" class=\"step-text\">(LLaMA-based, Multi-step LR)</text>\n",
      "    <rect x=\"620\" y=\"320\" width=\"200\" height=\"70\" rx=\"5\" ry=\"5\" fill=\"#c8e6c9\" />\n",
      "    <text x=\"720\" y=\"345\" class=\"step-text\">Train Models</text>\n",
      "    <text x=\"720\" y=\"365\" class=\"step-text\">(e.g., 7B, 67B)</text>\n",
      "    <!-- Output -->\n",
      "    <ellipse cx=\"500\" cy=\"425\" rx=\"150\" ry=\"20\" fill=\"#a5d6a7\"/>\n",
      "    <text x=\"500\" y=\"430\" class=\"output-text\">Output: DeepSeek LLM Base Models</text>\n",
      "  </g>\n",
      "\n",
      "  <!-- Arrow 2 -->\n",
      "  <line x1=\"500\" y1=\"450\" x2=\"500\" y2=\"480\" class=\"arrow-line\" marker-end=\"url(#arrow)\" />\n",
      "\n",
      "  <!-- Phase 3: Alignment -->\n",
      "  <g id=\"phase3\">\n",
      "    <rect x=\"150\" y=\"490\" width=\"700\" height=\"150\" rx=\"15\" ry=\"15\" fill=\"#fff3e0\" stroke=\"#ffe0b2\" stroke-width=\"1\"/>\n",
      "    <text x=\"500\" y=\"520\" class=\"phase-title\">Phase 3: Alignment</text>\n",
      "    <rect x=\"180\" y=\"540\" width=\"270\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#ffe0b2\" />\n",
      "    <text x=\"315\" y=\"565\" class=\"step-text\">SFT Data Collection (>1M Instances)</text>\n",
      "    <rect x=\"470\" y=\"540\" width=\"170\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#ffe0b2\" />\n",
      "    <text x=\"555\" y=\"565\" class=\"step-text\">Supervised Fine-Tuning (SFT)</text>\n",
      "    <rect x=\"660\" y=\"540\" width=\"190\" height=\"40\" rx=\"5\" ry=\"5\" fill=\"#ffe0b2\" />\n",
      "    <text x=\"755\" y=\"565\" class=\"step-text\">Direct Preference Opt. (DPO)</text>\n",
      "    <!-- Output -->\n",
      "    <ellipse cx=\"500\" cy=\"615\" rx=\"150\" ry=\"20\" fill=\"#ffcc80\"/>\n",
      "    <text x=\"500\" y=\"620\" class=\"output-text\">Output: DeepSeek Chat Models</text>\n",
      "  </g>\n",
      "\n",
      "  <!-- Arrow 3 -->\n",
      "  <line x1=\"500\" y1=\"640\" x2=\"500\" y2=\"670\" class=\"arrow-line\" marker-end=\"url(#arrow)\" />\n",
      "\n",
      "  <!-- Phase 4: Evaluation -->\n",
      "  <g id=\"phase4\">\n",
      "    <rect x=\"150\" y=\"680\" width=\"700\" height=\"100\" rx=\"15\" ry=\"15\" fill=\"#ede7f6\" stroke=\"#d1c4e9\" stroke-width=\"1\"/>\n",
      "    <text x=\"500\" y=\"705\" class=\"phase-title\">Phase 4: Evaluation</text>\n",
      "    <rect x=\"180\" y=\"720\" width=\"310\" height=\"45\" rx=\"5\" ry=\"5\" fill=\"#d1c4e9\" />\n",
      "    <text x=\"335\" y=\"738\" class=\"step-text\">Evaluate Base Models</text>\n",
      "    <text x=\"335\" y=\"755\" class=\"step-text\">(Benchmarks: Code, Math, Reasoning)</text>\n",
      "    <rect x=\"510\" y=\"720\" width=\"310\" height=\"45\" rx=\"5\" ry=\"5\" fill=\"#d1c4e9\" />\n",
      "    <text x=\"665\" y=\"738\" class=\"step-text\">Evaluate Chat Models</text>\n",
      "    <text x=\"665\" y=\"755\" class=\"step-text\">(Benchmarks, Open-Ended, Safety)</text>\n",
      "  </g>\n",
      "\n",
      "</svg>\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SERVER_IP = \"192.168.1.100\"  # Change to your server's IP\n",
    "\n",
    "response = requests.post(\n",
    "    f\"http://{SERVER_IP}:5000/chat\", \n",
    "    json={\"message\": \"Hello Claude\", \"max_tokens\": 8192}\n",
    ")\n",
    "\n",
    "print(response.json()[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.arxiv_utils import (\n",
    "    download_paper_text\n",
    ")\n",
    "\n",
    "result = download_paper_text(\"2502.18417\")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=300)\n",
    "texts = text_splitter.split_text(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=300)\n",
    "texts = text_splitter.split_text(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "\n",
    "def get_arxiv_abstract(arxiv_id: str) -> str:\n",
    "    \"\"\"Get abstract from arXiv API.\"\"\"\n",
    "    try:\n",
    "        api_url = f\"http://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
    "        response = requests.get(api_url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            root = ET.fromstring(response.text)\n",
    "            ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "            entry = root.find('.//atom:entry', ns)\n",
    "            if entry:\n",
    "                abstract = entry.find('.//atom:summary', ns)\n",
    "                if abstract is not None:\n",
    "                    text = abstract.text.strip()\n",
    "                    text = ' '.join(text.split())\n",
    "                    if len(text) > 150:\n",
    "                        text = text[:150] + \"...\"\n",
    "                    return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching abstract for {arxiv_id}: {e}\")\n",
    "    return \"Abstract not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract not available'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_arxiv_abstract(\"2502.18417\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2465\n",
      "arXiv:2502.18417v4  [cs.CV]  9 Jun 2025\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Alexander Groshev‚àó1 Anastasiia Iashchenko‚àó1 Pavel Paramonov‚àó1 Denis Dimitrov‚àó‚àó1,2 Andrey Kuznetsov‚àó‚àó1,2\n",
      "1Sber AI 2AIRI\n",
      "Source Target Result\n",
      "Figure 1. Results of GHOST 2.0 model on the task of head swap.\n",
      "Head from source image is animated in correspondence with the\n",
      "target motion and blended into target background.\n",
      "Abstract\n",
      "While the task of face swapping has recently\n",
      "gained attention in the research community, a re-\n",
      "lated problem of head swapping remains largely\n",
      "unexplored. In addition to skin color transfer,\n",
      "head swap poses extra challenges, such as the\n",
      "need to preserve structural information of the\n",
      "whole head during synthesis and inpaint gaps be-\n",
      "tween swapped head and background. In this pa-\n",
      "per, we address these concerns with GHOST 2.0,\n",
      "which consists of two problem-specific modules.\n",
      "First, we introduce enhanced Aligner model for\n",
      "head reenactment, which preserves identity infor-\n",
      "mation at multiple scales and is robust to extreme\n",
      "pose variations. Secondly, we use a Blender mod-\n",
      "ule that seamlessly integrates the reenacted head\n",
      "into the target background by transferring skin\n",
      "color and inpainting mismatched regions. Both\n",
      "modules outperform the baselines on the corre-\n",
      "sponding tasks, allowing to achieve state-of-the-\n",
      "art results in head swapping. We also tackle com-\n",
      "plex cases, such as large difference in hair styles\n",
      "of source and target.\n",
      "1. Introduction\n",
      "The use of virtual humans has long passed purely entertain-\n",
      "ment scope, finding applications in movie and advertisement\n",
      "composition, virtual try-on, deepfake detection, and portrait\n",
      "editing. Head swap, the task of replacing head in the target\n",
      "image with head from the source image, plays an integral\n",
      "role for these use-cases. It requires reenacting source head\n",
      "with the driving one, and seamlessly integrating the result\n",
      "with the target‚Äòs background. Active research is conducted\n",
      "on generation and animation of head avatars, mainly using\n",
      "warping, generative adversarial networks (GANs) and diffu-\n",
      "sion models (Siarohin et al., 2019; Drobyshev et al., 2022;\n",
      "Zhang et al.; Kirschstein et al., 2023). However, the prob-\n",
      "lem of blending the generated head with the surrounding\n",
      "environment remains largely unaddressed.\n",
      "Recently, a noticeable progress in a related task of face\n",
      "swapping has been made (Chen et al., 2020; Wang et al.,\n",
      "2021; Groshev et al., 2022; Zhu et al., 2021; Zhao et al.,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "632\n",
      "lem of blending the generated head with the surrounding\n",
      "environment remains largely unaddressed.\n",
      "Recently, a noticeable progress in a related task of face\n",
      "swapping has been made (Chen et al., 2020; Wang et al.,\n",
      "2021; Groshev et al., 2022; Zhu et al., 2021; Zhao et al.,\n",
      "2023). The task requires source identity preservation and\n",
      "reenactment only within facial region. Since information\n",
      "about head shape and hair is omitted, face swap is a less\n",
      "complex problem than swapping of the whole head for sev-\n",
      "‚àó indicates equal contribution\n",
      "‚àó‚àó Corresponding authors: Andrey Kuznetsov\n",
      "<kuznetsov@airi.net>, Denis Dimitrov <dimitrov@airi.net>\n",
      "1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2498\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "eral reasons. First, face recognition models (Cao et al., 2018;\n",
      "Deng et al., 2019; Alansari et al., 2023), usually used to con-\n",
      "trol identity transfer, are trained on narrow face crops. Thus,\n",
      "they cannot be used to embed identity information about\n",
      "the whole head, requiring consideration of other models\n",
      "for this purpose. Secondly, head generation also involves\n",
      "generation of hair, which is a complex task due to its high-\n",
      "frequency texture and diverse styles. Finally, variations in\n",
      "face shapes are usually smaller than variations in head forms.\n",
      "This implies potentially larger regions for inpainting as a\n",
      "result of head replacement. And, similarly to face swap, the\n",
      "skin color of target should be carefully transferred to the\n",
      "generated head for a realistic result.\n",
      "Currently there are only few papers that address the task\n",
      "of head swap. The first steps were taken by DeepFace-\n",
      "Lab (Perov et al., 2020), which requires finetuning for each\n",
      "source identity. Next work, StylePoseGAN (Sarkar et al.,\n",
      "2021), tends to modify background and skin color of the\n",
      "target. A seminal work that partially tackles the aforemen-\n",
      "tioned issues is HeSer (Shu et al., 2022). It splits the gen-\n",
      "eration process into head reenactment stage and reference-\n",
      "based blending stage. This work was followed by diffusion-\n",
      "based approaches (Baliah et al., 2025; Han et al., 2023).\n",
      "However, latest methods still face issues concerning quality\n",
      "of head generation and color transfer.\n",
      "In this paper we present GHOST 2.0, a one-shot high-fidelity\n",
      "framework for head swap. It consists of two modules:\n",
      "Aligner for head reenactment, and Blender for natural in-\n",
      "painting of the result into the target background. Aligner\n",
      "includes a set of encoders to obtain information on source\n",
      "identity and target motion, which is then used to condition\n",
      "StyleGAN-based generator to obtained aligned head. The\n",
      "choice of architecture ensures good in-the-wild performance,\n",
      "supporting high-quality generation even in extreme poses.\n",
      "Blender creates references for head color transfer and back-\n",
      "ground inpainting, and uses them to stitch the generated\n",
      "head with the background via blending UNet (Ronneberger\n",
      "et al., 2015). For color transfer, the composition of refer-\n",
      "ence is based on correlation between respective head parts\n",
      "of generated and driving head. Background reference is\n",
      "determined by masking the potential gaps arising due to\n",
      "differences in head shape. These references are used to\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2475\n",
      "et al., 2015). For color transfer, the composition of refer-\n",
      "ence is based on correlation between respective head parts\n",
      "of generated and driving head. Background reference is\n",
      "determined by masking the potential gaps arising due to\n",
      "differences in head shape. These references are used to\n",
      "condition UNet which outputs final result.\n",
      "Our contributions are the following:\n",
      "‚Ä¢ We introduce a new model for head reenactment that\n",
      "surpasses competitors on a range of metrics. In contrast\n",
      "to previous works that focus on face region only, it\n",
      "generates full human head, accounting both for low-\n",
      "frequency and high-frequency details and preserving\n",
      "identity at different scales. Moreover, when trained\n",
      "on the same dataset as other methods, it gives superior\n",
      "results on generation in extreme poses.\n",
      "‚Ä¢ We increase quality of final generations by improv-\n",
      "ing robustness of blending module to corner-case sce-\n",
      "narios. Specifically, these include the situation when\n",
      "hair styles of source and target are extremely differ-\n",
      "ent, which previously led to inconsistent hair transfer.\n",
      "Additionally, we consider the case when target head\n",
      "lacks color references for the source one, resulting in\n",
      "poor color transfer. Finally, we enhance background\n",
      "inpainting to achieve more seamless blending.\n",
      "‚Ä¢ We trained a new segmentation model specifically for\n",
      "the head swap task. Unlike existing segmentation mod-\n",
      "els, we have annotated the data in such a way as to\n",
      "separate beard and facial hair into a separate class,\n",
      "which is necessary for correct color transfer.\n",
      "2. Related Work\n",
      "Face swap There are a large number of face-swapping\n",
      "methods. Conceptually, they can be divided into several\n",
      "different groups. Methods from the first group (Chen et al.,\n",
      "2020; Wang et al., 2021; Groshev et al., 2022) extract the\n",
      "identity vector and some other features of the source face\n",
      "and use a generative model to blend them with the attributes\n",
      "of the target. Often, such models rely on the ArcFace (Deng\n",
      "et al., 2019) model and a 3D shape-aware identity extractor,\n",
      "which allows encoding the 3D geometry of the face. There\n",
      "are also approaches (Zhu et al., 2021) based on StyleGAN2\n",
      "(Karras et al., 2020). They propose inverting the source\n",
      "and driving images into the latent space and feeding them\n",
      "into the StyleGAN2 generator to perform the swap. This\n",
      "approach allows for higher-resolution results but is sensitive\n",
      "to input data and does not perform well on strong rotations\n",
      "or small details of images. With the development of dif-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1372\n",
      "and driving images into the latent space and feeding them\n",
      "into the StyleGAN2 generator to perform the swap. This\n",
      "approach allows for higher-resolution results but is sensitive\n",
      "to input data and does not perform well on strong rotations\n",
      "or small details of images. With the development of dif-\n",
      "fusion models, approaches to face replacement using this\n",
      "method have emerged (Zhao et al., 2023; Chen et al., 2024).\n",
      "Diffusion models allow for high-quality results, but they are\n",
      "typically slow and require significant computational power\n",
      "and sufficient VRAM.\n",
      "Head swap The task of head swap is covered by a limited\n",
      "number of works. DeepFaceLab (Perov et al., 2020) is the\n",
      "first approach enabling this capability. However, it requires\n",
      "a large amount of source data for training, and poorly per-\n",
      "forms color transfer and fusion of generated head with the\n",
      "background. StylePoseGAN (Sarkar et al., 2021) performs\n",
      "head swap by conditioning StyleGAN (Karras et al., 2020)\n",
      "on pose and combined texture map, with body parts taken\n",
      "from target and head ‚Äî from source. Still, it tends to mod-\n",
      "ify the background and skin color of the target. HeSer (Shu\n",
      "et al., 2022) tackles these issues by designing a separate\n",
      "module for each task. First, it uses head reenactment model\n",
      "based on (Burkov et al., 2020) to align source head with\n",
      "target in pose and expression. In the second stage a refer-\n",
      "2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2479\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "ence is created for skin color transfer based on correlation\n",
      "between pixels from the same head parts. Together with\n",
      "background inpainting prior, it is used to condition blending\n",
      "UNet (Ronneberger et al., 2015) which fuses the head with\n",
      "background. While this method outperforms the competi-\n",
      "tors, it suffers from identity leakage of target and is unable\n",
      "to color head parts present in source image but absent in the\n",
      "driving one. While previous methods are based on GANs\n",
      "(Goodfellow et al., 2014), there have been attempts (Baliah\n",
      "et al., 2025; Han et al., 2023; Wang et al., 2022a) to use\n",
      "diffusion models instead (Ho et al., 2020). However, these\n",
      "approaches face issues of pose controllability, preservation\n",
      "of target skin color and overall realism of generated head.\n",
      "Head reenactment Head reenactment methods can be\n",
      "generally categorized as either warping-based (Siarohin\n",
      "et al., 2019; Zakharov et al., 2020b; Drobyshev et al., 2022;\n",
      "Zhang et al.; Wang et al., 2022c) or reconstruction-based\n",
      "(Zielonka et al., 2022; Li et al., 2023; Qian et al., 2024; Chu\n",
      "et al., 2024; Deng et al., 2024). Warping-based approaches\n",
      "utilize motion and facial expression descriptors of the target\n",
      "to deform source image. These descriptors can be based on\n",
      "keypoints (Siarohin et al., 2019; Zakharov et al., 2020b),\n",
      "blendshapes from parametric models (Ren et al., 2021; Yin\n",
      "et al., 2022) or latent representations. The latter usually\n",
      "achieves better expressiveness, however, it requires care-\n",
      "ful disentanglemet of motion from the appearance of the\n",
      "target. This can be achieved via the use of special losses\n",
      "(Drobyshev et al., 2022; 2024; Pang et al., 2023) or addi-\n",
      "tional regularization embedded into the architecture (Pang\n",
      "et al., 2023). However, warping-based approaches generally\n",
      "perform well only if the difference between source and tar-\n",
      "get poses is small. Reconstruction-based methods (Zielonka\n",
      "et al., 2022; Li et al., 2023; Qian et al., 2024; Chu et al.,\n",
      "2024; Deng et al., 2024) construct latent model of source\n",
      "head, and therefore are robust to larger pose deviations.\n",
      "These methods often utilize implicit representations such\n",
      "as TriPlanes (Ma et al., 2023; Ye et al., 2024) and NeRF\n",
      "(Zielonka et al., 2022; Zheng et al., 2022; Bai et al., 2023),\n",
      "or explicit ones, such as voxels (Xu et al., 2023), point\n",
      "clouds (Zheng et al., 2023) and meshes (Khakhulin et al.,\n",
      "2022; Grassal et al., 2022), with particularly photorealis-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2463\n",
      "as TriPlanes (Ma et al., 2023; Ye et al., 2024) and NeRF\n",
      "(Zielonka et al., 2022; Zheng et al., 2022; Bai et al., 2023),\n",
      "or explicit ones, such as voxels (Xu et al., 2023), point\n",
      "clouds (Zheng et al., 2023) and meshes (Khakhulin et al.,\n",
      "2022; Grassal et al., 2022), with particularly photorealis-\n",
      "tic results achieved recently with Gaussian splatting (Qian\n",
      "et al., 2024; Giebenhain et al., 2024). However, reconstruc-\n",
      "tion with these approaches requires an additional step of\n",
      "per-frame estimation of camera parameters, which increases\n",
      "runtime. Also, due to high computational cost of rendering,\n",
      "the resolution of output images does not exceed 256 √ó 256\n",
      "and upsampling to higher resolutions is performed by an\n",
      "additional network.\n",
      "3. Approach\n",
      "Our pipeline consists of two modules, Aligner and Blender.\n",
      "Aligner is used to perform cross-reenactment by transfer-\n",
      "ring target motion to source head. Several encoders embed\n",
      "relevant information from input images at different scales,\n",
      "which is then fused in decoder network. Positionally align-\n",
      "ing both heads allows to perform further blending. It is\n",
      "based on identifying regions that require inpainting, and\n",
      "construction of color references for them. Color reference\n",
      "for head is obtained via correlation learning, while for back-\n",
      "ground we use LaMa inpainting network (Suvorov et al.,\n",
      "2021). They are supplied to UNet network, which performs\n",
      "final blending of reenacted head into the target background.\n",
      "Figure 2. Aligner architecture. Two appearance encoders Epor\n",
      "and Eid take images of source head IS and face Crop (IS) and\n",
      "produce embeddings fpor and fid. Motion encoder Emotion is\n",
      "used to obtain respective embedding fmtn from the target image\n",
      "IT . The embeddings fpor, fid and fmtn are concatenated and\n",
      "used to condition generator G via AdaIN (Huang & Belongie,\n",
      "2017) layers. The generator takes a learnable tensor T as input\n",
      "and outputs reenacted head IA and binary mask Mreenact.\n",
      "3.1. Aligner\n",
      "As we target model usage for in-the-wild scenario, we have\n",
      "chosen the reconstruction approach to face reenactment to\n",
      "increase robustness to large pose variations. We decided to\n",
      "use a simple architecture for a faster and more lightweight\n",
      "solution. Thus it is based on 2D instead of 3D volumetric\n",
      "representations to remove the need for camera estimation,\n",
      "rendering and additional upsampling to higher resolution.\n",
      "However, in principle any reenactment model can be used\n",
      "at this stage, provided it reconstructs the whole head.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "763\n",
      "solution. Thus it is based on 2D instead of 3D volumetric\n",
      "representations to remove the need for camera estimation,\n",
      "rendering and additional upsampling to higher resolution.\n",
      "However, in principle any reenactment model can be used\n",
      "at this stage, provided it reconstructs the whole head.\n",
      "Aligner architecture Aligner module, based on (Shu\n",
      "et al., 2022), is illustrated in Fig. 2. It consists of a set of en-\n",
      "coders to embed relevant information from source and target\n",
      "images, which is then passed to condition generator. Two\n",
      "encoders, Eid and Epor, are used to encode identity of the\n",
      "source at multiple scales. Local information is encoded by\n",
      "Eid, a pretrained state-of-the-art face recognition network\n",
      "(Deng et al., 2019). It takes central face crops Crop(IS) as\n",
      "3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2477\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "input and outputs face embedding fid ‚àà R512. Global infor-\n",
      "mation, including hair and head shape, is extracted by Epor,\n",
      "which takes full source image and outputs fpor ‚àà R512.\n",
      "Such combination of appearance encoders allows to obtain\n",
      "more fine-grained head reconstruction, while paying special\n",
      "attention to more discriminative facial features.\n",
      "To embed driving head pose and facial expression, we use\n",
      "motion encoder Emotion. Given full augmented target im-\n",
      "age IT , it produces motion embedding fmtn ‚àà R256 . We\n",
      "address disentanglement of motion from appearance by\n",
      "stretching IT independently along horizontal and vertical\n",
      "axes before supplying it to Emotion. In this way, we change\n",
      "identity of the person, while preserving head pose and ex-\n",
      "pression.\n",
      "The obtained descriptors fid, fpor and fmtn are concate-\n",
      "nated and used to condition StyleGAN-like generator G\n",
      "via AdaIN (Huang & Belongie, 2017). It takes learnable\n",
      "tensor T ‚àà R512√ó4√ó4 as input and passes it through a series\n",
      "of convolutional blocks to obtain final image IA, injecting\n",
      "the appearance and motion information at each step. To\n",
      "stabilize training, the generator outputs binary head mask\n",
      "Mreenact along with reenacted head.\n",
      "Source Target Iexp Ipose If ull\n",
      "Figure 3. Ablation results on representations learned by pose\n",
      "Epose and expression Eexp encoders\n",
      "Refined motion encoder The original Aligner architec-\n",
      "ture of HeSer (Shu et al., 2022) included separate pose\n",
      "Epose and expression Eexp encoders. However, we encoun-\n",
      "tered the problem of target identity leakage with this design,\n",
      "resulting in the mixing of target and source identitties in the\n",
      "output image IA . To obtain more insights into the problem,\n",
      "we conducted ablation study to learn which information is\n",
      "embedded by each encoder (fig. 3). We used embedding\n",
      "of canonical pose as output of Epose, and allowed Eexp to\n",
      "obtain expression embedding from target image, resulting in\n",
      "generation Ipose. In this case, head is still generated in pose\n",
      "of target, not in canonical one. It is overall quite similar to\n",
      "full result If ull, except for difference in skin color which is\n",
      "now closer to the source one. On the other hand, by using\n",
      "canonical expression and image-based pose representation\n",
      "we obtain final generation Iexp with almost canonical ex-\n",
      "pression and pose. This ablation indicates that Eexp learns\n",
      "both expression and pose embeddings, while Epose mainly\n",
      "transfers appearance of the target.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2351\n",
      "canonical expression and image-based pose representation\n",
      "we obtain final generation Iexp with almost canonical ex-\n",
      "pression and pose. This ablation indicates that Eexp learns\n",
      "both expression and pose embeddings, while Epose mainly\n",
      "transfers appearance of the target.\n",
      "Next we decided to regularize each encoder to split the\n",
      "motion information into meaningful separate embeddings.\n",
      "For this we tested different approaches by 1) decreasing\n",
      "size of encoders, 2) using disentanglement and cycle losses\n",
      "as in (Drobyshev et al., 2022; Pang et al., 2023), 3) using\n",
      "pretrained encoders from (Feng et al., 2021; Drobyshev\n",
      "et al., 2022). However, these experiments resulted in inferior\n",
      "performance compared to the baseline of single motion\n",
      "encoder Emotion, which embeds both pose and expression\n",
      "into single vector fmtn ‚àà R256. Relevant ablation is shown\n",
      "in section results.\n",
      "Training losses To train Aligner, we use hinge adversar-\n",
      "ial loss Ladv, feature-matching loss LF M (Salimans et al.,\n",
      "2016), L1 reconstruction loss LL1, VGG-based percep-\n",
      "tual loss LV GG\n",
      "perc (Simonyan & Zisserman, 2014) and dice\n",
      "loss Ldice (Sudre et al., 2017). To improve source identity\n",
      "preservation, we introduce cosineLID\n",
      "cos and perceptual LID\n",
      "perc\n",
      "losses comparing embeddings and feature maps of IA and\n",
      "IS obtained with our pretrained encoder Eid. We also no-\n",
      "ticed that with the new combined motion encoder Emotion\n",
      "expressiveness decreases, and the closure of mouth and eye-\n",
      "lids does not follow the target. We introduce additional\n",
      "emotion loss Lemo (Danecek et al., 2022) and keypoint clo-\n",
      "sure loss that compares distance between lips and eyelids\n",
      "for generated and driving heads:\n",
      "Lkpt =\n",
      "X\n",
      "klower\n",
      "i ‚ààKlower\n",
      "|klower\n",
      "i ‚àí kupper\n",
      "i | (1)\n",
      "where Klower is the set of keypoints on lower eyelid or lip,\n",
      "and klower\n",
      "i and kupper\n",
      "i are symmetric keypoints on lower\n",
      "and upper eyelid or lip, respectively. Since reenactment\n",
      "quality is also influenced by such subtle factors as gaze\n",
      "direction, we also include perceptual gaze loss as in (Droby-\n",
      "shev et al., 2022) starting from 1000 epochs. Additional\n",
      "details on losses are given in supplementary material.\n",
      "3.2. Blender\n",
      "3.2.1. P RELIMINARY\n",
      "We base our Blender on the corresponding module from\n",
      "(Shu et al., 2022). It involves data preprocessing, color\n",
      "reference creation, and blending steps.\n",
      "Figure 5. Masks obtained in data preprocessing stage\n",
      "4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2475\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Figure 4. Blender architecture. Reenacted IA and target IT images are preprocessed to obtain grayscale animated head I GH\n",
      "A , target\n",
      "background I BG\n",
      "T and masks M H\n",
      "A and M I\n",
      "A defining head and background inpainting regions. Also, IA and IT are passed to Reference\n",
      "Creation module to obtain head color reference I HR\n",
      "T ‚ÜíA for color transfer. The background reference I IR\n",
      "T ‚ÜíA is obtained via External\n",
      "Inpainting module (Suvorov et al., 2021). These inputs are passed to Blending UNet to achieve final result IB.\n",
      "Data preprocessing Data preprocessing stage prepares\n",
      "inputs for color transfer and background inpainting. In\n",
      "particular, the first problem is viewed as a problem of re-\n",
      "coloring gray-scale reenacted head with colors of the target\n",
      "head. For this, segmentation masks MA of the head regions\n",
      "and a binary head mask M H\n",
      "A are obtained from reenacted\n",
      "image IA. Similarly, target segmentation MT and head\n",
      "M H\n",
      "T are obtained from target image IT . Then gray-scale\n",
      "image of the head to be colored is obtained as I GH\n",
      "A =\n",
      "Gray(IA ‚àó M H\n",
      "A )\n",
      "Additionally, we need to define regions that need inpainting\n",
      "due to differences in head shape between source and target.\n",
      "For this, union MU of reenacted M H\n",
      "A and target M H\n",
      "T head\n",
      "masks is dilated to an enlarged head mask ÀÜM H\n",
      "A . Then the\n",
      "area that requires inpainting is denoted as M I\n",
      "A = ÀÜM H\n",
      "A ‚àí\n",
      "M H\n",
      "A . The region to serve as color reference for background\n",
      "inpainting is then defined as M I\n",
      "T = ÀÜM H\n",
      "T ‚àí M H\n",
      "T , where\n",
      "ÀÜM H\n",
      "T is dilated target head mask M H\n",
      "T . Finally, background\n",
      "without head can be obtained as I BG\n",
      "T = IT ‚àó (1 ‚àí ÀÜM H\n",
      "A ).\n",
      "Additionally, we introduce augmentation of inpainting mask\n",
      "M I\n",
      "A to enhance inpainting of large mismatched regions.\n",
      "Figure 6. Examples of inpainting mask M I\n",
      "A augmentations\n",
      "Color reference creation The next step is to provide color\n",
      "references for background inpainting and head color transfer.\n",
      "Creation of these references is based on learning the cor-\n",
      "relation between corresponding semantic regions of input\n",
      "IA and target IT images. During training of corresponding\n",
      "Reference Creation (RC) module, the same image serves as\n",
      "both input IA and target IT . To prevent the network from\n",
      "merely copying pixels from the same position, random color\n",
      "augmentation C ‚Ä≤ is applied to IA and random horizontal\n",
      "flip F is applied to IT . Correlation learning takes place in\n",
      "the latent space based on the representation obtained with\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1097\n",
      "both input IA and target IT . To prevent the network from\n",
      "merely copying pixels from the same position, random color\n",
      "augmentation C ‚Ä≤ is applied to IA and random horizontal\n",
      "flip F is applied to IT . Correlation learning takes place in\n",
      "the latent space based on the representation obtained with\n",
      "Feature Pyramid Network (FPN) (Lin et al., 2017) for IA\n",
      "and IT :\n",
      "fA = F P N(C ‚Ä≤(IA)) (2)\n",
      "fT = F P N(F (IT )) (3)\n",
      "Next, correlation is calculated between features\n",
      "fA and fT for each spatial location. It is used\n",
      "to weight pixels during resampling in the fol-\n",
      "lowing step. For each semantic region r ‚àà\n",
      "{face, ears, eyes, brows, nose, lips, mouth, teeth, beard, hair,\n",
      "glasses, hat, headphones, earrings} a correlation matrix\n",
      "Œìr ‚àà RN r\n",
      "A√óN r\n",
      "T is computed, where N r\n",
      "A and N r\n",
      "T are\n",
      "numbers of pixels in region r in input IA and target IT\n",
      "images. Thus, each element Œìr(u, v) is calculated as:\n",
      "Œìr(u, v) =\n",
      "¬Øf r\n",
      "A(u)T ¬Øf r\n",
      "T (v)\n",
      "‚à• ¬Øf r\n",
      "A(u)‚à•‚à• ¬Øf r\n",
      "T (v)‚à• , u ‚àà M r\n",
      "A, v ‚àà M r\n",
      "T (4)\n",
      "where ¬Øf r\n",
      "A(u) and ¬Øf r\n",
      "T (v) are channelwise centralized features\n",
      "fA and fT at locations u and v. Œìr(u, v) is then normalized\n",
      "5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2456\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "via softmax and used to determine contribution of each pixel\n",
      "v in corresponding region of IT to the color of pixel u in\n",
      "the head reference image I r\n",
      "T ‚ÜíA(u):\n",
      "I r\n",
      "T ‚ÜíA(u) =\n",
      "X\n",
      "u‚ààM r\n",
      "T\n",
      "softmaxv(Œìr(u, v)/œÑ)¬∑IT (v), u ‚àà M r\n",
      "A\n",
      "(5)\n",
      "where œÑ is temperature coefficient. Overall, computation of\n",
      "head color reference I HR\n",
      "T ‚ÜíA is based on the following inputs:\n",
      "I HR\n",
      "T ‚ÜíA = RC(fA, MA, fT , MT , IT ‚àó M H\n",
      "T ) (6)\n",
      "However, during inference a label mismatch can occur if\n",
      "an area is present in the reenacted image IA but not in the\n",
      "target one IT . In this case, such face region is not colored by\n",
      "the blending UNet, since no color reference for it is given.\n",
      "To remedy this problem, we propose to assign colors from\n",
      "other semantically relevant areas of IT . In case no such area\n",
      "is found (for instance hat is present in IA but not in IT ) or\n",
      "is too small to provide robust reference, we copy the colors\n",
      "from the original image IA.\n",
      "To create color inpainting reference I IR\n",
      "T ‚ÜíA, we use LaMa\n",
      "model (Suvorov et al., 2021), conditioned on inpainting\n",
      "mask M I\n",
      "T . We found this solution to provide superior qual-\n",
      "ity compared to creation with RC module.\n",
      "Blending UNet Blending of reenacted head into target\n",
      "background is performed by Blending UNet B submodule.\n",
      "As input it takes concatenated head I HR\n",
      "T ‚ÜíA and background\n",
      "inpainting I IR\n",
      "T ‚ÜíA references, head mask M H\n",
      "A , background\n",
      "I BG\n",
      "T , inpainting mask M I\n",
      "A and gray-scale head I GH\n",
      "A and\n",
      "outputs final image IB:\n",
      "IB = B(I HR\n",
      "T ‚ÜíA, IIR\n",
      "T ‚ÜíA, M H\n",
      "A , IBG\n",
      "T , M I\n",
      "A, IGH\n",
      "A ) (7)\n",
      "The UNet is trained with standard adversarial Ladv, recon-\n",
      "struction LL1 and perceptual losses LV GG\n",
      "perc . Additionally, to\n",
      "learn a meaningful correlation matrix Œìr, cycle consistency\n",
      "loss is used:\n",
      "Lc = Œªc‚à•IT ‚ÜíA‚ÜíT ‚àí IT ‚à•1 (8)\n",
      "where I k\n",
      "T ‚ÜíA‚ÜíT (u) = P\n",
      "v‚ààM k\n",
      "A\n",
      "softmaxv(Œìk(u, v)/œÑ) ¬∑\n",
      "IT ‚ÜíA(v), u ‚àà M k\n",
      "T . Additional regularization is performed\n",
      "by calculation of cycle loss with another target image IT ‚Ä≤\n",
      "having the same identity as IA:\n",
      "Lc‚Ä≤ = Œªc‚à•IT ‚Ä≤‚ÜíA‚ÜíT ‚Ä≤ ‚àí IT ‚à•1 (9)\n",
      "To make head color reference more similar to source image,\n",
      "regularization loss is used:\n",
      "Lreg = Œªreg‚à•M H\n",
      "A ¬∑\n",
      "\u0000\n",
      "(grayscale(IA) ‚àí grayscale(I HR\n",
      "T ‚ÜíA)\n",
      "\u0001\n",
      "‚à•1\n",
      "(10)\n",
      "More details on corresponding hyperparameters are given\n",
      "in supplementary.\n",
      "IA Iext cIA IB cIB\n",
      "Figure 7. Improved blending of hair. cIB and IB are final results\n",
      "with and without background extrapolation\n",
      "Improved blending of hair We also implement additional\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1747\n",
      "T ‚ÜíA)\n",
      "\u0001\n",
      "‚à•1\n",
      "(10)\n",
      "More details on corresponding hyperparameters are given\n",
      "in supplementary.\n",
      "IA Iext cIA IB cIB\n",
      "Figure 7. Improved blending of hair. cIB and IB are final results\n",
      "with and without background extrapolation\n",
      "Improved blending of hair We also implement additional\n",
      "step on Aligner output to refine blending of hair in the re-\n",
      "sulting image IB. We utilize soft portrait masks Msof t to\n",
      "segment hair area. They provide better segmentation results\n",
      "than hard ones due to the uncertainty of edge estimation\n",
      "of the hair region. However, due to such choice of mask-\n",
      "ing blending UNet recognizes soft mask areas to belong\n",
      "to the head region and does not extrapolate background to\n",
      "them, resulting in a visible border. To remedy this problem,\n",
      "we create additional image Iext, where the background is\n",
      "extrapolated over the head mask M H\n",
      "A , as shown in fig. 7.\n",
      "Then we obtain refined animated portrait cIA by blending\n",
      "extrapolated background Iext with Aligner output based on\n",
      "matting mask Msof t:cIA = Msof t‚àóIA+(1 ‚àíMsof t)‚àóIext.\n",
      "Figure 8. Post-processing of blended image. Given target IT and\n",
      "reenacted IA images, we obtain masks of reenacted head M head\n",
      "A\n",
      "and target hair M hair\n",
      "T . We substract the first mask from the second\n",
      "one to obtain area for inpanting Minpainting, which is filled by\n",
      "Kandinsky model (Shakhmatov et al., 2023)\n",
      "Head transfer on real data When transferring to real\n",
      "data, we must take into account that our model works within\n",
      "cropped images. In contrast to the face swap problem, where\n",
      "the person‚Äôs face is always inside the generated region, in the\n",
      "head swap problem the hair can extend beyond the bound-\n",
      "aries of the considered area. In this case, we need to remove\n",
      "excess hair from the image. For this purpose, we propose a\n",
      "6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2452\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "post-processing blending algorithm using Kandinsky diffu-\n",
      "sion model (Shakhmatov et al., 2023). We obtain the final\n",
      "image by inpaiting the region masked by Minpainting for\n",
      "UNet output IB, as shown in fig. 8. It should be noted that\n",
      "this step is optional and is applied when person in the target\n",
      "image has hair that significantly differs from hair of source.\n",
      "Image BiSeNet Ours\n",
      "Figure 9. Qualitative comparison of segmentation models\n",
      "3.3. Segmentation\n",
      "For our model to work, it is necessary to have a high-quality\n",
      "segmentation model‚Äîit will be used in the Blender module,\n",
      "as well as during preprocessing stages to select a person‚Äôs\n",
      "head. There are two main requirements for the model: it\n",
      "must be able to segment hairstyles and facial hair as sep-\n",
      "arate classes, and it must be additive, meaning it should\n",
      "segment in such a way that we can combine the segments to\n",
      "obtain a complete head. Additionally, each region should\n",
      "be homogeneous in color‚Äîfor example, the ‚Äôbeard‚Äô class\n",
      "should not overlap with the ‚Äôskin‚Äô class, as these regions\n",
      "will be used later for color transfer. To train the model, a\n",
      "dataset of 20,000 FullHD images was collected and anno-\n",
      "tated (Kapitanov et al., 2023), and a segmentation model\n",
      "was trained based on it. We settled on Segformer-B5 (Xie\n",
      "et al., 2021) as the model for face parsing and segmentation.\n",
      "As a result, the model can segment the following classes:\n",
      "background, person, skin, left brow, right brow, left eye,\n",
      "right eye, mouth, teeth, lips, left ear, right ear, nose, neck,\n",
      "beard, hair, hat, headphone, glasses, earring. Fig. 9 shows\n",
      "a visual comparison of our model and BiSeNet (Yu et al.,\n",
      "2018).\n",
      "4. Experiments\n",
      "4.1. Experiment Setup\n",
      "Dataset We use V oxCeleb2 (Chung et al., 2018) dataset\n",
      "to train and evaluate our model at 512 √ó 512 and 256 √ó 256\n",
      "resolutions. Since some videos originally have low quality,\n",
      "we filter data using image quality assessment methods (Su\n",
      "et al., 2020; Wang et al., 2023), leaving approximately 70%\n",
      "of the original dataset. We additionally preprocess the data\n",
      "by cropping face and head regions and calculating keypoints\n",
      "with Feng et al. (2021). Our final train and test sets include\n",
      "respectively 135500 and 5500 videos. The split is made\n",
      "so as to avoid intersection between train and test identities.\n",
      "During training, we sample source and target from the same\n",
      "video, while during inference they can feature different\n",
      "identities.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1577\n",
      "respectively 135500 and 5500 videos. The split is made\n",
      "so as to avoid intersection between train and test identities.\n",
      "During training, we sample source and target from the same\n",
      "video, while during inference they can feature different\n",
      "identities.\n",
      "Evaluation metrics To evaluate our Aligner and Blender\n",
      "against the baselines, we use LPIPS (Zhang et al., 2018),\n",
      "SSIM (Wang et al., 2004) and MS-SSIM (Wang et al., 2003)\n",
      "to assess perceptual quality, and PSNR to measure recon-\n",
      "struction error. For Aligner, to compare source identity\n",
      "preservation, we compute cosine distance (CSIM) between\n",
      "embeddings of IA and IS from face recognition model\n",
      "(Deng et al., 2019). On cross-reenactment, we also uti-\n",
      "lize Frechet Inception Distance (FID) (Heusel et al., 2017).\n",
      "Additionally, for Blender we also measure reconstruction for\n",
      "background inpainting PSNRinpainting and head color transfer\n",
      "PSNRhead.\n",
      "Additionally, in cross-reenactment scenario we conduct a\n",
      "user study to qualitatively compare preservation of source\n",
      "identity (UAPP), transfer of target motion (UMTN) and\n",
      "overall quality (UQLT).\n",
      "4.2. Aligner evaluation\n",
      "Baselines Our Aligner is compared against the open-\n",
      "source baselines at 512 √ó 512 and 256 √ó 256 resolutions.\n",
      "We note that the majority of competing models are trained\n",
      "on narrow face crops and hence do not reconstruct whole\n",
      "head and hair.\n",
      "Few 2D reenactment models are available at 512 √ó 512 res-\n",
      "olution. We compare against StyleHEAT (Yin et al., 2022),\n",
      "based on StyleGAN (Karras, 2019) inversion. Additionally,\n",
      "we train baseline from HeSer (Shu et al., 2022) at512 √ó512\n",
      "7\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2495\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Table 1. Quantitative results on head reenactment at 512 √ó 512 and 256 √ó 256 resolution\n",
      "Method Self-reenactment (256 √ó 256) Cross-reenactment (256 √ó 256)\n",
      "CSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë CSIM ‚Üë FID ‚Üì\n",
      "X2Face (Wiles et al., 2018) 0.789 0.201 19.66 0.715 0.636 41.95\n",
      "FOMM (Siarohin et al., 2019) 0.847 0.128 21.83 0.783 0.629 28.12\n",
      "PIRender (Ren et al., 2021) 0.827 0.173 19.46 0.704 0.665 19.95\n",
      "Bi-layer (Zakharov et al., 2020a) 0.788 0.198 20.33 0.772 0.654 32.70\n",
      "DaGAN (Hong et al., 2022) 0.789 0.208 19.19 0.712 0.580 44.00\n",
      "DPE (Pang et al., 2023) 0.845 0.161 20.92 0.768 0.613 50.61\n",
      "LIA (Wang et al., 2022b) 0.842 0.128 22.11 0.786 0.659 24.19\n",
      "TPSMM (Zhao & Zhang, 2022) 0.843 0.124 21.86 0.804 0.647 22.06\n",
      "Ours 0.747 0.116 22.30 0.815 0.616 36.89\n",
      "Method Self-reenactment (512 √ó 512) Cross-reenactment (512 √ó 512)\n",
      "CSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë CSIM ‚Üë FID ‚Üì\n",
      "HeSer (Shu et al., 2022) 0.780 0.139 22.99 0.856 0.612 35.33\n",
      "StyleHEAT (Yin et al., 2022) 0.789 0.574 8.791 0.582 0.673 75.856\n",
      "Ours 0.754 0.142 22.04 0.848 0.628 29.57\n",
      "resolution for full head synthesis.\n",
      "At 256 √ó 256 resolution, we compare against the follow-\n",
      "ing warping-based approaches: First Order Motion Model\n",
      "(FOMM) (Siarohin et al., 2019), PIRenderer (Ren et al.,\n",
      "2021), Depth-Aware Generative Adversarial Network (Da-\n",
      "GAN) (Hong et al., 2022), Disentanglement of Pose and\n",
      "Expression (DPE) model (Pang et al., 2023), Latent Image\n",
      "Animator (LIA) (Wang et al., 2022b) and Thin-Plate Spline\n",
      "Motion Model (TPSMM) (Zhao & Zhang, 2022). We also\n",
      "include methods based on latent face reconstruction with\n",
      "target motion: X2Face (Wiles et al., 2018) and Fast Bi-\n",
      "layer Neural Synthesis (Zakharov et al., 2020b). All these\n",
      "methods generate only narrow face crops and not the whole\n",
      "head.\n",
      "Results The results for self- and cross-reenactment sce-\n",
      "narios at 512 √ó 512 and 256 √ó 256 resolutions are presented\n",
      "in table 1. At 512 √ó 512 resolution, HeSer (Shu et al.,\n",
      "2022) outperforms other methods by almost all metrics at\n",
      "self-reenactment. This is largely attributed to the leakage\n",
      "of target identity into final generation, which supplies ad-\n",
      "ditional information on the desired result. However, it is\n",
      "inferior to GHOST 2.0 on cross-reenactment. As can be\n",
      "seen from fig. 10, GHOST 2.0 is significantly better at\n",
      "preserving source identity and skin color, while the HeSer\n",
      "produces a mixed identity of source and driver. Also, com-\n",
      "pared to StyleHEAT, our model is more robust to generation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2081\n",
      "inferior to GHOST 2.0 on cross-reenactment. As can be\n",
      "seen from fig. 10, GHOST 2.0 is significantly better at\n",
      "preserving source identity and skin color, while the HeSer\n",
      "produces a mixed identity of source and driver. Also, com-\n",
      "pared to StyleHEAT, our model is more robust to generation\n",
      "in extreme poses, although it may be slightly inferior in\n",
      "terms of identity preservation in frontal views.\n",
      "At 256 √ó 256 resolution, we outperform other methods by\n",
      "LPIPS, PSNR and SSIM in self-reenactment. This is in\n",
      "part explained by robustness of our method to generation\n",
      "in difficult poses. Warping-based methods perform well\n",
      "only in case of small displacements, resulting in severe face\n",
      "distortion and artifacts otherwise. However, they usually\n",
      "Table 2. Side-by-side comparison at 512 √ó 512 resolution\n",
      "Method UAPP ‚Üë UMTN ‚Üë UQLT ‚Üë\n",
      "HeSer (Shu et al., 2022) 0.04 0.15 0.04\n",
      "StyleHEAT (Yin et al., 2022) 0.13 0.05 0.03\n",
      "Ours 0.83 0.80 0.93\n",
      "Table 3. Side-by-side comparison at 256 √ó 256 resolution\n",
      "Method UAPP ‚Üë UMTN ‚Üë UQLT ‚Üë\n",
      "Bi-layer (Zakharov et al., 2020a) 0.81 0.83 0.91\n",
      "DaGAN (Hong et al., 2022) 0.11 0.13 0.07\n",
      "X2Face (Wiles et al., 2018) 0.08 0.04 0.02\n",
      "LIA (Wang et al., 2022b) 0.52 0.42 0.51\n",
      "TPSMM (Zhao & Zhang, 2022) 0.20 0.35 0.25\n",
      "PIRender (Ren et al., 2021) 0.28 0.23 0.24\n",
      "DPE (Pang et al., 2023) 0.27 0.13 0.12\n",
      "FOMM (Siarohin et al., 2019) 0.10 0.12 0.07\n",
      "Ours 0.63 0.75 0.81\n",
      "Bi-layer (Zakharov et al., 2020a) 0.13 0.10 0.07\n",
      "LIA (Wang et al., 2022b) 0.41 0.29 0.29\n",
      "Ours 0.46 0.61 0.64\n",
      "excel in identity preservation, as evidenced by CSIM both\n",
      "on self- and cross-reenactment. Please see supplement for\n",
      "visual comparison of the models.\n",
      "Side-by-side We also conducted side-by-side comparison\n",
      "on cross-reenactment scenario between our Aligner and the\n",
      "competitors both at256√ó256 and 512√ó512 resolutions. We\n",
      "asked the users to choose the model which performs best in\n",
      "terms of the following criteria: source identity preservation\n",
      "(UAPP), target movement transfer (UMTN) and overall\n",
      "generation quality (UQLT). We present the percentage of\n",
      "examples where each model is chosen in tables 2 and 3.\n",
      "8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1819\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Source Target GHOST 2.0 HeSer StyleHEAT\n",
      "Figure 10. Cross-reenactment results at 512 √ó 512 resolution\n",
      "Since at 256 √ó 256 resolution we have a large number of\n",
      "models to compare against, we split them into three triplets\n",
      "first, compare within them, and then compare winners of\n",
      "these triplets. On average, each image from the test dataset\n",
      "is shown to 41 users.\n",
      "Our method outperforms the competitors by a large margin\n",
      "in terms of motion preservation and generation quality at\n",
      "both resolutions. It is also significantly better than most\n",
      "of the methods in terms of identity preservation. Several\n",
      "methods, such as DPE (Pang et al., 2023) or LIA (Wang\n",
      "et al., 2022b), fail at generation with large head rotations\n",
      "and produce blank outputs. Our method shows robustness\n",
      "to various head poses and expressions.\n",
      "Ablation We conducted ablation study to justify our de-\n",
      "sign of combined motion encoder Emtn. We compared\n",
      "different strategies to disentangle pose and facial expression\n",
      "descriptors through special losses and architectural changes.\n",
      "The results are shown in table 5. In addition to FID and\n",
      "CSIM, we also use Average Keypoint Distance (AKD) to\n",
      "measure motion transfer.\n",
      "In HeSer, two separate MobileNetV2 (Sandler et al., 2018)\n",
      "encoders are used to capture pose and expression. We also\n",
      "Source Target GHOST 2.0\n",
      "Figure 11. GHOST 2.0 results on the task of head swap. Our\n",
      "method achieves natural blending of the reenacted head into target\n",
      "background, corresponding in skin color, lighting and contrast to\n",
      "the rest of the image.\n",
      "conducted experiment with decreasing their number of chan-\n",
      "nels in half to force them to learn only relevant information.\n",
      "In other experiments, we replaced them with corresponding\n",
      "pretrained encoders from DECA (Feng et al., 2021), and\n",
      "9\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2479\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Table 4. Quantitative results on head blending\n",
      "Method Plain data\n",
      "LPIPS ‚Üì SSIM ‚Üë MSSIM ‚Üë PSNR ‚Üë PSNRinpainting ‚Üë PSNRhead ‚Üë\n",
      "Baseline 0.034 0.972 0.987 33.86 35.13 42.13\n",
      "REFace 0.24 0.89 0.78 23.45 36.72 29.29\n",
      "InstantID + SDXL inpainting 0.10 0.94 0.91 29.49 38.54 30.38\n",
      "External inpainting 0.018 0.984 0.992 36.65 37.98 45.83\n",
      "Method Mask augmentations and source color jitter\n",
      "LPIPS ‚Üì SSIM ‚Üë MSSIM ‚Üë PSNR ‚Üë PSNRinpainting ‚Üë PSNRhead ‚Üë\n",
      "Baseline 0.093 0.926 0.945 27.76 28.15 41.25\n",
      "External inpainting 0.060 0.947 0.951 28.21 28.45 44.13\n",
      "also tried to disentangle representations with cyclic loss\n",
      "from DPE (Pang et al., 2023). Our design with combined\n",
      "encoder Emtn achieves the best quality by all metrics. Fur-\n",
      "ther details on other ablations and calculation of AKD are\n",
      "given in supplementary.\n",
      "Table 5. Results on cross-reenactment for different disentangle-\n",
      "ment strategies between target motion and identity\n",
      "Method FID ‚Üì CSIM ‚Üë AKD ‚Üì\n",
      "HeSer (Shu et al., 2022) 28.63 0.596 0.0095\n",
      "Smaller encoders 28.38 0.622 0.0108\n",
      "DECA encoders (Feng et al.,\n",
      "2021)\n",
      "35.41 0.627 0.0155\n",
      "DPE loss (Pang et al., 2023) 31.70 0.603 0.0103\n",
      "Combined Emtn 26.83 0.622 0.0098\n",
      "4.3. Blender evaluation\n",
      "To evaluate our Blender and justify external inpainting usage\n",
      "in it, we trained baseline HeSer Blender on the same data.\n",
      "Additionally, we compare with diffusion-based head swap\n",
      "approaches, such as REFace (Baliah et al., 2025) and Instan-\n",
      "tID (Wang et al., 2024). For the latter solution, we first use\n",
      "InstantID (Wang et al., 2024) to generate source head with\n",
      "target pose directed by facial keypoints. Then the resulting\n",
      "head is blended into target background using SDXL (Podell\n",
      "et al., 2023) inpainting model. It should be noted that both\n",
      "solutions frequently fail to preserve identity and target skin\n",
      "color. As can be seen from table 4, our version with external\n",
      "inpainting outperforms baseline and competitors on almost\n",
      "every metric. However, for background inpainting, SDXL\n",
      "shows slightly better results than LaMa, so further investi-\n",
      "gation into better inpainting models may constitute an area\n",
      "for future research. Furthermore, for external inpainting and\n",
      "baseline, we present results on training with additional data\n",
      "augmentations. Our solution outperforms baseline in this\n",
      "scenario as well and also doesn‚Äôt fail in hard cases when\n",
      "source and target differ significantly (fig. 11).\n",
      "In addition, we present inference results of our model on\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2273\n",
      "baseline, we present results on training with additional data\n",
      "augmentations. Our solution outperforms baseline in this\n",
      "scenario as well and also doesn‚Äôt fail in hard cases when\n",
      "source and target differ significantly (fig. 11).\n",
      "In addition, we present inference results of our model on\n",
      "real-life and outdoor photos in supplementary 17. Although\n",
      "our model successfully copes with most cases, sometimes\n",
      "Kandinsky model hallucinates during post-processing and\n",
      "may produce additional attributes such as collars when in-\n",
      "painting excess hair. Also, hair for reenacted source head is\n",
      "generated only inside the crop region. In case target person\n",
      "has shorter hair, it is likely that in the resulting image source\n",
      "hair would look unnaturally cropped from below.\n",
      "5. Conclusion\n",
      "We have presented a two-stage method for realistic head\n",
      "swapping for in-the-wild images. We improve Aligner archi-\n",
      "tecture by merging pose and expression encoders into single\n",
      "motion encoder, which remedies the problem of driver iden-\n",
      "tity leakage. Our head reenactment model outperforms other\n",
      "methods by both qualitative and quantitative metrics and\n",
      "is more robust to large pose variations. We also introduce\n",
      "additional refinements during blending to improve quality of\n",
      "head transfer and inpainting, which allows to obtain superior\n",
      "results compared to baseline solution.\n",
      "The limitations of our model are the following. In sev-\n",
      "eral cases, our method does not reproduce fine details of\n",
      "source appearance. This can be tackled by using stronger\n",
      "appearance encoders in Aligner. Concerning Blender, some\n",
      "face parts may not be evenly colored if the area of corre-\n",
      "sponding color reference is small. Additionally, blending\n",
      "stage could be improved by mitigating the generation of\n",
      "additional clothing attributes (e.g. collars) and better hair\n",
      "processing. Complex lighting conditions can also produce\n",
      "inferior results during blending.\n",
      "6. Impact Statement\n",
      "This paper enhances approach to head swapping via bet-\n",
      "ter head reenactment and inpainting modules. While such\n",
      "models find applications in commercial scenarios, they are\n",
      "also known to be used for fraudulent activities. However,\n",
      "we suppose that results of this work can be used to fight\n",
      "such misuse by aiding research on more robust deepfake\n",
      "detection systems.\n",
      "10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2495\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "7. Acknowledgments\n",
      "We thank Nikolay Gerasimenko, Anna Averchenkova and\n",
      "ABT data labelling team for help with side-by-side com-\n",
      "parison. We also thank Viacheslav Vasilev for suggestions\n",
      "and comments regarding text. Last but not least, we thank\n",
      "Alexander Kapitanov and his team from SberDevices for\n",
      "training of segmentation model.\n",
      "References\n",
      "Alansari, M., Hay, O. A., Javed, S., Shoufan, A., Zweiri,\n",
      "Y ., and Werghi, N. Ghostfacenets: Lightweight face\n",
      "recognition model from cheap operations. IEEE Access,\n",
      "11:35429‚Äì35446, 2023. doi: 10.1109/ACCESS.2023.\n",
      "3266068.\n",
      "Bai, Z., Tan, F., Huang, Z., Sarkar, K., Tang, D., Qiu,\n",
      "D., Meka, A., Du, R., Dou, M., Orts-Escolano, S.,\n",
      "et al. Learning personalized high quality volumetric head\n",
      "avatars from monocular rgb videos. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pp. 16890‚Äì16900, 2023.\n",
      "Baliah, S., Lin, Q., Liao, S., Liang, X., and Khan, M. H.\n",
      "Realistic and efficient face swapping: A unified approach\n",
      "with diffusion models. In Proceedings of the IEEE/CVF\n",
      "Winter Conference on Applications of Computer Vision\n",
      "(WACV), pp. 1062‚Äì1071. IEEE, 2025.\n",
      "Burkov, E., Pasechnik, I., Grigorev, A., and Lempitsky, V .\n",
      "Neural head reenactment with latent pose descriptors. In\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), June 2020.\n",
      "Cao, Q., Shen, L., Xie, W., Parkhi, O. M., and Zisserman,\n",
      "A. Vggface2: A dataset for recognising faces across pose\n",
      "and age. In 2018 13th IEEE international conference\n",
      "on automatic face & gesture recognition (FG 2018), pp.\n",
      "67‚Äì74. IEEE, 2018.\n",
      "Chen, R., Chen, X., Ni, B., and Ge, Y . Simswap: An\n",
      "efficient framework for high fidelity face swapping. In\n",
      "MM ‚Äô20: The 28th ACM International Conference on\n",
      "Multimedia, 2020.\n",
      "Chen, X., He, K., Zhu, J., Ge, Y ., Li, W., and Wang, C.\n",
      "Hifivfs: High fidelity video face swapping, 2024.\n",
      "Chu, X., Li, Y ., Zeng, A., Yang, T., Lin, L., Liu, Y ., and\n",
      "Harada, T. GPAvatar: Generalizable and precise head\n",
      "avatar from image(s). In The Twelfth International Con-\n",
      "ference on Learning Representations, 2024.\n",
      "Chung, J. S., Nagrani, A., and Zisserman, A. V ox-\n",
      "celeb2: Deep speaker recognition. arXiv preprint\n",
      "arXiv:1806.05622, 2018.\n",
      "Danecek, R., Black, M. J., and Bolkart, T. EMOCA: Emo-\n",
      "tion driven monocular face capture and animation. In\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pp. 20311‚Äì20322, 2022.\n",
      "Deng, J., Guo, J., Xue, N., and Zafeiriou, S. Arcface:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2332\n",
      "arXiv:1806.05622, 2018.\n",
      "Danecek, R., Black, M. J., and Bolkart, T. EMOCA: Emo-\n",
      "tion driven monocular face capture and animation. In\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pp. 20311‚Äì20322, 2022.\n",
      "Deng, J., Guo, J., Xue, N., and Zafeiriou, S. Arcface:\n",
      "Additive angular margin loss for deep face recognition.\n",
      "In Proceedings of the IEEE/CVF conference on computer\n",
      "vision and pattern recognition, pp. 4690‚Äì4699, 2019.\n",
      "Deng, Y ., Wang, D., Ren, X., Chen, X., and Wang, B. Por-\n",
      "trait4d: Learning one-shot 4d head avatar synthesis using\n",
      "synthetic data. In IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, 2024.\n",
      "Drobyshev, N., Chelishev, J., Khakhulin, T., Ivakhnenko, A.,\n",
      "Lempitsky, V ., and Zakharov, E. Megaportraits: One-shot\n",
      "megapixel neural head avatars. 2022.\n",
      "Drobyshev, N., Casademunt, A. B., V ougioukas, K., Land-\n",
      "graf, Z., Petridis, S., and Pantic, M. Emoportraits:\n",
      "Emotion-enhanced multimodal one-shot head avatars. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pp. 8498‚Äì8507, 2024.\n",
      "Duta, I. C., Liu, L., Zhu, F., and Shao, L. Improved residual\n",
      "networks for image and video recognition. In 2020 25th\n",
      "International Conference on Pattern Recognition (ICPR),\n",
      "pp. 9415‚Äì9422. IEEE, 2021.\n",
      "Feng, Y ., Feng, H., Black, M. J., and Bolkart, T. Learning\n",
      "an animatable detailed 3D face model from in-the-wild\n",
      "images. volume 40, 2021.\n",
      "Giebenhain, S., Kirschstein, T., R¬®unz, M., Agapito, L., and\n",
      "Nie√üner, M. Npga: Neural parametric gaussian avatars.\n",
      "In SIGGRAPH Asia 2024 Conference Papers (SA Con-\n",
      "ference Papers ‚Äô24), December 3-6, Tokyo, Japan, 2024.\n",
      "ISBN 979-8-4007-1131-2/24/12. doi: 10.1145/3680528.\n",
      "3687689.\n",
      "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\n",
      "Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y .\n",
      "Generative adversarial nets. Advances in neural informa-\n",
      "tion processing systems, 27, 2014.\n",
      "Grassal, P.-W., Prinzler, M., Leistner, T., Rother, C.,\n",
      "Nie√üner, M., and Thies, J. Neural head avatars from\n",
      "monocular rgb videos. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 18653‚Äì18664, 2022.\n",
      "Groshev, A., Maltseva, A., Chesakov, D., Kuznetsov, A.,\n",
      "and Dimitrov, D. Ghost‚Äîa new face swap approach\n",
      "for image and video domains. IEEE Access, 10:83452‚Äì\n",
      "83462, 2022. doi: 10.1109/ACCESS.2022.3196668.\n",
      "11\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2497\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Han, Y ., Zhang, J., Zhu, J., Li, X., Ge, Y ., Li, W., Wang,\n",
      "C., Liu, Y ., Liu, X., and Tai, Y . A generalist facex via\n",
      "learning unified facial representation, 2023.\n",
      "Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\n",
      "Hochreiter, S. Gans trained by a two time-scale update\n",
      "rule converge to a local nash equilibrium. In Guyon, I.,\n",
      "Luxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vish-\n",
      "wanathan, S., and Garnett, R. (eds.), Advances in Neural\n",
      "Information Processing Systems, volume 30. Curran As-\n",
      "sociates, Inc., 2017.\n",
      "Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\n",
      "bilistic models. Advances in neural information process-\n",
      "ing systems, 33:6840‚Äì6851, 2020.\n",
      "Hong, F.-T., Zhang, L., Shen, L., and Xu, D. Depth-aware\n",
      "generative adversarial network for talking head video\n",
      "generation. 2022.\n",
      "Huang, X. and Belongie, S. Arbitrary style transfer in\n",
      "real-time with adaptive instance normalization. In ICCV,\n",
      "2017.\n",
      "Kapitanov, A., Kvanchiani, K., and Sofia, K. Easyportrait\n",
      "- face parsing and portrait segmentation dataset. arXiv\n",
      "preprint arXiv:2304.13509, 2023.\n",
      "Karras, T. A style-based generator architecture for\n",
      "generative adversarial networks. arXiv preprint\n",
      "arXiv:1812.04948, 2019.\n",
      "Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J.,\n",
      "and Aila, T. Analyzing and improving the image quality\n",
      "of StyleGAN. In Proc. CVPR, 2020.\n",
      "Khakhulin, T., Sklyarova, V ., Lempitsky, V ., and Zakharov,\n",
      "E. Realistic one-shot mesh-based head avatars. In Euro-\n",
      "pean Conference of Computer vision (ECCV), 2022.\n",
      "Kingma, D. P. Adam: A method for stochastic optimization.\n",
      "arXiv preprint arXiv:1412.6980, 2014.\n",
      "Kirschstein, T., Giebenhain, S., and Nie√üner, M. Diffu-\n",
      "sionavatars: Deferred diffusion for high-fidelity 3d head\n",
      "avatars. arXiv preprint arXiv:2311.18635, 2023.\n",
      "Li, X., De Mello, S., Liu, S., Nagano, K., Iqbal, U., and\n",
      "Kautz, J. Generalizable one-shot 3d neural head avatar. In\n",
      "Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\n",
      "M., and Levine, S. (eds.), Advances in Neural Informa-\n",
      "tion Processing Systems, volume 36, pp. 47239‚Äì47250.\n",
      "Curran Associates, Inc., 2023.\n",
      "Lin, T.-Y ., Doll¬¥ar, P., Girshick, R., He, K., Hariharan, B.,\n",
      "and Belongie, S. Feature pyramid networks for object\n",
      "detection. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition, pp. 2117‚Äì2125,\n",
      "2017.\n",
      "Ma, Z., Zhu, X., Qi, G.-J., Lei, Z., and Zhang, L. Otavatar:\n",
      "One-shot talking face avatar with controllable tri-plane\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2458\n",
      "and Belongie, S. Feature pyramid networks for object\n",
      "detection. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition, pp. 2117‚Äì2125,\n",
      "2017.\n",
      "Ma, Z., Zhu, X., Qi, G.-J., Lei, Z., and Zhang, L. Otavatar:\n",
      "One-shot talking face avatar with controllable tri-plane\n",
      "rendering. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, pp. 16901‚Äì\n",
      "16910, 2023.\n",
      "Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y . Spec-\n",
      "tral normalization for generative adversarial networks.\n",
      "arXiv preprint arXiv:1802.05957, 2018.\n",
      "Pang, Y ., Zhang, Y ., Quan, W., Fan, Y ., Cun, X., Shan, Y .,\n",
      "and Yan, D.-M. Dpe: Disentanglement of pose and ex-\n",
      "pression for general video portrait editing. InProceedings\n",
      "of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), pp. 427‚Äì436, June 2023.\n",
      "Perov, I., Gao, D., Chervoniy, N., Liu, K., Marangonda,\n",
      "S., Um ¬¥e, C., Facenheim, C. S., RP, L., Jiang, J.,\n",
      "Zhang, S., et al. Deepfacelab: Integrated, flexible and\n",
      "extensible face-swapping framework. arXiv preprint\n",
      "arXiv:2005.05535, 2020.\n",
      "Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\n",
      "T., Muller, J., Penna, J., and Rombach, R. Sdxl: Im-\n",
      "proving latent diffusion models for high-resolution image\n",
      "synthesis. arXiv preprint arXiv:2307.01952, 2023.\n",
      "Qian, S., Kirschstein, T., Schoneveld, L., Davoli, D., Gieben-\n",
      "hain, S., and Nie√üner, M. Gaussianavatars: Photorealistic\n",
      "head avatars with rigged 3d gaussians. In Proceedings\n",
      "of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 20299‚Äì20309, 2024.\n",
      "Ren, Y ., Li, G., Chen, Y ., Li, T. H., and Liu, S. Pirenderer:\n",
      "Controllable portrait image generation via semantic neu-\n",
      "ral rendering. In Proceedings of the IEEE/CVF interna-\n",
      "tional conference on computer vision, pp. 13759‚Äì13768,\n",
      "2021.\n",
      "Ronneberger, O., Fischer, P., and Brox, T. U-net: Con-\n",
      "volutional networks for biomedical image segmenta-\n",
      "tion. In Medical image computing and computer-assisted\n",
      "intervention‚ÄìMICCAI 2015: 18th international confer-\n",
      "ence, Munich, Germany, October 5-9, 2015, proceedings,\n",
      "part III 18, pp. 234‚Äì241. Springer, 2015.\n",
      "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V .,\n",
      "Radford, A., and Chen, X. Improved techniques for\n",
      "training gans. Advances in neural information processing\n",
      "systems, 29, 2016.\n",
      "Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\n",
      "Chen, L.-C. Mobilenetv2: Inverted residuals and linear\n",
      "bottlenecks. In Proceedings of the IEEE conference on\n",
      "----------------------------------------------------------------------------------------------------\n",
      "352\n",
      "Radford, A., and Chen, X. Improved techniques for\n",
      "training gans. Advances in neural information processing\n",
      "systems, 29, 2016.\n",
      "Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and\n",
      "Chen, L.-C. Mobilenetv2: Inverted residuals and linear\n",
      "bottlenecks. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition, pp. 4510‚Äì4520,\n",
      "2018.\n",
      "12\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2493\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Sarkar, K., Golyanik, V ., Liu, L., and Theobalt, C. Style and\n",
      "pose control for image synthesis of humans from a single\n",
      "monocular view. arXiv preprint arXiv:2102.11263, 2021.\n",
      "Shakhmatov, A., Razzhigaev, A., Nikolich, A., Arkhipkin,\n",
      "V ., Pavlov, I., Kuznetsov, A., and Dimitrov, D. kandinsky\n",
      "2.2, 2023.\n",
      "Shu, C., Wu, H., Zhou, H., Liu, J., Hong, Z., Ding, C.,\n",
      "Han, J., Liu, J., Ding, E., and Wang, J. Few-shot head\n",
      "swapping in the wild. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 10789‚Äì10798, 2022.\n",
      "Siarohin, A., Lathuili `ere, S., Tulyakov, S., Ricci, E., and\n",
      "Sebe, N. First order motion model for image animation.\n",
      "In Conference on Neural Information Processing Systems\n",
      "(NeurIPS), December 2019.\n",
      "Simonyan, K. and Zisserman, A. Very deep convolu-\n",
      "tional networks for large-scale image recognition. arXiv\n",
      "preprint arXiv:1409.1556, 2014.\n",
      "Su, S., Yan, Q., Zhu, Y ., Zhang, C., Ge, X., Sun, J., and\n",
      "Zhang, Y . Blindly assess image quality in the wild guided\n",
      "by a self-adaptive hyper network. In IEEE/CVF Con-\n",
      "ference on Computer Vision and Pattern Recognition\n",
      "(CVPR), June 2020.\n",
      "Sudre, C. H., Li, W., Vercauteren, T., Ourselin, S., and\n",
      "Jorge Cardoso, M. Generalised dice overlap as a deep\n",
      "learning loss function for highly unbalanced segmen-\n",
      "tations. In Deep Learning in Medical Image Analysis\n",
      "and Multimodal Learning for Clinical Decision Support:\n",
      "Third International Workshop, DLMIA 2017, and 7th\n",
      "International Workshop, ML-CDS 2017, Held in Con-\n",
      "junction with MICCAI 2017, Qu¬¥ebec City, QC, Canada,\n",
      "September 14, Proceedings 3 , pp. 240‚Äì248. Springer,\n",
      "2017.\n",
      "Suvorov, R., Logacheva, E., Mashikhin, A., Remizova,\n",
      "A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H.,\n",
      "Park, K., and Lempitsky, V . Resolution-robust large\n",
      "mask inpainting with fourier convolutions. arXiv preprint\n",
      "arXiv:2109.07161, 2021.\n",
      "Wang, J., Chan, K. C., and Loy, C. C. Exploring clip for as-\n",
      "sessing the look and feel of images. In Proceedings of the\n",
      "AAAI Conference on Artificial Intelligence, volume 37,\n",
      "pp. 2555‚Äì2563, 2023.\n",
      "Wang, Q., Liu, L., Hua, M., Zhu, P., Zuo, W., Hu, Q., Lu, H.,\n",
      "and Cao, B. Hs-diffusion: Semantic-mixing diffusion for\n",
      "head swapping. arXiv preprint arXiv:2212.06458, 2022a.\n",
      "Wang, Q., Bai, X., Wang, H., Qin, Z., and Chen, A. Instan-\n",
      "tid: Zero-shot identity-preserving generation in seconds.\n",
      "arXiv preprint arXiv:2401.07519, 2024.\n",
      "Wang, Y ., Chen, X., Zhu, J., Chu, W., Tai, Y ., Wang, C., Li,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2465\n",
      "head swapping. arXiv preprint arXiv:2212.06458, 2022a.\n",
      "Wang, Q., Bai, X., Wang, H., Qin, Z., and Chen, A. Instan-\n",
      "tid: Zero-shot identity-preserving generation in seconds.\n",
      "arXiv preprint arXiv:2401.07519, 2024.\n",
      "Wang, Y ., Chen, X., Zhu, J., Chu, W., Tai, Y ., Wang, C., Li,\n",
      "J., Wu, Y ., Huang, F., and Ji, R. Hififace: 3d shape and\n",
      "semantic prior guided high fidelity face swapping. CoRR,\n",
      "abs/2106.09965, 2021.\n",
      "Wang, Y ., Yang, D., Br¬¥emond, F., and Dantcheva, A. Latent\n",
      "image animator: Learning to animate images via latent\n",
      "space navigation. ArXiv, abs/2203.09043, 2022b.\n",
      "Wang, Y ., Yang, D., Bremond, F., and Dantcheva, A. La-\n",
      "tent image animator: Learning to animate images via\n",
      "latent space navigation. In International Conference on\n",
      "Learning Representations, 2022c.\n",
      "Wang, Z., Simoncelli, E., and Bovik, A. Multiscale struc-\n",
      "tural similarity for image quality assessment. In The\n",
      "Thrity-Seventh Asilomar Conference on Signals, Systems\n",
      "and Computers, 2003, volume 2, pp. 1398‚Äì1402 V ol.2,\n",
      "2003. doi: 10.1109/ACSSC.2003.1292216.\n",
      "Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli,\n",
      "E. P. Image quality assessment: from error visibility\n",
      "to structural similarity. IEEE Transactions on Image\n",
      "Processing, 13:600‚Äì612, 2004.\n",
      "Wiles, O., Koepke, A., and Zisserman, A. X2face: A net-\n",
      "work for controlling face generation using images, audio,\n",
      "and pose codes. In Proceedings of the European confer-\n",
      "ence on computer vision (ECCV), pp. 670‚Äì686, 2018.\n",
      "Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M.,\n",
      "and Luo, P. Segformer: Simple and efficient design\n",
      "for semantic segmentation with transformers. In Neural\n",
      "Information Processing Systems (NeurIPS), 2021.\n",
      "Xie, S., Girshick, R., Doll¬¥ar, P., Tu, Z., and He, K. Aggre-\n",
      "gated residual transformations for deep neural networks.\n",
      "In Proceedings of the IEEE conference on computer vi-\n",
      "sion and pattern recognition, pp. 1492‚Äì1500, 2017.\n",
      "Xu, Y ., Wang, L., Zhao, X., Zhang, H., and Liu, Y . Avatar-\n",
      "mav: Fast 3d head avatar reconstruction using motion-\n",
      "aware neural voxels. In ACM SIGGRAPH 2023 Confer-\n",
      "ence Proceedings, pp. 1‚Äì10, 2023.\n",
      "Ye, Z., Zhong, T., Ren, Y ., Yang, J., Li, W., Huang, J., Jiang,\n",
      "Z., He, J., Huang, R., Liu, J., Zhang, C., Yin, X., Ma,\n",
      "Z., and Zhao, Z. Real3d-portrait: One-shot realistic 3d\n",
      "talking portrait synthesis. 2024.\n",
      "Yin, F., Zhang, Y ., Cun, X., Cao, M., Fan, Y ., Wang, X., Bai,\n",
      "Q., Wu, B., Wang, J., and Yang, Y . Styleheat: One-shot\n",
      "high-resolution editable talking face generation via pre-\n",
      "----------------------------------------------------------------------------------------------------\n",
      "471\n",
      "Z., and Zhao, Z. Real3d-portrait: One-shot realistic 3d\n",
      "talking portrait synthesis. 2024.\n",
      "Yin, F., Zhang, Y ., Cun, X., Cao, M., Fan, Y ., Wang, X., Bai,\n",
      "Q., Wu, B., Wang, J., and Yang, Y . Styleheat: One-shot\n",
      "high-resolution editable talking face generation via pre-\n",
      "trained stylegan. In European conference on computer\n",
      "vision, pp. 85‚Äì101. Springer, 2022.\n",
      "Yu, C., Wang, J., Peng, C., Gao, C., Yu, G., and Sang, N.\n",
      "Bisenet: Bilateral segmentation network for real-time\n",
      "13\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2134\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "semantic segmentation. In Proceedings of the European\n",
      "conference on computer vision (ECCV) , pp. 325‚Äì341,\n",
      "2018.\n",
      "Zakharov, E., Ivakhnenko, A., Shysheya, A., and Lempitsky,\n",
      "V . Fast bi-layer neural synthesis of one-shot realistic head\n",
      "avatars. In European Conference of Computer vision\n",
      "(ECCV), August 2020a.\n",
      "Zakharov, E., Ivakhnenko, A., Shysheya, A., and Lempitsky,\n",
      "V . Fast bi-layer neural synthesis of one-shot realistic\n",
      "head avatars. In Computer Vision‚ÄìECCV 2020: 16th\n",
      "European Conference, Glasgow, UK, August 23‚Äì28, 2020,\n",
      "Proceedings, Part XII 16, pp. 524‚Äì540. Springer, 2020b.\n",
      "Zhang, B., Qi, C., Zhang, P., Zhang, B., Wu, H., Chen, D.,\n",
      "Chen, Q., Wang, Y ., and Wen, F. Metaportrait: Identity-\n",
      "preserving talking head generation with fast personalized\n",
      "adaptation.\n",
      "Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,\n",
      "O. The unreasonable effectiveness of deep features as a\n",
      "perceptual metric. In CVPR, 2018.\n",
      "Zhao, J. and Zhang, H. Thin-plate spline motion model\n",
      "for image animation. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pp. 3657‚Äì3666, 2022.\n",
      "Zhao, W., Rao, Y ., Shi, W., Liu, Z., Zhou, J., and Lu, J.\n",
      "Diffswap: High-fidelity and controllable face swapping\n",
      "via 3d-aware masked diffusion. CVPR, 2023.\n",
      "Zheng, Y ., Abrevaya, V . F., B¬®uhler, M. C., Chen, X., Black,\n",
      "M. J., and Hilliges, O. I M Avatar: Implicit morphable\n",
      "head avatars from videos. InComputer Vision and Pattern\n",
      "Recognition (CVPR), 2022.\n",
      "Zheng, Y ., Yifan, W., Wetzstein, G., Black, M. J., and\n",
      "Hilliges, O. Pointavatar: Deformable point-based head\n",
      "avatars from videos. In Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition,\n",
      "pp. 21057‚Äì21067, 2023.\n",
      "Zhu, Y ., Li, Q., Wang, J., Xu, C., and Sun, Z. One shot face\n",
      "swapping on megapixels. In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition\n",
      "(CVPR), pp. 4834‚Äì4844, June 2021.\n",
      "Zielonka, W., Bolkart, T., and Thies, J. Instant volumetric\n",
      "head avatars. 2023 IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR), pp. 4574‚Äì4584,\n",
      "2022.\n",
      "14\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2473\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "A. Details on losses\n",
      "For training Aligner, we use the following combination of\n",
      "losses:\n",
      "Laligner = ŒªadvLadv + ŒªF MLF M + ŒªL1LL1\n",
      "+ŒªV GG\n",
      "perc LV GG\n",
      "perc + ŒªID\n",
      "percLID\n",
      "perc + ŒªID\n",
      "cosLID\n",
      "cos + ŒªdiceLdice+\n",
      "ŒªemoLemo + ŒªkptLkpt + Œªgaze Lgaze\n",
      "(11)\n",
      "We set the following weights for the losses: Œªadv = 0 .1,\n",
      "ŒªF M = 10, ŒªL1 = 30, ŒªV GG\n",
      "perc = 0.01, ŒªID\n",
      "perc = 2 √ó 10‚àí3,\n",
      "ŒªID\n",
      "cos = 0.01, Œªdice = 1, Œªemo = 1, Œªkpt = 30 and Œªgaze =\n",
      "0.5 For training Blender, we use the following losses:\n",
      "Lblender = ŒªadvLadv + ŒªL1LL1 + ŒªV GG\n",
      "perc LV GG\n",
      "perc\n",
      "+ŒªcLc + ŒªcLc‚Ä≤ + ŒªregLreg\n",
      "(12)\n",
      "The weighs are set as Œªadv = 1, ŒªL1 = 1, ŒªV GG\n",
      "perc = 0.01,\n",
      "Œªc = 1 and Œªreg = 1.\n",
      "B. Training details\n",
      "We trained Aligner for 1 000 000 iterations with batch size\n",
      "of 20 on 512x512 resolution, and for 800 000 iterations\n",
      "with batch size of 32 on 256x256 resolution. On 8 NVIDIA\n",
      "A 100 GPUs, it takes 27 and 9 days respectively. We use\n",
      "Adam optimizer (Kingma, 2014) with generator learning\n",
      "rate 1 √ó10‚àí4 and discriminator learning rate 4 √ó10‚àí4, with\n",
      "gradient clipping threshold of 10. We also apply spectral\n",
      "normalization (Miyato et al., 2018) to stabilize training.\n",
      "We trained Blender for 50000 iterations with batch size of\n",
      "20 on 512x512 resolution. On 4 NVIDIA A 100 GPUs it\n",
      "takes 2 days. We use Adam optimizer and same optimizer\n",
      "options as in Aligner.\n",
      "C. Detailed architecture\n",
      "Aligner We use ResNeXt-50 (Xie et al., 2017) as our por-\n",
      "trait encoder Epor, IResNet-50 (Duta et al., 2021) pretrained\n",
      "with Arcface (Deng et al., 2019) loss as identity encoderEid\n",
      "and MobileNetV2 (Sandler et al., 2018) as motion encoder\n",
      "Emotion. The dimensions of embeddings produced by these\n",
      "encoders are 512, 512 and 256, respectively.\n",
      "These embeddings are concatenated and processed with 2-\n",
      "layer MLP with ReLU activation and spectral normalization.\n",
      "The intermediate dimension is maintained the same as the\n",
      "input one. The resulting vector is supplied to AdaIn (Huang\n",
      "& Belongie, 2017) layers to condition the generator, which\n",
      "is borrowed from (Burkov et al., 2020). We add one addi-\n",
      "tional upsampling residual block to the original generator to\n",
      "increase output resolution from 256 to 512. Discriminator\n",
      "is also borrowed from (Burkov et al., 2020) in its default\n",
      "version.\n",
      "Table 6. Ablation of addition of keypoint Lkpt and emotion Lemo\n",
      "losses\n",
      "Experiment Cross-reenactment\n",
      "CSIM ‚Üë FID ‚Üì AKD ‚Üì\n",
      "with losses 0.621 26.83 0.0098\n",
      "w/o losses 0.607 28.60 0.0107\n",
      "Experiment Self-reenactment\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1976\n",
      "is also borrowed from (Burkov et al., 2020) in its default\n",
      "version.\n",
      "Table 6. Ablation of addition of keypoint Lkpt and emotion Lemo\n",
      "losses\n",
      "Experiment Cross-reenactment\n",
      "CSIM ‚Üë FID ‚Üì AKD ‚Üì\n",
      "with losses 0.621 26.83 0.0098\n",
      "w/o losses 0.607 28.60 0.0107\n",
      "Experiment Self-reenactment\n",
      "CSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë AKD ‚Üì\n",
      "with losses 0.745 0.150 21.87 0.845 0.0074\n",
      "w/o losses 0.724 0.154 21.97 0.841 0.0079\n",
      "Table 7. Ablation on the start epoch for gaze loss Lgaze\n",
      "Experiment Cross-reenactment\n",
      "CSIM ‚Üë FID ‚Üì AKD ‚Üì\n",
      "Epoch 1000 0.621 26.83 0.0098\n",
      "Epoch 10 0.538 37.22 0.0091\n",
      "Experiment Self-reenactment\n",
      "CSIM ‚Üë LPIPS ‚Üì PSNR ‚Üë SSIM ‚Üë AKD ‚Üì\n",
      "Epoch 1000 0.745 0.150 21.87 0.845 0.0074\n",
      "Epoch 10 0.699 0.166 21.47 0.837 0.0082\n",
      "D. Further ablations on Aligner\n",
      "We calculate Average Keypoint Distance (AKD) using key-\n",
      "points from DECA (Feng et al., 2021) model. Given a triplet\n",
      "of source IS, target IT and generated IA images, we com-\n",
      "pute absolute distance between pair of normalized keypoints,\n",
      "which are based on shape blensdshapes from IS and pose\n",
      "and expression blendshapes from IT and IA. In this way,\n",
      "we assess motion transfer, while keeping source appearance\n",
      "invariant.\n",
      "We also show the effect of adding keypointLkpt and emo-\n",
      "tion Lemo losses when training motion encoder Emtn. As\n",
      "can be seen in table 6, they improve image quality, identity\n",
      "preservation and pose transfer in cross-reenactment scenario,\n",
      "and generally lead to better disentanglement between tar-\n",
      "get motion and identity. On self-reenactment, metrics also\n",
      "generally improve with addition of these losses.\n",
      "Finally, we also justify our choice to include gaze lossLgaze\n",
      "only at the end of training after 1000 epochs in table 7.\n",
      "We compare it to the experiment when we include Lgaze\n",
      "only after 10 training epochs, when the model is capable\n",
      "to generate eyes with enough details. Early addition of this\n",
      "loss results in a significant deterioration of source identity\n",
      "preservation and in a noticeable fall in general quality of\n",
      "images.\n",
      "15\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1049\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Source Target GHOST 2.0 HeSer (Shu et al., 2022) StyleHEAT (Yin et al., 2022)\n",
      "Figure 12. Cross-reenactment results on 512 √ó 512\n",
      "resolution\n",
      "16\n",
      "\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Source Target GHOST 2.0 HeSer (Shu et al., 2022) StyleHEAT (Yin et al., 2022)\n",
      "Figure 13. Self-reenactment results on 512 √ó 512\n",
      "resolution\n",
      "17\n",
      "\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Source Target GHOST 2.0 DPE DaGAN LIA TPSMM\n",
      "Source Target GHOST 2.0 X2face FOMM Bi-layer PIRender\n",
      "Figure 14. Cross-reenactment results on 256 √ó 256\n",
      "resolution\n",
      "18\n",
      "\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Figure 15. Head swap results with the same identity and color augmentation applied to the source\n",
      "19\n",
      "\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Figure 16. Head swap results with different identities\n",
      "20\n",
      "\n",
      "GHOST 2.0: Generative High-fidelity One Shot Transfer of Heads\n",
      "Figure 17. Head swap results on real-life and outdoor photos\n",
      "21\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for t in texts:\n",
    "    print(len(t))\n",
    "    print(t)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('http://10.0.0.209:5001/search', json={\n",
    "    'query': 'transformer architecture',\n",
    "    'k': 2,                                      # optional, default 5\n",
    "    'return_scores': True                        # optional, default true\n",
    "})\n",
    "\n",
    "results = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_index': 9,\n",
       " 'chunk_source': 'pdf_original',\n",
       " 'content': '1. **üìò Topic and Domain:** The paper presents X-Fusion, a framework for extending pre-trained Large Language Models (LLMs) for multimodal tasks in computer vision and natural language processing.\\n\\n2. **üí° Previous Research and New Ideas:** Based on previous research in unified vision-language models and LLM adaptation, it introduces a novel dual-tower architecture that keeps the LLM frozen while adding vision-specific capabilities.\\n\\n3. **‚ùì Problem:** The paper addresses how to add new modalities (specifically vision) to pre-trained LLMs while preserving their original language capabilities and avoiding the need for full retraining.\\n\\n4. **üõ†Ô∏è Methods:** Uses a dual-tower architecture with frozen language weights and trainable vision-specific weights, employing both diffusion loss for images and autoregressive loss for text, while incorporating strategies for data ratio optimization and noise reduction.\\n\\n5. **üìä Results and Evaluation:** X-Fusion outperforms alternative architectures on both image-to-text and text-to-image tasks, with results showing that incorporating understanding-focused data improves generation quality, reducing image noise enhances performance, and feature alignment benefits smaller models more than larger ones.',\n",
       " 'date': '2025-04-30',\n",
       " 'flow_chart': '<svg width=\"100%\" viewBox=\"0 0 1000 800\" xmlns=\"http://www.w3.org/2000/svg\" font-family=\"Arial, sans-serif\">\\n\\n  <defs>\\n    <linearGradient id=\"grad1\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"0%\">\\n      <stop offset=\"0%\" style=\"stop-color:#4CAF50;stop-opacity:1\" />\\n      <stop offset=\"100%\" style=\"stop-color:#8BC34A;stop-opacity:1\" />\\n    </linearGradient>\\n    <linearGradient id=\"grad2\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"0%\">\\n      <stop offset=\"0%\" style=\"stop-color:#2196F3;stop-opacity:1\" />\\n      <stop offset=\"100%\" style=\"stop-color:#03A9F4;stop-opacity:1\" />\\n    </linearGradient>\\n    <linearGradient id=\"grad3\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"0%\">\\n      <stop offset=\"0%\" style=\"stop-color:#FF9800;stop-opacity:1\" />\\n      <stop offset=\"100%\" style=\"stop-color:#FFC107;stop-opacity:1\" />\\n    </linearGradient>\\n    <linearGradient id=\"grad4\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"0%\">\\n      <stop offset=\"0%\" style=\"stop-color:#9C27B0;stop-opacity:1\" />\\n      <stop offset=\"100%\" style=\"stop-color:#BA68C8;stop-opacity:1\" />\\n    </linearGradient>\\n    <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"0\" refY=\"3.5\" orient=\"auto\">\\n      <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"#555\" />\\n    </marker>\\n    <filter id=\"shadow\" x=\"-20%\" y=\"-20%\" width=\"140%\" height=\"140%\">\\n      <feGaussianBlur in=\"SourceAlpha\" stdDeviation=\"3\"/>\\n      <feOffset dx=\"2\" dy=\"2\" result=\"offsetblur\"/>\\n      <feComponentTransfer>\\n        <feFuncA type=\"linear\" slope=\"0.5\"/>\\n      </feComponentTransfer>\\n      <feMerge>\\n        <feMergeNode/>\\n        <feMergeNode in=\"SourceGraphic\"/>\\n      </feMerge>\\n    </filter>\\n  </defs>\\n\\n  <!-- Title -->\\n  <text x=\"500\" y=\"40\" font-size=\"28\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">X-Fusion Methodology Flowchart</text>\\n  <text x=\"500\" y=\"65\" font-size=\"16\" text-anchor=\"middle\" fill=\"#666\">Adapting Frozen LLMs for Vision Tasks</text>\\n\\n  <!-- Input Stage -->\\n  <g id=\"input_stage\" transform=\"translate(50, 100)\">\\n    <rect x=\"0\" y=\"0\" width=\"200\" height=\"100\" rx=\"10\" ry=\"10\" fill=\"url(#grad1)\" filter=\"url(#shadow)\"/>\\n    <text x=\"100\" y=\"30\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#fff\">Input Modalities</text>\\n    <text x=\"100\" y=\"60\" font-size=\"14\" text-anchor=\"middle\" fill=\"#fff\">üñºÔ∏è Image</text>\\n    <text x=\"100\" y=\"80\" font-size=\"14\" text-anchor=\"middle\" fill=\"#fff\">üìù Text</text>\\n  </g>\\n\\n  <!-- Tokenization/Encoding -->\\n  <g id=\"encoding_stage\" transform=\"translate(300, 100)\">\\n    <rect x=\"0\" y=\"0\" width=\"400\" height=\"100\" rx=\"10\" ry=\"10\" fill=\"url(#grad2)\" filter=\"url(#shadow)\"/>\\n    <text x=\"200\" y=\"25\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#fff\">Input Processing</text>\\n    <rect x=\"15\" y=\"40\" width=\"180\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#E3F2FD\"/>\\n    <text x=\"105\" y=\"58\" font-size=\"12\" text-anchor=\"middle\" fill=\"#1E88E5\">Image Encoding</text>\\n    <text x=\"105\" y=\"75\" font-size=\"10\" text-anchor=\"middle\" fill=\"#1E88E5\">(VAE Encoder + Patchify)</text>\\n    <rect x=\"205\" y=\"40\" width=\"180\" height=\"50\" rx=\"5\" ry=\"5\" fill=\"#E3F2FD\"/>\\n    <text x=\"295\" y=\"58\" font-size=\"12\" text-anchor=\"middle\" fill=\"#1E88E5\">Text Tokenization</text>\\n    <text x=\"295\" y=\"75\" font-size=\"10\" text-anchor=\"middle\" fill=\"#1E88E5\">(LLM Tokenizer)</text>\\n    <text x=\"200\" y=\"115\" font-size=\"12\" text-anchor=\"middle\" fill=\"#1976D2\">Interleaved Tokens [img, txt, img, ...]</text>\\n  </g>\\n\\n  <!-- Arrow 1 -->\\n  <line x1=\"250\" y1=\"150\" x2=\"300\" y2=\"150\" stroke=\"#555\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\" />\\n\\n  <!-- Core Model: Dual Tower -->\\n  <g id=\"dual_tower_stage\" transform=\"translate(200, 250)\">\\n    <rect x=\"0\" y=\"0\" width=\"600\" height=\"250\" rx=\"15\" ry=\"15\" fill=\"#F5F5F5\" stroke=\"#BDBDBD\" stroke-width=\"1\" filter=\"url(#shadow)\"/>\\n    <text x=\"300\" y=\"30\" font-size=\"18\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#424242\">X-Fusion Core: Dual Tower Architecture (Per Layer)</text>\\n\\n    <text x=\"300\" y=\"60\" font-size=\"14\" text-anchor=\"middle\" fill=\"#616161\">Input Sequence (Ein = {e1, e2, ...})</text>\\n    <line x1=\"300\" y1=\"70\" x2=\"300\" y2=\"90\" stroke=\"#9E9E9E\" stroke-width=\"2\"/>\\n    <line x1=\"150\" y1=\"90\" x2=\"450\" y2=\"90\" stroke=\"#9E9E9E\" stroke-width=\"2\"/>\\n    <line x1=\"150\" y1=\"90\" x2=\"150\" y2=\"110\" stroke=\"#9E9E9E\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\\n    <line x1=\"450\" y1=\"90\" x2=\"450\" y2=\"110\" stroke=\"#9E9E9E\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\\n\\n\\n    <!-- Frozen Text Tower -->\\n    <rect x=\"50\" y=\"110\" width=\"200\" height=\"80\" rx=\"10\" ry=\"10\" fill=\"#CFD8DC\"/>\\n    <text x=\"150\" y=\"135\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#37474F\">Frozen Text Tower</text>\\n    <text x=\"150\" y=\"155\" font-size=\"12\" text-anchor=\"middle\" fill=\"#37474F\">(Ftxt - LLM Block)</text>\\n    <text x=\"150\" y=\"175\" font-size=\"12\" text-anchor=\"middle\" fill=\"#37474F\">Output: Htxt</text>\\n\\n    <!-- Trainable Vision Tower -->\\n    <rect x=\"350\" y=\"110\" width=\"200\" height=\"80\" rx=\"10\" ry=\"10\" fill=\"url(#grad3)\" />\\n    <text x=\"450\" y=\"135\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#fff\">Trainable Vision Tower</text>\\n    <text x=\"450\" y=\"155\" font-size=\"12\" text-anchor=\"middle\" fill=\"#fff\">(Fimg - New Weights)</text>\\n    <text x=\"450\" y=\"175\" font-size=\"12\" text-anchor=\"middle\" fill=\"#fff\">Output: Himg</text>\\n\\n    <!-- Output Selection -->\\n    <line x1=\"150\" y1=\"190\" x2=\"150\" y2=\"210\" stroke=\"#9E9E9E\" stroke-width=\"2\"/>\\n    <line x1=\"450\" y1=\"190\" x2=\"450\" y2=\"210\" stroke=\"#9E9E9E\" stroke-width=\"2\"/>\\n    <line x1=\"150\" y1=\"210\" x2=\"450\" y2=\"210\" stroke=\"#9E9E9E\" stroke-width=\"2\"/>\\n    <line x1=\"300\" y1=\"210\" x2=\"300\" y2=\"230\" stroke=\"#9E9E9E\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\\n    <text x=\"300\" y=\"240\" font-size=\"14\" text-anchor=\"middle\" fill=\"#616161\">Output Selection (Hout): if token=text use Htxt, if token=vision use Himg</text>\\n\\n  </g>\\n\\n  <!-- Arrow 2 -->\\n  <line x1=\"500\" y1=\"200\" x2=\"500\" y2=\"250\" stroke=\"#555\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\" />\\n\\n  <!-- Arrow 3 -->\\n  <line x1=\"500\" y1=\"500\" x2=\"500\" y2=\"530\" stroke=\"#555\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\" />\\n\\n\\n  <!-- Output & Loss -->\\n  <g id=\"output_loss_stage\" transform=\"translate(200, 530)\">\\n     <rect x=\"0\" y=\"0\" width=\"600\" height=\"120\" rx=\"15\" ry=\"15\" fill=\"#F3E5F5\" stroke=\"#CE93D8\" stroke-width=\"1\" filter=\"url(#shadow)\"/>\\n     <text x=\"300\" y=\"25\" font-size=\"18\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#6A1B9A\">Output & Training Objective</text>\\n\\n     <!-- Text Output -->\\n     <rect x=\"20\" y=\"45\" width=\"170\" height=\"60\" rx=\"5\" ry=\"5\" fill=\"#E1BEE7\"/>\\n     <text x=\"105\" y=\"65\" font-size=\"12\" text-anchor=\"middle\" fill=\"#6A1B9A\">Text Output</text>\\n     <text x=\"105\" y=\"85\" font-size=\"12\" text-anchor=\"middle\" fill=\"#6A1B9A\">-> Autoregressive Loss (LAR)</text>\\n\\n     <!-- Image Output -->\\n     <rect x=\"210\" y=\"45\" width=\"170\" height=\"60\" rx=\"5\" ry=\"5\" fill=\"#E1BEE7\"/>\\n     <text x=\"295\" y=\"65\" font-size=\"12\" text-anchor=\"middle\" fill=\"#6A1B9A\">Image Feature Output</text>\\n     <text x=\"295\" y=\"85\" font-size=\"12\" text-anchor=\"middle\" fill=\"#6A1B9A\">-> Diffusion Loss (LDM)</text>\\n\\n     <!-- Combined Loss -->\\n     <rect x=\"400\" y=\"45\" width=\"180\" height=\"60\" rx=\"5\" ry=\"5\" fill=\"#CE93D8\"/>\\n     <text x=\"490\" y=\"65\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4A148C\">Total Loss</text>\\n     <text x=\"490\" y=\"85\" font-size=\"12\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4A148C\">L = ŒªAR*LAR + ŒªDM*LDM</text>\\n  </g>\\n\\n  <!-- Key Findings / Optional Steps -->\\n  <g id=\"findings\" transform=\"translate(820, 100)\">\\n    <rect x=\"0\" y=\"0\" width=\"160\" height=\"550\" rx=\"10\" ry=\"10\" fill=\"#FFF3E0\" stroke=\"#FFB74D\" stroke-width=\"1\" filter=\"url(#shadow)\"/>\\n    <text x=\"80\" y=\"25\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#E65100\">Key Insights & Options</text>\\n\\n    <rect x=\"10\" y=\"45\" width=\"140\" height=\"80\" rx=\"5\" ry=\"5\" fill=\"#FFE0B2\"/>\\n    <text x=\"80\" y=\"65\" font-size=\"11\" text-anchor=\"middle\" fill=\"#E65100\" font-weight=\"bold\">Language Preservation:</text>\\n    <text x=\"80\" y=\"80\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Text Tower Frozen</text>\\n    <text x=\"80\" y=\"95\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Retains LLM abilities (MMLU)</text>\\n\\n    <rect x=\"10\" y=\"135\" width=\"140\" height=\"90\" rx=\"5\" ry=\"5\" fill=\"#FFE0B2\"/>\\n    <text x=\"80\" y=\"155\" font-size=\"11\" text-anchor=\"middle\" fill=\"#E65100\" font-weight=\"bold\">Training Data Insights:</text>\\n    <text x=\"80\" y=\"170\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">1. Clean I2T Images:</text>\\n    <text x=\"80\" y=\"180\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">   Improves BOTH tasks</text>\\n    <text x=\"80\" y=\"195\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">2. Data Ratio (T2I:I2T ~2:1):</text>\\n    <text x=\"80\" y=\"205\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">   I2T helps T2I (not vice-versa)</text>\\n\\n    <rect x=\"10\" y=\"235\" width=\"140\" height=\"90\" rx=\"5\" ry=\"5\" fill=\"#FFE0B2\"/>\\n    <text x=\"80\" y=\"255\" font-size=\"11\" text-anchor=\"middle\" fill=\"#E65100\" font-weight=\"bold\">Optional Features:</text>\\n    <text x=\"80\" y=\"270\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">1. X-Fuse Layer:</text>\\n    <text x=\"80\" y=\"280\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">   Fuse tower outputs (+Perf, +FLOPs)</text>\\n    <text x=\"80\" y=\"295\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">2. Feature Alignment (REPA):</text>\\n    <text x=\"80\" y=\"305\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">   Align w/ CLIP (helps small models)</text>\\n\\n     <rect x=\"10\" y=\"335\" width=\"140\" height=\"60\" rx=\"5\" ry=\"5\" fill=\"#FFE0B2\"/>\\n    <text x=\"80\" y=\"355\" font-size=\"11\" text-anchor=\"middle\" fill=\"#E65100\" font-weight=\"bold\">Vision Tower Init:</text>\\n    <text x=\"80\" y=\"370\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Can init from pretrained</text>\\n     <text x=\"80\" y=\"380\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Diffusion Model (e.g., DiT)</text>\\n\\n    <rect x=\"10\" y=\"405\" width=\"140\" height=\"60\" rx=\"5\" ry=\"5\" fill=\"#FFE0B2\"/>\\n    <text x=\"80\" y=\"425\" font-size=\"11\" text-anchor=\"middle\" fill=\"#E65100\" font-weight=\"bold\">Evaluation:</text>\\n    <text x=\"80\" y=\"440\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">T2I (FID), I2T (BLIP)</text>\\n    <text x=\"80\" y=\"450\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Language (MMLU)</text>\\n\\n    <rect x=\"10\" y=\"475\" width=\"140\" height=\"60\" rx=\"5\" ry=\"5\" fill=\"#FFE0B2\"/>\\n    <text x=\"80\" y=\"495\" font-size=\"11\" text-anchor=\"middle\" fill=\"#E65100\" font-weight=\"bold\">Fine-tuning:</text>\\n    <text x=\"80\" y=\"510\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Extensible to VQA, Editing,</text>\\n    <text x=\"80\" y=\"520\" font-size=\"10\" text-anchor=\"middle\" fill=\"#E65100\">Localization, In/Out-painting</text>\\n\\n  </g>\\n\\n  <!-- Connection Lines -->\\n  <path d=\"M 500 200 Q 650 225 820 300\" stroke=\"#BDBDBD\" stroke-width=\"1.5\" stroke-dasharray=\"5,5\" fill=\"none\"/>\\n  <path d=\"M 500 500 Q 650 525 820 450\" stroke=\"#BDBDBD\" stroke-width=\"1.5\" stroke-dasharray=\"5,5\" fill=\"none\"/>\\n\\n  <!-- Final Output -->\\n   <g id=\"final_output\" transform=\"translate(350, 680)\">\\n     <rect x=\"0\" y=\"0\" width=\"300\" height=\"70\" rx=\"10\" ry=\"10\" fill=\"url(#grad4)\" filter=\"url(#shadow)\"/>\\n     <text x=\"150\" y=\"30\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#fff\">Unified Multimodal Model</text>\\n     <text x=\"150\" y=\"55\" font-size=\"12\" text-anchor=\"middle\" fill=\"#fff\">(Image Understanding & Generation + Language)</text>\\n   </g>\\n\\n   <!-- Arrow 4 -->\\n   <line x1=\"500\" y1=\"650\" x2=\"500\" y2=\"680\" stroke=\"#555\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\" />\\n\\n</svg>',\n",
       " 'pdf_pages': 18,\n",
       " 'published_at': '2025-04-29',\n",
       " 'questions': {'question1': {'answer': 'option2',\n",
       "   'option1': 'Training multimodal models from scratch efficiently.',\n",
       "   'option2': \"Preventing the loss of the LLM's original language abilities when adding vision.\",\n",
       "   'option3': 'Developing a new type of vision encoder entirely from scratch.',\n",
       "   'question': 'What is the primary challenge X-Fusion aims to address when introducing vision capabilities to Large Language Models (LLMs)?'},\n",
       "  'question2': {'answer': 'option2',\n",
       "   'option1': 'A single tower where the entire LLM is fine-tuned on multimodal data.',\n",
       "   'option2': 'A dual-tower design with a frozen language tower and a trainable vision tower.',\n",
       "   'option3': 'A gated layer added to each LLM block to handle visual information.',\n",
       "   'question': 'Which architectural design does X-Fusion employ to integrate vision into a frozen LLM?'},\n",
       "  'question3': {'answer': 'option2',\n",
       "   'option1': 'It significantly degrades image generation quality.',\n",
       "   'option2': 'It enhances image generation quality.',\n",
       "   'option3': 'It has no noticeable impact on image generation performance.',\n",
       "   'question': \"Based on the paper's findings, how does incorporating image understanding data affect image generation performance in X-Fusion?\"}},\n",
       " 'title': 'X-Fusion: Introducing New Modality to Frozen Large Language Models',\n",
       " 'total_chunks': 41,\n",
       " 'url': 'http://arxiv.org/pdf/2504.20996'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chunk_index', 'chunk_source', 'content', 'date', 'flow_chart', 'pdf_pages', 'published_at', 'questions', 'title', 'total_chunks', 'url'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X-Fusion: Introducing New Modality to Frozen Large Language Models'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results'][0]['metadata']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('http://10.0.0.209:5001/query', json={\n",
    "    'sql': \"SELECT * FROM papers LIMIT 1\"\n",
    "})\n",
    "\n",
    "results = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_results': 1,\n",
       " 'results': [{'content': \"Here's an analysis of the paper, following the requested format:\\n\\n1.  **üìò Topic and Domain:** The paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT) for Large Vision-Language Models (LVLMs) in the domain of multi-modal machine learning, specifically focusing on visual perception tasks.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds on Reinforcement Fine-Tuning (RFT) and verifiable rewards used in large reasoning models like DeepSeek-R1, and proposes extending this approach to visual tasks by designing task-specific, rule-based verifiable reward functions (e.g., IoU reward for object detection).\\n\\n3.  **‚ùì Problem:** The paper aims to solve the data inefficiency problem of supervised fine-tuning (SFT) for LVLMs in visual perception tasks, and to extend the application of RFT beyond math and code to the visual domain.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used Visual-RFT, which employs LVLMs to generate multiple responses with reasoning tokens, verifiable reward functions (IoU and CLS rewards), and policy optimization algorithms like Group Relative Policy Optimization (GRPO).\\n\\n5.  **üìä Results and Evaluation:** Visual-RFT significantly outperformed SFT on few-shot fine-grained image classification, few-shot object detection, reasoning grounding, and open-vocabulary object detection benchmarks, demonstrating improved data efficiency and generalization ability.\\n\",\n",
       "   'published_at': '2025-03-03',\n",
       "   'title': 'Visual-RFT: Visual Reinforcement Fine-Tuning',\n",
       "   'url': 'http://arxiv.org/pdf/2503.01785'}],\n",
       " 'sql': 'SELECT * FROM papers LIMIT 1'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post('http://10.0.0.209:5001/query', json={\n",
    "    'sql': \"SELECT * FROM papers LIMIT 1\"\n",
    "})\n",
    "\n",
    "results = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': [{'description': 'paper title', 'name': 'title', 'type': 'TEXT'},\n",
       "  {'description': 'publication date in YYYY-MM-DD format',\n",
       "   'name': 'published_at',\n",
       "   'type': 'TEXT'},\n",
       "  {'description': 'arXiv link to the paper', 'name': 'url', 'type': 'TEXT'},\n",
       "  {'description': 'paper summary/abstract',\n",
       "   'name': 'content',\n",
       "   'type': 'TEXT'}],\n",
       " 'table_name': 'papers'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('http://10.0.0.209:5001/schema')\n",
    "results = response.json()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': {'content': \"Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need. How about you‚Äîhow are you doing today? Anything on your mind or something I can assist with? üòä\", 'role': 'assistant'}, 'model': 'ministral-3:8b', 'timestamp': '2026-02-16T00:33:52.441858'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    'http://192.168.1.164:8081/chat',\n",
    "    json={\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': 'Hello, how are you?'}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LocalServer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
