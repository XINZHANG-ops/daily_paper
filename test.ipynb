{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.transparenttextures.com/\n",
    "\n",
    "https://xinzhang-ops.github.io/daily_paper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "1. üìò Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. 2. üí° Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. 3. ‚ùì Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. 4. üõ†Ô∏è Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. 5. üìä Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. üìò Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. 2. üí° Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. 3. ‚ùì Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. 4. üõ†Ô∏è Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. 5. üìä Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_formatted = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', content)\n",
    "content_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '1. üìò Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. ',\n",
       " '2. üí° Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. ',\n",
       " '3. ‚ùì Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. ',\n",
       " '4. üõ†Ô∏è Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. ',\n",
       " '5. üìä Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'(?=\\d+\\.\\s*[üìòüí°‚ùìüõ†Ô∏èüìä])', content_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "from string import Template \n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# HTML Ê®°ÊùøÔºö‰∏ªÈ°µÈù¢Ôºå‰ΩøÁî® $ ‰Ωú‰∏∫Âç†‰ΩçÁ¨¶\n",
    "INDEX_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Daily Paper</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            color: #333;\n",
    "        }\n",
    "        ul {\n",
    "            list-style: none;\n",
    "            padding: 0;\n",
    "        }\n",
    "        li {\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        a {\n",
    "            text-decoration: none;\n",
    "            color: #1a73e8;\n",
    "        }\n",
    "        a:hover {\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Daily Paper</h1>\n",
    "    <ul>\n",
    "        $date_links\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML Ê®°ÊùøÔºöÂ≠êÈ°µÈù¢Ôºå‰ΩøÁî® $ ‰Ωú‰∏∫Âç†‰ΩçÁ¨¶\n",
    "SUBPAGE_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>$date ËÆ∫ÊñáÊé®ÈÄÅ</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        .paper-card {\n",
    "            background-color: #f9f9f9;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "            padding: 15px;\n",
    "            margin-bottom: 20px;\n",
    "            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */\n",
    "        }\n",
    "        .paper-card:hover {\n",
    "            transform: translateY(-5px); /* Lift effect on hover */\n",
    "            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */\n",
    "        }\n",
    "        .paper-card h2 {\n",
    "            margin: 0 0 10px;\n",
    "            font-size: 1.2em;\n",
    "        }\n",
    "        .paper-card p {\n",
    "            margin: 5px 0;\n",
    "        }\n",
    "        .paper-card a {\n",
    "            color: #1a73e8;\n",
    "            text-decoration: none;\n",
    "        }\n",
    "        .paper-card a:hover {\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "        .category-chunk {\n",
    "            padding: 10px;\n",
    "            margin: 5px 0;\n",
    "            border-radius: 5px;\n",
    "            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */\n",
    "        }\n",
    "        .category-chunk:hover {\n",
    "            transform: translateY(-3px); /* Slightly smaller lift for categories */\n",
    "            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */\n",
    "        }\n",
    "        .category-chunk:nth-child(1) { /* 1. Topic and Domain */\n",
    "            background-color: #d3e3fd; /* Blue */\n",
    "        }\n",
    "        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */\n",
    "            background-color: #e6d6fa; /* Purple */\n",
    "        }\n",
    "        .category-chunk:nth-child(3) { /* 3. Problem */\n",
    "            background-color: #d4f8d9; /* Green */\n",
    "        }\n",
    "        .category-chunk:nth-child(4) { /* 4. Methods */\n",
    "            background-color: #ffd7d5; /* Pink */\n",
    "        }\n",
    "        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */\n",
    "            background-color: #d3e3fd; /* Reuse Blue */\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>$date ËÆ∫ÊñáÊé®ÈÄÅ</h1>\n",
    "    $paper_content\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_categories(text):\n",
    "    \"\"\"\n",
    "    Extract the 5 categories and their content from a formatted text string.\n",
    "    \n",
    "    Each category is identified by its unique emoji (üìò, üí°, ‚ùì, üõ†Ô∏è, üìä) regardless of the\n",
    "    exact title text. The function maps these to standardized category names.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the 5 categories\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with standardized category titles as keys and their content as values\n",
    "    \"\"\"\n",
    "    # Define patterns based on emojis only, not the category titles\n",
    "    patterns = [\n",
    "        (r'\\d+\\.\\s+\\*\\*üìò.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"üìò Topic and Domain\",),\n",
    "        (r'\\d+\\.\\s+\\*\\*üí°.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"üí° Previous Research and New Ideas\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*‚ùì.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"‚ùì Problem\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*üõ†Ô∏è.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"üõ†Ô∏è Methods\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*üìä.*?\\*\\*\\s+(.*?)(?=\\n\\n|\\Z)', \"üìä Results and Evaluation\")\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store results\n",
    "    results = []\n",
    "    \n",
    "    # Apply each pattern and store results with standardized category names\n",
    "    for pattern, category_name in patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            results.append((category_name, match.group(1).strip()))    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_paper_html(articles):\n",
    "    \"\"\"ÁîüÊàêÂ≠êÈ°µÈù¢ÁöÑËÆ∫ÊñáÂÜÖÂÆπ HTMLÔºå‰∏é Google Chat Êé®ÈÄÅÂÜÖÂÆπ‰∏ÄËá¥\"\"\"\n",
    "    # logger.debug(articles)\n",
    "    paper_html = \"\"\n",
    "    for idx, article in enumerate(articles):\n",
    "        title = article.get('title', 'No Title')\n",
    "        published_at = article.get('published_at', 'No Date')\n",
    "        url = article.get('url', '#')\n",
    "        content = article.get('content', 'No Content')\n",
    "        categories = extract_categories(content)\n",
    "        # ‰∏∫ÊØè‰∏™Á±ªÂà´Ê∑ªÂä† div ÂíåÊ†∑Âºè\n",
    "        content_html = \"\"\n",
    "        for idx, (cat, cat_content) in enumerate(categories):\n",
    "            content_html += f\"\"\"<div class=\"category-chunk\">{idx+1}.  <strong>{cat}:</strong> {cat_content}</div>\"\"\"\n",
    "\n",
    "        paper_html += f\"\"\"\n",
    "        <div class=\"paper-card\">\n",
    "            <h2>Paper: {idx+1}</h2>\n",
    "            <p><strong>{title}</strong></p>\n",
    "            <p><strong>Published: </strong>{published_at}</p>\n",
    "            <p><strong>Link: </strong><a href=\"{url}\" target=\"_blank\">{url}</a></p>\n",
    "            <div>{content_html}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    return paper_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [{'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'published_at': '2025-03-19', 'url': 'http://arxiv.org/pdf/2503.15417', 'content': '1.  **üìò Topic and Domain:** The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.\\n\\n5.  **üìä Results and Evaluation:** FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.\\n'}, {'title': 'Optimizing Decomposition for Optimal Claim Verification', 'published_at': '2025-03-19', 'url': 'http://arxiv.org/pdf/2503.15354', 'content': 'Here\\'s a concise analysis of the paper based on your requested format:\\n\\n1.  **üìò Topic and Domain:** The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don\\'t generate subclaims with optimal atomicity for downstream verification.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.\\n\\n5.  **üìä Results and Evaluation:** The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.\\n'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.  **üìò Topic and Domain:** The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.\\n\\n5.  **üìä Results and Evaluation:** FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a concise analysis of the paper based on your requested format:\\n\\n1.  **üìò Topic and Domain:** The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.\\n\\n2.  **üí° Previous Research and New Ideas:** The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.\\n\\n3.  **‚ùì Problem:** The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don\\'t generate subclaims with optimal atomicity for downstream verification.\\n\\n4.  **üõ†Ô∏è Methods:** The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.\\n\\n5.  **üìä Results and Evaluation:** The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <div class=\"paper-card\">\n",
      "            <h2>Paper: 5</h2>\n",
      "            <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>\n",
      "            <p><strong>Published: </strong>2025-03-19</p>\n",
      "            <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15417\" target=\"_blank\">http://arxiv.org/pdf/2503.15417</a></p>\n",
      "            <div><div class=\"category-chunk\">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.</div><div class=\"category-chunk\">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.</div><div class=\"category-chunk\">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.</div><div class=\"category-chunk\">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.</div><div class=\"category-chunk\">5.  <strong>üìä Results and Evaluation:</strong> FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.</div></div>\n",
      "        </div>\n",
      "        \n",
      "        <div class=\"paper-card\">\n",
      "            <h2>Paper: 5</h2>\n",
      "            <p><strong>Optimizing Decomposition for Optimal Claim Verification</strong></p>\n",
      "            <p><strong>Published: </strong>2025-03-19</p>\n",
      "            <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15354\" target=\"_blank\">http://arxiv.org/pdf/2503.15354</a></p>\n",
      "            <div><div class=\"category-chunk\">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.</div><div class=\"category-chunk\">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.</div><div class=\"category-chunk\">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don't generate subclaims with optimal atomicity for downstream verification.</div><div class=\"category-chunk\">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.</div><div class=\"category-chunk\">5.  <strong>üìä Results and Evaluation:</strong> The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.</div></div>\n",
      "        </div>\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(generate_paper_html(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"\"\"\n",
    "<div class=\"paper-card\">\n",
    "    <h2>Paper: 1</h2>\n",
    "    <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>\n",
    "    <p><strong>Published: </strong>2025-03-19</p>\n",
    "    <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15417\" target=\"_blank\">http://arxiv.org/pdf/2503.15417</a></p>\n",
    "    <div>\n",
    "        <div class=\"category-chunk\">1.  <strong>üìò Topic and Domain:</strong> The paper focuses on improving the temporal quality of video generation, specifically addressing temporal coherence and diversity, within the domain of computer vision and deep learning.</div>\n",
    "        <div class=\"category-chunk\">2.  <strong>üí° Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization), and proposes FLUX FLOW, a novel data-level temporal augmentation strategy.</div>\n",
    "        <div class=\"category-chunk\">3.  <strong>‚ùì Problem:</strong> The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion) and limited temporal diversity in videos generated by current video generation models.</div>\n",
    "        <div class=\"category-chunk\">4.  <strong>üõ†Ô∏è Methods:</strong> The authors used FLUX FLOW, which involves frame-level and block-level temporal perturbations (random shuffling of frames or blocks of frames) during the training of video generation models.</div>\n",
    "        <div class=\"category-chunk\">5.  <strong>üìä Results and Evaluation:</strong> FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and various VBench temporal and frame-wise quality scores, and was further supported by a user study.</div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "summaries = []\n",
    "with open('summaries.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        summaries.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GENAI_GATEWAY_API_KEY = os.getenv(\"GENAI_GATEWAY_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = GENAI_GATEWAY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from geotab_genai.genai_gateway_client import GenaiGatewayClient\n",
    "\n",
    "\n",
    "claude35_sonnet_v2 = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='anthropics',\n",
    "    chat_model='claude-3-5-sonnet-v2',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "\n",
    "claude37_sonnet = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='anthropics',\n",
    "    chat_model='claude-3-7-sonnet',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "gemini_20_pro = GenaiGatewayClient(\n",
    "    api_key=os.getenv(\"GENAI_GATEWAY_API_KEY\"),\n",
    "    env=\"staging\",\n",
    "    jurisdiction=\"us\",\n",
    "    temperature=0.8,\n",
    "    provider='vertex-ai',\n",
    "    chat_model='gemini-2.0-pro',\n",
    "    max_tokens=8192,\n",
    "    safety_filtering='off'\n",
    ")\n",
    "\n",
    "\n",
    "model_map = {\n",
    "    'claude35': claude35_sonnet_v2,\n",
    "    # 'claude35_haiku': claude35_haiku,\n",
    "    'claude37': claude37_sonnet,\n",
    "    '2.0 pro': gemini_20_pro,\n",
    "}\n",
    "\n",
    "\n",
    "def model_response(prompt, model_name, max_tokens=4096):\n",
    "    model = model_map[model_name]\n",
    "    version = None\n",
    "    if model_name == '2.0 flash':\n",
    "        version = '001'\n",
    "    response = model.create_message(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        provider=model._provider,\n",
    "        model=model._chat_model,\n",
    "        version=version,\n",
    "\n",
    "    )['message']['content']\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_article.txt', 'r') as f:\n",
    "    test_article = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a research assistant. You job is to help me to create a flow chart of the paper content. \n",
    "Since it is about the workflow of the paper, your focus is the method applied in the paper.\n",
    "\n",
    "Your should contain your answer in a SVG format as following format:\n",
    "<format>\n",
    "you should have your output with this specific <svg> tag.\n",
    "\n",
    "<svg width=\"100%\" viewBox=\"0 0 1000 800\">\n",
    "Here are the content you can create freely, use all shapes, text format or styles as you like.\n",
    "Try to be creative, and make it look good and colorful.\n",
    "</svg>\n",
    "\n",
    "</format>\n",
    "\n",
    "Here is the content of the paper:\n",
    "<content>\n",
    "{article_content}\n",
    "</content>\n",
    "\n",
    "Now please give me the SVG format of the flow chart, you should only give me the SVG format directly, do not output backticks for formatting, no other text.\n",
    "\"\"\"\n",
    "\n",
    "prompt = system_prompt.format(article_content=test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_response(prompt, 'claude35', max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<svg width=\"100%\" viewBox=\"0 0 1000 800\">\n",
      "    <!-- Background -->\n",
      "    <rect x=\"0\" y=\"0\" width=\"1000\" height=\"800\" fill=\"#f8f9fa\"/>\n",
      "    \n",
      "    <!-- Title -->\n",
      "    <text x=\"500\" y=\"50\" text-anchor=\"middle\" font-size=\"24\" font-weight=\"bold\" fill=\"#2c3e50\">DeepSeek LLM Workflow</text>\n",
      "    \n",
      "    <!-- Main Flow -->\n",
      "    <g transform=\"translate(0,0)\">\n",
      "        <!-- Pre-training Phase -->\n",
      "        <rect x=\"100\" y=\"100\" width=\"200\" height=\"80\" rx=\"10\" fill=\"#3498db\" opacity=\"0.8\"/>\n",
      "        <text x=\"200\" y=\"145\" text-anchor=\"middle\" fill=\"white\" font-size=\"16\">Pre-training Data</text>\n",
      "        <text x=\"200\" y=\"165\" text-anchor=\"middle\" fill=\"white\" font-size=\"12\">2 Trillion Tokens</text>\n",
      "        \n",
      "        <!-- Architecture -->\n",
      "        <rect x=\"400\" y=\"100\" width=\"200\" height=\"80\" rx=\"10\" fill=\"#2ecc71\" opacity=\"0.8\"/>\n",
      "        <text x=\"500\" y=\"145\" text-anchor=\"middle\" fill=\"white\" font-size=\"16\">Model Architecture</text>\n",
      "        <text x=\"500\" y=\"165\" text-anchor=\"middle\" fill=\"white\" font-size=\"12\">LLaMA-based</text>\n",
      "        \n",
      "        <!-- Base Models -->\n",
      "        <rect x=\"700\" y=\"100\" width=\"200\" height=\"80\" rx=\"10\" fill=\"#e74c3c\" opacity=\"0.8\"/>\n",
      "        <text x=\"800\" y=\"145\" text-anchor=\"middle\" fill=\"white\" font-size=\"16\">Base Models</text>\n",
      "        <text x=\"800\" y=\"165\" text-anchor=\"middle\" fill=\"white\" font-size=\"12\">7B and 67B Configs</text>\n",
      "        \n",
      "        <!-- Fine-tuning -->\n",
      "        <rect x=\"400\" y=\"300\" width=\"200\" height=\"80\" rx=\"10\" fill=\"#9b59b6\" opacity=\"0.8\"/>\n",
      "        <text x=\"500\" y=\"345\" text-anchor=\"middle\" fill=\"white\" font-size=\"16\">SFT</text>\n",
      "        <text x=\"500\" y=\"365\" text-anchor=\"middle\" fill=\"white\" font-size=\"12\">1M+ Instances</text>\n",
      "        \n",
      "        <!-- DPO -->\n",
      "        <rect x=\"400\" y=\"500\" width=\"200\" height=\"80\" rx=\"10\" fill=\"#f1c40f\" opacity=\"0.8\"/>\n",
      "        <text x=\"500\" y=\"545\" text-anchor=\"middle\" fill=\"white\" font-size=\"16\">DPO</text>\n",
      "        <text x=\"500\" y=\"565\" text-anchor=\"middle\" fill=\"white\" font-size=\"12\">Preference Optimization</text>\n",
      "        \n",
      "        <!-- Final Chat Models -->\n",
      "        <rect x=\"400\" y=\"700\" width=\"200\" height=\"80\" rx=\"10\" fill=\"#1abc9c\" opacity=\"0.8\"/>\n",
      "        <text x=\"500\" y=\"745\" text-anchor=\"middle\" fill=\"white\" font-size=\"16\">DeepSeek Chat Models</text>\n",
      "        \n",
      "        <!-- Connecting Lines -->\n",
      "        <path d=\"M300 140 L400 140\" stroke=\"#95a5a6\" stroke-width=\"2\" fill=\"none\"/>\n",
      "        <path d=\"M600 140 L700 140\" stroke=\"#95a5a6\" stroke-width=\"2\" fill=\"none\"/>\n",
      "        <path d=\"M800 180 L800 340 L600 340\" stroke=\"#95a5a6\" stroke-width=\"2\" fill=\"none\"/>\n",
      "        <path d=\"M500 380 L500 500\" stroke=\"#95a5a6\" stroke-width=\"2\" fill=\"none\"/>\n",
      "        <path d=\"M500 580 L500 700\" stroke=\"#95a5a6\" stroke-width=\"2\" fill=\"none\"/>\n",
      "        \n",
      "        <!-- Arrows -->\n",
      "        <polygon points=\"395,140 405,135 405,145\" fill=\"#95a5a6\"/>\n",
      "        <polygon points=\"695,140 705,135 705,145\" fill=\"#95a5a6\"/>\n",
      "        <polygon points=\"605,340 595,335 595,345\" fill=\"#95a5a6\"/>\n",
      "        <polygon points=\"500,495 495,485 505,485\" fill=\"#95a5a6\"/>\n",
      "        <polygon points=\"500,695 495,685 505,685\" fill=\"#95a5a6\"/>\n",
      "    </g>\n",
      "</svg>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
