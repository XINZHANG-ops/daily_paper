{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.transparenttextures.com/\n",
    "\n",
    "https://xinzhang-ops.github.io/daily_paper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "1. ğŸ“˜ Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. 2. ğŸ’¡ Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. 3. â“ Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. 4. ğŸ› ï¸ Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. 5. ğŸ“Š Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. ğŸ“˜ Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. 2. ğŸ’¡ Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. 3. â“ Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. 4. ğŸ› ï¸ Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. 5. ğŸ“Š Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_formatted = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', content)\n",
    "content_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '1. ğŸ“˜ Topic and Domain: The paper focuses on improving the temporal quality of generated videos, specifically addressing temporal coherence and diversity, within the domain of video generation. ',\n",
       " '2. ğŸ’¡ Previous Research and New Ideas: The paper builds upon existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization) and proposes FLUX FLOW, a novel data-level temporal augmentation strategy. ',\n",
       " '3. â“ Problem: The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion, repetitive dynamics) and limited temporal diversity in videos produced by existing video generation models. ',\n",
       " '4. ğŸ› ï¸ Methods: The authors used FLUX FLOW, which introduces controlled temporal perturbations at the data level through frame-level (random shuffling of frames) and block-level (reordering of frame blocks) operations during training. ',\n",
       " '5. ğŸ“Š Results and Evaluation: FLUX FLOW significantly improved temporal coherence and diversity across various video generation models on UCF-101 and VBench benchmarks, while maintaining or improving spatial fidelity, as evaluated using metrics like FVD, IS, and various VBench dimensions, supported by user studies.\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'(?=\\d+\\.\\s*[ğŸ“˜ğŸ’¡â“ğŸ› ï¸ğŸ“Š])', content_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n",
    "from string import Template \n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# HTML æ¨¡æ¿ï¼šä¸»é¡µé¢ï¼Œä½¿ç”¨ $ ä½œä¸ºå ä½ç¬¦\n",
    "INDEX_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Daily Paper</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            color: #333;\n",
    "        }\n",
    "        ul {\n",
    "            list-style: none;\n",
    "            padding: 0;\n",
    "        }\n",
    "        li {\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        a {\n",
    "            text-decoration: none;\n",
    "            color: #1a73e8;\n",
    "        }\n",
    "        a:hover {\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Daily Paper</h1>\n",
    "    <ul>\n",
    "        $date_links\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML æ¨¡æ¿ï¼šå­é¡µé¢ï¼Œä½¿ç”¨ $ ä½œä¸ºå ä½ç¬¦\n",
    "SUBPAGE_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"zh-CN\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>$date è®ºæ–‡æ¨é€</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            line-height: 1.6;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        .paper-card {\n",
    "            background-color: #f9f9f9;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 5px;\n",
    "            padding: 15px;\n",
    "            margin-bottom: 20px;\n",
    "            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */\n",
    "        }\n",
    "        .paper-card:hover {\n",
    "            transform: translateY(-5px); /* Lift effect on hover */\n",
    "            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2); /* Shadow on hover */\n",
    "        }\n",
    "        .paper-card h2 {\n",
    "            margin: 0 0 10px;\n",
    "            font-size: 1.2em;\n",
    "        }\n",
    "        .paper-card p {\n",
    "            margin: 5px 0;\n",
    "        }\n",
    "        .paper-card a {\n",
    "            color: #1a73e8;\n",
    "            text-decoration: none;\n",
    "        }\n",
    "        .paper-card a:hover {\n",
    "            text-decoration: underline;\n",
    "        }\n",
    "        .category-chunk {\n",
    "            padding: 10px;\n",
    "            margin: 5px 0;\n",
    "            border-radius: 5px;\n",
    "            transition: transform 0.2s, box-shadow 0.2s; /* Smooth transition for hover effect */\n",
    "        }\n",
    "        .category-chunk:hover {\n",
    "            transform: translateY(-3px); /* Slightly smaller lift for categories */\n",
    "            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.15); /* Slightly smaller shadow for categories */\n",
    "        }\n",
    "        .category-chunk:nth-child(1) { /* 1. Topic and Domain */\n",
    "            background-color: #d3e3fd; /* Blue */\n",
    "        }\n",
    "        .category-chunk:nth-child(2) { /* 2. Previous Research and New Ideas */\n",
    "            background-color: #e6d6fa; /* Purple */\n",
    "        }\n",
    "        .category-chunk:nth-child(3) { /* 3. Problem */\n",
    "            background-color: #d4f8d9; /* Green */\n",
    "        }\n",
    "        .category-chunk:nth-child(4) { /* 4. Methods */\n",
    "            background-color: #ffd7d5; /* Pink */\n",
    "        }\n",
    "        .category-chunk:nth-child(5) { /* 5. Results and Evaluation */\n",
    "            background-color: #d3e3fd; /* Reuse Blue */\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>$date è®ºæ–‡æ¨é€</h1>\n",
    "    $paper_content\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_categories(text):\n",
    "    \"\"\"\n",
    "    Extract the 5 categories and their content from a formatted text string.\n",
    "    \n",
    "    Each category is identified by its unique emoji (ğŸ“˜, ğŸ’¡, â“, ğŸ› ï¸, ğŸ“Š) regardless of the\n",
    "    exact title text. The function maps these to standardized category names.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the 5 categories\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with standardized category titles as keys and their content as values\n",
    "    \"\"\"\n",
    "    # Define patterns based on emojis only, not the category titles\n",
    "    patterns = [\n",
    "        (r'\\d+\\.\\s+\\*\\*ğŸ“˜.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"ğŸ“˜ Topic and Domain\",),\n",
    "        (r'\\d+\\.\\s+\\*\\*ğŸ’¡.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"ğŸ’¡ Previous Research and New Ideas\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*â“.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"â“ Problem\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*ğŸ› ï¸.*?\\*\\*\\s+(.*?)(?=\\n\\n\\d+\\.|\\Z)', \"ğŸ› ï¸ Methods\"),\n",
    "        (r'\\d+\\.\\s+\\*\\*ğŸ“Š.*?\\*\\*\\s+(.*?)(?=\\n\\n|\\Z)', \"ğŸ“Š Results and Evaluation\")\n",
    "    ]\n",
    "    \n",
    "    # Create a dictionary to store results\n",
    "    results = []\n",
    "    \n",
    "    # Apply each pattern and store results with standardized category names\n",
    "    for pattern, category_name in patterns:\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            results.append((category_name, match.group(1).strip()))    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def generate_paper_html(articles):\n",
    "    \"\"\"ç”Ÿæˆå­é¡µé¢çš„è®ºæ–‡å†…å®¹ HTMLï¼Œä¸ Google Chat æ¨é€å†…å®¹ä¸€è‡´\"\"\"\n",
    "    # logger.debug(articles)\n",
    "    paper_html = \"\"\n",
    "    for idx, article in enumerate(articles):\n",
    "        title = article.get('title', 'No Title')\n",
    "        published_at = article.get('published_at', 'No Date')\n",
    "        url = article.get('url', '#')\n",
    "        content = article.get('content', 'No Content')\n",
    "        categories = extract_categories(content)\n",
    "        # ä¸ºæ¯ä¸ªç±»åˆ«æ·»åŠ  div å’Œæ ·å¼\n",
    "        content_html = \"\"\n",
    "        for idx, (cat, cat_content) in enumerate(categories):\n",
    "            content_html += f\"\"\"<div class=\"category-chunk\">{idx+1}.  <strong>{cat}:</strong> {cat_content}</div>\"\"\"\n",
    "\n",
    "        paper_html += f\"\"\"\n",
    "        <div class=\"paper-card\">\n",
    "            <h2>Paper: {idx+1}</h2>\n",
    "            <p><strong>{title}</strong></p>\n",
    "            <p><strong>Published: </strong>{published_at}</p>\n",
    "            <p><strong>Link: </strong><a href=\"{url}\" target=\"_blank\">{url}</a></p>\n",
    "            <div>{content_html}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    return paper_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [{'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'published_at': '2025-03-19', 'url': 'http://arxiv.org/pdf/2503.15417', 'content': '1.  **ğŸ“˜ Topic and Domain:** The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.\\n\\n2.  **ğŸ’¡ Previous Research and New Ideas:** The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.\\n\\n3.  **â“ Problem:** The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.\\n\\n4.  **ğŸ› ï¸ Methods:** The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.\\n\\n5.  **ğŸ“Š Results and Evaluation:** FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.\\n'}, {'title': 'Optimizing Decomposition for Optimal Claim Verification', 'published_at': '2025-03-19', 'url': 'http://arxiv.org/pdf/2503.15354', 'content': 'Here\\'s a concise analysis of the paper based on your requested format:\\n\\n1.  **ğŸ“˜ Topic and Domain:** The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.\\n\\n2.  **ğŸ’¡ Previous Research and New Ideas:** The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.\\n\\n3.  **â“ Problem:** The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don\\'t generate subclaims with optimal atomicity for downstream verification.\\n\\n4.  **ğŸ› ï¸ Methods:** The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.\\n\\n5.  **ğŸ“Š Results and Evaluation:** The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.\\n'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.  **ğŸ“˜ Topic and Domain:** The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.\\n\\n2.  **ğŸ’¡ Previous Research and New Ideas:** The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.\\n\\n3.  **â“ Problem:** The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.\\n\\n4.  **ğŸ› ï¸ Methods:** The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.\\n\\n5.  **ğŸ“Š Results and Evaluation:** FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a concise analysis of the paper based on your requested format:\\n\\n1.  **ğŸ“˜ Topic and Domain:** The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.\\n\\n2.  **ğŸ’¡ Previous Research and New Ideas:** The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.\\n\\n3.  **â“ Problem:** The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don\\'t generate subclaims with optimal atomicity for downstream verification.\\n\\n4.  **ğŸ› ï¸ Methods:** The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.\\n\\n5.  **ğŸ“Š Results and Evaluation:** The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <div class=\"paper-card\">\n",
      "            <h2>Paper: 5</h2>\n",
      "            <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>\n",
      "            <p><strong>Published: </strong>2025-03-19</p>\n",
      "            <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15417\" target=\"_blank\">http://arxiv.org/pdf/2503.15417</a></p>\n",
      "            <div><div class=\"category-chunk\">1.  <strong>ğŸ“˜ Topic and Domain:</strong> The paper focuses on temporal data augmentation for video generation, specifically within the domain of computer vision and deep learning.</div><div class=\"category-chunk\">2.  <strong>ğŸ’¡ Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and proposes FLUX FLOW, a novel temporal augmentation strategy that perturbs frame order during training.</div><div class=\"category-chunk\">3.  <strong>â“ Problem:</strong> The paper aims to solve the problem of temporal inconsistency and limited temporal diversity in generated videos, such as flickering and unnatural motion.</div><div class=\"category-chunk\">4.  <strong>ğŸ› ï¸ Methods:</strong> The authors used FLUX FLOW, which includes frame-level and block-level temporal perturbations, applied as a pre-processing step during the training of video generation models.</div><div class=\"category-chunk\">5.  <strong>ğŸ“Š Results and Evaluation:</strong> FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and user studies.</div></div>\n",
      "        </div>\n",
      "        \n",
      "        <div class=\"paper-card\">\n",
      "            <h2>Paper: 5</h2>\n",
      "            <p><strong>Optimizing Decomposition for Optimal Claim Verification</strong></p>\n",
      "            <p><strong>Published: </strong>2025-03-19</p>\n",
      "            <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15354\" target=\"_blank\">http://arxiv.org/pdf/2503.15354</a></p>\n",
      "            <div><div class=\"category-chunk\">1.  <strong>ğŸ“˜ Topic and Domain:</strong> The paper focuses on fact-checking of long-form text, specifically optimizing the decomposition stage within the \"Decompose-Then-Verify\" paradigm in the domain of Natural Language Processing.</div><div class=\"category-chunk\">2.  <strong>ğŸ’¡ Previous Research and New Ideas:</strong> The paper builds upon existing \"Decompose-Then-Verify\" fact-checking methods that use handcrafted prompts, and proposes a novel reinforcement learning framework (dynamic decomposition) to learn a decomposition policy tailored to the verifier, introducing the concept of \"atomicity\" to quantify information density.</div><div class=\"category-chunk\">3.  <strong>â“ Problem:</strong> The paper aims to solve the misalignment between decomposers and verifiers in existing fact-checking systems, where static decomposition policies don't generate subclaims with optimal atomicity for downstream verification.</div><div class=\"category-chunk\">4.  <strong>ğŸ› ï¸ Methods:</strong> The authors used a reinforcement learning (RL) framework, specifically Proximal Policy Optimization (PPO) in an Advantage Actor-Critic (A2C) style, to train a dynamic decomposition policy that interacts with a verifier and receives feedback.</div><div class=\"category-chunk\">5.  <strong>ğŸ“Š Results and Evaluation:</strong> The results, evaluated on verification confidence and accuracy across various datasets and verifiers, show that dynamic decomposition outperforms existing static decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 on average.</div></div>\n",
      "        </div>\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(generate_paper_html(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"\"\"\n",
    "<div class=\"paper-card\">\n",
    "    <h2>Paper: 1</h2>\n",
    "    <p><strong>Temporal Regularization Makes Your Video Generator Stronger</strong></p>\n",
    "    <p><strong>Published: </strong>2025-03-19</p>\n",
    "    <p><strong>Link: </strong><a href=\"http://arxiv.org/pdf/2503.15417\" target=\"_blank\">http://arxiv.org/pdf/2503.15417</a></p>\n",
    "    <div>\n",
    "        <div class=\"category-chunk\">1.  <strong>ğŸ“˜ Topic and Domain:</strong> The paper focuses on improving the temporal quality of video generation, specifically addressing temporal coherence and diversity, within the domain of computer vision and deep learning.</div>\n",
    "        <div class=\"category-chunk\">2.  <strong>ğŸ’¡ Previous Research and New Ideas:</strong> The paper builds on existing video generation models (U-Net, DiT, AR-based) and temporal refinement techniques (architecture-centric modeling, physics-informed regularization, training dynamics optimization), and proposes FLUX FLOW, a novel data-level temporal augmentation strategy.</div>\n",
    "        <div class=\"category-chunk\">3.  <strong>â“ Problem:</strong> The paper aims to solve the problem of temporal artifacts (flickering, discontinuous motion) and limited temporal diversity in videos generated by current video generation models.</div>\n",
    "        <div class=\"category-chunk\">4.  <strong>ğŸ› ï¸ Methods:</strong> The authors used FLUX FLOW, which involves frame-level and block-level temporal perturbations (random shuffling of frames or blocks of frames) during the training of video generation models.</div>\n",
    "        <div class=\"category-chunk\">5.  <strong>ğŸ“Š Results and Evaluation:</strong> FLUX FLOW significantly improved temporal coherence and diversity across various video generation models, as evaluated on UCF-101 and VBench benchmarks using metrics like FVD, IS, and various VBench temporal and frame-wise quality scores, and was further supported by a user study.</div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
